{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Setting Up Ray and RLlib\n",
    "\n",
    "## Learning Objectives\n",
    "- Install and configure Ray and RLlib\n",
    "- Understand Ray's architecture and concepts\n",
    "- Run your first RLlib training job\n",
    "- Monitor training with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Ray?\n",
    "\n",
    "Ray is a unified framework for scaling AI and Python applications. It provides:\n",
    "\n",
    "- **Ray Core**: Distributed computing primitives\n",
    "- **Ray RLlib**: Scalable reinforcement learning\n",
    "- **Ray Tune**: Hyperparameter tuning\n",
    "- **Ray Serve**: Model serving\n",
    "- **Ray Data**: Distributed data processing\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    Ray Libraries                         │\n",
    "├──────────┬──────────┬──────────┬──────────┬────────────┤\n",
    "│  RLlib   │   Tune   │  Serve   │   Data   │   Train    │\n",
    "├──────────┴──────────┴──────────┴──────────┴────────────┤\n",
    "│                      Ray Core                           │\n",
    "│            (Tasks, Actors, Objects)                     │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│              Cluster Management                         │\n",
    "│        (Local, Cloud, Kubernetes)                       │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install Ray with RLlib support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ray with RLlib\n",
    "# !pip install \"ray[rllib]\" gymnasium torch tensorboard\n",
    "\n",
    "# For specific versions (recommended for reproducibility):\n",
    "# !pip install \"ray[rllib]==2.9.0\" gymnasium torch tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "print(f\"Ray version: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Core Basics\n",
    "\n",
    "Before using RLlib, let's understand Ray's core concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray (use ray.init() for local, or connect to a cluster)\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Check cluster resources\n",
    "print(ray.cluster_resources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray Tasks: Parallel function execution\n",
    "@ray.remote\n",
    "def simulate_episode(env_name: str) -> float:\n",
    "    \"\"\"Simulate one episode and return total reward.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        action = env.action_space.sample()  # Random policy\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    return total_reward\n",
    "\n",
    "# Run 10 episodes in parallel\n",
    "futures = [simulate_episode.remote(\"CartPole-v1\") for _ in range(10)]\n",
    "results = ray.get(futures)\n",
    "\n",
    "print(f\"Episode rewards: {results}\")\n",
    "print(f\"Mean reward: {np.mean(results):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray Actors: Stateful distributed objects\n",
    "@ray.remote\n",
    "class Counter:\n",
    "    def __init__(self):\n",
    "        self.value = 0\n",
    "    \n",
    "    def increment(self):\n",
    "        self.value += 1\n",
    "        return self.value\n",
    "    \n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "\n",
    "# Create actor and call methods\n",
    "counter = Counter.remote()\n",
    "for _ in range(5):\n",
    "    ray.get(counter.increment.remote())\n",
    "\n",
    "print(f\"Counter value: {ray.get(counter.get_value.remote())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLlib: Your First Training Job\n",
    "\n",
    "RLlib uses a config-based API. Let's train PPO on CartPole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure PPO algorithm\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")  # or \"tf2\" for TensorFlow\n",
    "    .env_runners(\n",
    "        num_env_runners=2,  # Number of parallel workers\n",
    "        num_envs_per_env_runner=1,\n",
    "    )\n",
    "    .training(\n",
    "        lr=0.0003,\n",
    "        gamma=0.99,\n",
    "        train_batch_size=4000,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build the algorithm\n",
    "algo = config.build()\n",
    "\n",
    "print(\"Algorithm built successfully!\")\n",
    "print(f\"Policy: {algo.get_policy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for a few iterations\n",
    "results_history = []\n",
    "\n",
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "    results_history.append(result)\n",
    "    \n",
    "    # Extract key metrics\n",
    "    mean_reward = result[\"env_runners\"][\"episode_reward_mean\"]\n",
    "    episodes = result[\"env_runners\"][\"num_episodes\"]\n",
    "    \n",
    "    print(f\"Iteration {i+1}: Mean Reward = {mean_reward:.2f}, Episodes = {episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained policy\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "eval_rewards = []\n",
    "for _ in range(10):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        action = algo.compute_single_action(state)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    eval_rewards.append(total_reward)\n",
    "\n",
    "print(f\"Evaluation rewards: {eval_rewards}\")\n",
    "print(f\"Mean evaluation reward: {np.mean(eval_rewards):.2f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "checkpoint_dir = algo.save()\n",
    "print(f\"Checkpoint saved to: {checkpoint_dir}\")\n",
    "\n",
    "# Clean up\n",
    "algo.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Ray Tune for Training\n",
    "\n",
    "Ray Tune provides a more robust way to run experiments with logging, checkpointing, and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.registry import register_env\n",
    "\n",
    "# Configure the experiment\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        lr=0.0003,\n",
    "        train_batch_size=4000,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run with Tune\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=config,\n",
    "    run_config=tune.RunConfig(\n",
    "        stop={\"env_runners/episode_reward_mean\": 450},  # Stop when solved\n",
    "        checkpoint_config=tune.CheckpointConfig(\n",
    "            checkpoint_at_end=True,\n",
    "            checkpoint_frequency=5,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "\n",
    "# Get best result\n",
    "best_result = results.get_best_result(metric=\"env_runners/episode_reward_mean\", mode=\"max\")\n",
    "print(f\"Best reward: {best_result.metrics['env_runners']['episode_reward_mean']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring with TensorBoard\n",
    "\n",
    "Ray logs metrics to TensorBoard by default.\n",
    "\n",
    "```bash\n",
    "# In terminal, run:\n",
    "tensorboard --logdir ~/ray_results\n",
    "```\n",
    "\n",
    "Then open http://localhost:6006 in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also load TensorBoard in Jupyter\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ~/ray_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLlib Configuration Deep Dive\n",
    "\n",
    "RLlib's config has several sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive configuration example\n",
    "full_config = (\n",
    "    PPOConfig()\n",
    "    \n",
    "    # Environment settings\n",
    "    .environment(\n",
    "        env=\"CartPole-v1\",\n",
    "        env_config={},  # Pass config to env\n",
    "        observation_space=None,  # Override if needed\n",
    "        action_space=None,\n",
    "    )\n",
    "    \n",
    "    # Framework (PyTorch or TensorFlow)\n",
    "    .framework(\n",
    "        framework=\"torch\",\n",
    "    )\n",
    "    \n",
    "    # Rollout workers\n",
    "    .env_runners(\n",
    "        num_env_runners=4,          # Parallel workers\n",
    "        num_envs_per_env_runner=1,  # Envs per worker\n",
    "        rollout_fragment_length=200, # Steps per rollout\n",
    "        batch_mode=\"truncate_episodes\",\n",
    "    )\n",
    "    \n",
    "    # Training settings\n",
    "    .training(\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        train_batch_size=4000,\n",
    "        model={\n",
    "            \"fcnet_hiddens\": [256, 256],\n",
    "            \"fcnet_activation\": \"tanh\",\n",
    "        },\n",
    "        # PPO-specific\n",
    "        sgd_minibatch_size=128,\n",
    "        num_sgd_iter=10,\n",
    "        clip_param=0.2,\n",
    "    )\n",
    "    \n",
    "    # Resources\n",
    "    .resources(\n",
    "        num_gpus=0,  # GPUs for training\n",
    "        num_cpus_per_env_runner=1,\n",
    "    )\n",
    "    \n",
    "    # Evaluation\n",
    "    .evaluation(\n",
    "        evaluation_interval=5,       # Eval every N iterations\n",
    "        evaluation_num_env_runners=2,\n",
    "        evaluation_duration=10,      # Episodes per eval\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Configuration created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Restoring Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "# Restore from checkpoint\n",
    "# algo = Algorithm.from_checkpoint(checkpoint_dir)\n",
    "\n",
    "# Or restore a specific algorithm type\n",
    "# from ray.rllib.algorithms.ppo import PPO\n",
    "# algo = PPO.from_checkpoint(checkpoint_dir)\n",
    "\n",
    "print(\"Checkpoint loading example (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Ray Core** provides distributed computing primitives (tasks, actors)\n",
    "\n",
    "2. **RLlib** uses a config-based API for easy experimentation\n",
    "\n",
    "3. **Ray Tune** adds experiment management, logging, and hyperparameter tuning\n",
    "\n",
    "4. **TensorBoard** integration for monitoring training progress\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll explore different RLlib algorithms and when to use each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up Ray\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
