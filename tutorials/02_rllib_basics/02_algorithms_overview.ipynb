{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLlib Algorithms: Choosing the Right Tool\n",
    "\n",
    "**Prerequisites**: Complete [02.1 RLlib Setup](./01_ray_setup.ipynb)\n",
    "\n",
    "RLlib has many algorithms. How do you choose? This guide will help.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                        ALGORITHM DECISION FLOWCHART                         │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│                         What's your ACTION SPACE?                           │\n",
    "│                                   │                                         │\n",
    "│                    ┌──────────────┴──────────────┐                          │\n",
    "│                    │                             │                          │\n",
    "│              DISCRETE                       CONTINUOUS                      │\n",
    "│           (left/right/jump)              (torque: -1.0 to 1.0)              │\n",
    "│                    │                             │                          │\n",
    "│             ┌──────┴──────┐               ┌──────┴──────┐                   │\n",
    "│             │             │               │             │                   │\n",
    "│    Need sample      Don't care     Need sample     Don't care               │\n",
    "│    efficiency?                     efficiency?                              │\n",
    "│             │             │               │             │                   │\n",
    "│             v             v               v             v                   │\n",
    "│         ┌─────┐       ┌─────┐         ┌─────┐       ┌─────┐                │\n",
    "│         │ DQN │       │ PPO │         │ SAC │       │ PPO │                │\n",
    "│         └─────┘       └─────┘         └─────┘       └─────┘                │\n",
    "│                           │                             │                   │\n",
    "│                           └──────────────┬──────────────┘                   │\n",
    "│                                          │                                  │\n",
    "│                                  ┌───────────────┐                          │\n",
    "│                                  │  PPO is your  │                          │\n",
    "│                                  │  DEFAULT!     │                          │\n",
    "│                                  └───────────────┘                          │\n",
    "│                                                                             │\n",
    "│  Rule of thumb: When in doubt, use PPO. It works almost everywhere.         │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"ray\").setLevel(logging.ERROR)\n",
    "\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray.rllib.algorithms.sac import SACConfig\n",
    "from ray.rllib.algorithms.a2c import A2CConfig\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=4,\n",
    "    object_store_memory=1 * 1024 * 1024 * 1024,\n",
    "    ignore_reinit_error=True,\n",
    ")\n",
    "print(f\"Ray initialized: {ray.cluster_resources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Understanding the Algorithm Zoo\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                           ALGORITHM TAXONOMY                                │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│                            RL Algorithms                                    │\n",
    "│                                  │                                          │\n",
    "│              ┌───────────────────┴───────────────────┐                      │\n",
    "│              │                                       │                      │\n",
    "│        MODEL-FREE                              MODEL-BASED                  │\n",
    "│    \"Learn from experience\"               \"Learn environment model\"          │\n",
    "│              │                              (Dreamer, MBPO)                 │\n",
    "│              │                                                              │\n",
    "│      ┌───────┴───────┐                                                      │\n",
    "│      │               │                                                      │\n",
    "│  VALUE-BASED    POLICY-BASED                                                │\n",
    "│  \"Learn Q(s,a)\"  \"Learn π(a|s)\"                                             │\n",
    "│      │               │                                                      │\n",
    "│      │         ┌─────┴─────┐                                                │\n",
    "│      │         │           │                                                │\n",
    "│    DQN    Actor-Critic  Pure Policy                                         │\n",
    "│   Rainbow  (both!)     REINFORCE                                            │\n",
    "│              │                                                              │\n",
    "│       ┌──────┴──────┐                                                       │\n",
    "│       │             │                                                       │\n",
    "│   ON-POLICY     OFF-POLICY                                                  │\n",
    "│   \"Fresh data\"  \"Replay buffer\"                                             │\n",
    "│       │             │                                                       │\n",
    "│   ┌───┴───┐     ┌───┴───┐                                                   │\n",
    "│   │       │     │       │                                                   │\n",
    "│  A2C     PPO   SAC     TD3                                                  │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-Policy vs Off-Policy: The Key Distinction\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                     ON-POLICY vs OFF-POLICY                                 │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│  ON-POLICY (PPO, A2C)                OFF-POLICY (DQN, SAC)                  │\n",
    "│  ────────────────────                ──────────────────────                 │\n",
    "│                                                                             │\n",
    "│  Collect ──> Train ──> DISCARD       Collect ──> Store ──> Sample ──> Train │\n",
    "│   data        data      data          data       buffer    randomly         │\n",
    "│                                                     │                       │\n",
    "│     ┌──────┐                              ┌────────────────┐                │\n",
    "│     │ Data │  Use once,                   │ Replay Buffer  │                │\n",
    "│     │      │  throw away                  │ ┌─┐┌─┐┌─┐┌─┐   │                │\n",
    "│     └──────┘                              │ │ ││ ││ ││ │...│  Reuse many    │\n",
    "│                                           │ └─┘└─┘└─┘└─┘   │  times!        │\n",
    "│                                           └────────────────┘                │\n",
    "│                                                                             │\n",
    "│  Pros:                                   Pros:                              │\n",
    "│  ✓ More stable                           ✓ Sample efficient                 │\n",
    "│  ✓ Simpler to implement                  ✓ Can learn from old data          │\n",
    "│  ✓ Guaranteed convergence                ✓ Better for expensive envs        │\n",
    "│                                                                             │\n",
    "│  Cons:                                   Cons:                              │\n",
    "│  ✗ Needs lots of data                    ✗ Can be unstable                  │\n",
    "│  ✗ Can't reuse old experience            ✗ More hyperparameters             │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. PPO (Proximal Policy Optimization)\n",
    "\n",
    "**The safe default.** Works on almost anything.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                              PPO EXPLAINED                                  │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│  TYPE: On-policy, Actor-Critic                                              │\n",
    "│  ACTIONS: Discrete AND Continuous                                           │\n",
    "│                                                                             │\n",
    "│  KEY IDEA: Limit how much the policy can change per update                  │\n",
    "│                                                                             │\n",
    "│     Old Policy                    New Policy                                │\n",
    "│     π_old(a|s)                    π_new(a|s)                                │\n",
    "│         │                              │                                    │\n",
    "│         └──────────┬───────────────────┘                                    │\n",
    "│                    │                                                        │\n",
    "│                    v                                                        │\n",
    "│         ratio = π_new(a|s) / π_old(a|s)                                     │\n",
    "│                    │                                                        │\n",
    "│                    v                                                        │\n",
    "│         ┌──────────────────────────┐                                        │\n",
    "│         │  CLIP(ratio, 1-ε, 1+ε)   │  ← Keep ratio between 0.8 and 1.2     │\n",
    "│         │  where ε = 0.2           │    (prevents wild policy swings)       │\n",
    "│         └──────────────────────────┘                                        │\n",
    "│                                                                             │\n",
    "│  Without clipping:          With PPO clipping:                              │\n",
    "│  ┌──────────────────┐       ┌──────────────────┐                            │\n",
    "│  │       /\\         │       │         ___      │                            │\n",
    "│  │      /  \\  crash!│       │        /   \\     │  (smooth learning)         │\n",
    "│  │     /    \\___    │       │     __/     \\__  │                            │\n",
    "│  │    /             │       │    /             │                            │\n",
    "│  └──────────────────┘       └──────────────────┘                            │\n",
    "│                                                                             │\n",
    "│  WHEN TO USE:                                                               │\n",
    "│  • First algorithm to try on any new problem                                │\n",
    "│  • Continuous control (robotics, MuJoCo)                                    │\n",
    "│  • When stability matters more than sample efficiency                       │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Configuration\n",
    "ppo_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        lr=3e-4,                  # Learning rate\n",
    "        gamma=0.99,               # Discount factor\n",
    "        train_batch_size=4000,    # Samples per update\n",
    "        \n",
    "        # PPO-specific\n",
    "        sgd_minibatch_size=128,   # Minibatch size\n",
    "        num_sgd_iter=10,          # Epochs per batch\n",
    "        clip_param=0.2,           # PPO clipping (the key innovation!)\n",
    "        vf_loss_coeff=0.5,        # Value function loss weight\n",
    "        entropy_coeff=0.01,       # Exploration bonus\n",
    "        \n",
    "        # GAE (Generalized Advantage Estimation)\n",
    "        use_gae=True,\n",
    "        lambda_=0.95,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"PPO config created\")\n",
    "print(f\"  clip_param = 0.2 means policy can only change by ±20% per update\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. DQN (Deep Q-Network)\n",
    "\n",
    "**For discrete actions when you need sample efficiency.**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                              DQN EXPLAINED                                  │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│  TYPE: Off-policy, Value-based                                              │\n",
    "│  ACTIONS: Discrete ONLY (no continuous!)                                    │\n",
    "│                                                                             │\n",
    "│  KEY IDEA: Learn Q(s,a) with neural network + replay buffer + target net    │\n",
    "│                                                                             │\n",
    "│     ┌──────────────────────────────────────────────────────────────┐        │\n",
    "│     │                    DQN COMPONENTS                            │        │\n",
    "│     ├──────────────────────────────────────────────────────────────┤        │\n",
    "│     │                                                              │        │\n",
    "│     │  1. Q-Network: state → Q-values for each action              │        │\n",
    "│     │                                                              │        │\n",
    "│     │  2. Replay Buffer: stores (s, a, r, s') tuples               │        │\n",
    "│     │     └─> enables learning from past experience                │        │\n",
    "│     │                                                              │        │\n",
    "│     │  3. Target Network: slowly-updating copy of Q-network        │        │\n",
    "│     │     └─> provides stable learning targets                     │        │\n",
    "│     │                                                              │        │\n",
    "│     │  4. ε-greedy: explore randomly with probability ε            │        │\n",
    "│     │                                                              │        │\n",
    "│     └──────────────────────────────────────────────────────────────┘        │\n",
    "│                                                                             │\n",
    "│  ENHANCEMENTS (RLlib supports all of these):                                │\n",
    "│                                                                             │\n",
    "│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐        │\n",
    "│  │ Double DQN  │  │ Dueling DQN │  │ N-step      │  │ Prioritized │        │\n",
    "│  │             │  │             │  │             │  │ Replay      │        │\n",
    "│  │ Fixes over- │  │ Separates   │  │ Look ahead  │  │ Sample imp- │        │\n",
    "│  │ estimation  │  │ V and A     │  │ N steps     │  │ ortant exp. │        │\n",
    "│  └─────────────┘  └─────────────┘  └─────────────┘  └─────────────┘        │\n",
    "│                                                                             │\n",
    "│  WHEN TO USE:                                                               │\n",
    "│  • Discrete action spaces (Atari, board games)                              │\n",
    "│  • When sample efficiency matters (expensive environments)                  │\n",
    "│  • When you can collect lots of data over time                              │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Configuration\n",
    "dqn_config = (\n",
    "    DQNConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        train_batch_size=32,\n",
    "        \n",
    "        # DQN-specific\n",
    "        replay_buffer_config={\n",
    "            \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "            \"capacity\": 50000,\n",
    "        },\n",
    "        double_q=True,              # Double DQN (reduces overestimation)\n",
    "        dueling=True,               # Dueling DQN (separates V and A)\n",
    "        n_step=3,                   # N-step returns (look ahead)\n",
    "        target_network_update_freq=500,  # How often to update target\n",
    "    )\n",
    "    .exploration(\n",
    "        exploration_config={\n",
    "            \"type\": \"EpsilonGreedy\",\n",
    "            \"initial_epsilon\": 1.0,   # Start with 100% random\n",
    "            \"final_epsilon\": 0.02,    # End with 2% random\n",
    "            \"epsilon_timesteps\": 10000,\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"DQN config created\")\n",
    "print(\"  Using: Double DQN + Dueling DQN + Prioritized Replay + 3-step returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 3. SAC (Soft Actor-Critic)\n\n**For continuous control when you need sample efficiency.**\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                              SAC EXPLAINED                                  │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  TYPE: Off-policy, Actor-Critic                                             │\n│  ACTIONS: Continuous (can do discrete but not recommended)                  │\n│                                                                             │\n│  KEY IDEA: Maximize reward AND entropy (built-in exploration!)              │\n│                                                                             │\n│     Regular RL objective:        SAC objective:                             │\n│     ────────────────────         ──────────────                             │\n│     max  Σ γᵗ rₜ                 max  Σ γᵗ (rₜ + α H(π))                   │\n│                                            └─────────┘                      │\n│                                            Entropy bonus!                   │\n│                                            (keep exploring)                 │\n│                                                                             │\n│  WHY ENTROPY MATTERS:                                                       │\n│                                                                             │\n│     Low Entropy Policy          High Entropy Policy                         │\n│     ─────────────────           ────────────────────                        │\n│     \"Always do action 1\"        \"Spread bets across actions\"                │\n│                                                                             │\n│     P(a) █████████              P(a) ██ ██ ██ ██                            │\n│          │░░░░░░░│                   │  │  │  │                             │\n│          1 2 3 4                     1  2  3  4                             │\n│                                                                             │\n│     Problem: Gets stuck!        Benefit: Keeps exploring!                   │\n│                                                                             │\n│  ARCHITECTURE:                                                              │\n│                                                                             │\n│     ┌────────────┐    ┌────────────┐    ┌────────────┐                     │\n│     │   Actor    │    │  Critic 1  │    │  Critic 2  │  ← Two critics      │\n│     │   π(a|s)   │    │  Q₁(s,a)   │    │  Q₂(s,a)   │    (min of both)    │\n│     └────────────┘    └────────────┘    └────────────┘                     │\n│                                                                             │\n│  WHEN TO USE:                                                               │\n│  • Continuous control (robotics, MuJoCo)                                    │\n│  • When sample efficiency is critical                                       │\n│  • When you want automatic exploration tuning                               │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAC Configuration (for continuous action space)\n",
    "sac_config = (\n",
    "    SACConfig()\n",
    "    .environment(\"Pendulum-v1\")  # Continuous action space!\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        train_batch_size=256,\n",
    "        \n",
    "        # SAC-specific\n",
    "        tau=0.005,                # Soft update coefficient (Polyak averaging)\n",
    "        initial_alpha=1.0,        # Entropy coefficient\n",
    "        target_entropy=\"auto\",    # Auto-tune entropy (key SAC feature!)\n",
    "        n_step=1,\n",
    "        replay_buffer_config={\n",
    "            \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "            \"capacity\": 100000,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"SAC config created\")\n",
    "print(\"  target_entropy='auto' means SAC auto-tunes exploration!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 4. A2C (Advantage Actor-Critic)\n\n**Simple baseline, good for understanding.**\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                              A2C EXPLAINED                                  │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  TYPE: On-policy, Actor-Critic                                              │\n│  ACTIONS: Discrete AND Continuous                                           │\n│                                                                             │\n│  KEY IDEA: Synchronous advantage actor-critic                               │\n│                                                                             │\n│     A2C vs A3C:                                                             │\n│     ────────────                                                            │\n│                                                                             │\n│     A3C (Async):              A2C (Sync):                                   │\n│     Workers update            Workers wait,                                 │\n│     independently             update together                               │\n│                                                                             │\n│     W1 ──┐                    W1 ──┐                                        │\n│     W2 ──┼──> update          W2 ──┼──> wait ──> update                     │\n│     W3 ──┘    (anytime)       W3 ──┘    (sync)   (together)                 │\n│                                                                             │\n│     More throughput           More stable                                   │\n│     Less stable               Simpler to implement                          │\n│                                                                             │\n│  WHY USE A2C?                                                               │\n│  • Simpler than PPO (good for learning)                                     │\n│  • Good baseline to compare against                                         │\n│  • Fast iteration (no replay buffer overhead)                               │\n│                                                                             │\n│  WHY NOT USE A2C?                                                           │\n│  • PPO is almost always better                                              │\n│  • Less sample efficient than off-policy methods                            │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C Configuration\n",
    "a2c_config = (\n",
    "    A2CConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        train_batch_size=500,\n",
    "        \n",
    "        # A2C-specific\n",
    "        vf_loss_coeff=0.5,\n",
    "        entropy_coeff=0.01,\n",
    "        use_gae=True,\n",
    "        lambda_=0.95,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"A2C config created\")\n",
    "print(\"  A2C is simpler than PPO (no clipping) but less stable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Algorithm Comparison\n\nLet's train each algorithm and compare!\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                        WHAT TO EXPECT                                       │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  Algorithm   Sample Efficiency   Stability   Convergence Speed              │\n│  ─────────   ─────────────────   ─────────   ─────────────────              │\n│  PPO         Medium              High        Medium                         │\n│  DQN         High (replay)       Medium      Slow then fast                 │\n│  A2C         Low                 Medium      Fast but noisy                 │\n│                                                                             │\n│  Expected on CartPole:                                                      │\n│  • All should solve it (reach 475+)                                         │\n│  • PPO: Steady improvement                                                  │\n│  • DQN: Slow start, then takes off                                          │\n│  • A2C: Fast start, more variance                                           │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(config, name, n_iters=20):\n",
    "    \"\"\"Train an algorithm and return learning curve.\"\"\"\n",
    "    algo = config.build_algo()  # Use new API!\n",
    "    rewards = []\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        result = algo.train()\n",
    "        # Use new API key name\n",
    "        reward = result[\"env_runners\"][\"episode_return_mean\"]\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"{name:>6} - Iter {i+1:>2}: {reward:>6.1f}\")\n",
    "    \n",
    "    algo.stop()\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PPO vs DQN vs A2C on CartPole\n",
    "print(\"Training PPO...\")\n",
    "print(\"=\" * 40)\n",
    "ppo_rewards = train_and_evaluate(ppo_config, \"PPO\", n_iters=20)\n",
    "\n",
    "print(\"\\nTraining DQN...\")\n",
    "print(\"=\" * 40)\n",
    "dqn_rewards = train_and_evaluate(dqn_config, \"DQN\", n_iters=20)\n",
    "\n",
    "print(\"\\nTraining A2C...\")\n",
    "print(\"=\" * 40)\n",
    "a2c_rewards = train_and_evaluate(a2c_config, \"A2C\", n_iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ppo_rewards, label='PPO', linewidth=2, marker='o', markersize=4)\n",
    "plt.plot(dqn_rewards, label='DQN', linewidth=2, marker='s', markersize=4)\n",
    "plt.plot(a2c_rewards, label='A2C', linewidth=2, marker='^', markersize=4)\n",
    "plt.axhline(y=475, color='gray', linestyle='--', label='Solved (475)')\n",
    "plt.xlabel('Training Iteration')\n",
    "plt.ylabel('Mean Episode Return')\n",
    "plt.title('Algorithm Comparison on CartPole-v1')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Quick Reference: Algorithm Cheat Sheet\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                        ALGORITHM CHEAT SHEET                                │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  Algorithm │ Actions    │ Sample Eff │ Stability │ Best For                │\n│  ──────────┼────────────┼────────────┼───────────┼─────────────────────────│\n│  PPO       │ Both       │ Medium     │ High      │ DEFAULT CHOICE!         │\n│  DQN       │ Discrete   │ High       │ Medium    │ Atari, board games      │\n│  SAC       │ Continuous │ High       │ Medium    │ Robotics, MuJoCo        │\n│  A2C       │ Both       │ Low        │ Medium    │ Simple baseline         │\n│  IMPALA    │ Both       │ Medium     │ Medium    │ Massive scale           │\n│  APEX-DQN  │ Discrete   │ High       │ Medium    │ Distributed DQN         │\n│                                                                             │\n│  DECISION RULES:                                                            │\n│  ───────────────                                                            │\n│  1. Start with PPO. Always.                                                 │\n│  2. Discrete actions + need efficiency? --> Try DQN                         │\n│  3. Continuous actions + need efficiency? --> Try SAC                       │\n│  4. Massive scale (100+ workers)? --> Consider IMPALA                       │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Algorithms in RLlib\n",
    "\n",
    "RLlib has many more algorithms:\n",
    "\n",
    "| Category | Algorithms | Use Case |\n",
    "|----------|-----------|----------|\n",
    "| **Distributed** | IMPALA, APEX-DQN | Scale to many workers |\n",
    "| **Continuous** | TD3, DDPG | Alternative to SAC |\n",
    "| **Multi-Agent** | QMIX, MADDPG | Cooperative/competitive agents |\n",
    "| **Offline RL** | CQL, MARWIL | Learn from fixed datasets |\n",
    "| **Model-Based** | Dreamer | Learn world model |\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "```\n",
    "┌──────────────────┐          ┌──────────────────┐          ┌──────────────────┐\n",
    "│ 02.2 Algorithms  │   ───>   │ 03 Custom Envs   │   ───>   │ 05 Distributed   │\n",
    "│ (you are here)   │          │                  │          │                  │\n",
    "│                  │          │ Build your own   │          │ Scale to many    │\n",
    "│ - PPO, DQN, SAC  │          │ Gymnasium envs   │          │ GPUs & workers   │\n",
    "│ - When to use    │          │                  │          │                  │\n",
    "└──────────────────┘          └──────────────────┘          └──────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}