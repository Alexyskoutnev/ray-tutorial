{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 RLlib Algorithms Overview\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the main algorithm families in RLlib\n",
    "- Learn when to use each algorithm\n",
    "- Compare performance characteristics\n",
    "- Run multiple algorithms on the same environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "from ray.rllib.algorithms.sac import SACConfig\n",
    "from ray.rllib.algorithms.a2c import A2CConfig\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Taxonomy\n",
    "\n",
    "```\n",
    "                    RL Algorithms\n",
    "                          │\n",
    "            ┌─────────────┴─────────────┐\n",
    "            │                           │\n",
    "      Model-Free                   Model-Based\n",
    "            │                      (Dreamer, MBPO)\n",
    "    ┌───────┴───────┐\n",
    "    │               │\n",
    "Value-Based    Policy-Based\n",
    "(DQN, Rainbow)  (REINFORCE)\n",
    "                    │\n",
    "              Actor-Critic\n",
    "            ┌───────┴───────┐\n",
    "            │               │\n",
    "        On-Policy       Off-Policy\n",
    "        (A2C, PPO)      (SAC, TD3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DQN (Deep Q-Network)\n",
    "\n",
    "**Type**: Value-based, Off-policy\n",
    "\n",
    "**Best for**: Discrete action spaces, sample efficiency needed\n",
    "\n",
    "**Key features**:\n",
    "- Experience replay for sample efficiency\n",
    "- Target network for stability\n",
    "- Works only with discrete actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Configuration\n",
    "dqn_config = (\n",
    "    DQNConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        train_batch_size=32,\n",
    "        # DQN specific\n",
    "        replay_buffer_config={\n",
    "            \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "            \"capacity\": 50000,\n",
    "        },\n",
    "        double_q=True,        # Double DQN\n",
    "        dueling=True,         # Dueling DQN\n",
    "        n_step=3,             # N-step returns\n",
    "        target_network_update_freq=500,\n",
    "    )\n",
    "    .exploration(\n",
    "        exploration_config={\n",
    "            \"type\": \"EpsilonGreedy\",\n",
    "            \"initial_epsilon\": 1.0,\n",
    "            \"final_epsilon\": 0.02,\n",
    "            \"epsilon_timesteps\": 10000,\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"DQN config created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PPO (Proximal Policy Optimization)\n",
    "\n",
    "**Type**: Actor-Critic, On-policy\n",
    "\n",
    "**Best for**: General-purpose, continuous & discrete actions, robustness\n",
    "\n",
    "**Key features**:\n",
    "- Clipped objective prevents large policy updates\n",
    "- Stable and reliable across many tasks\n",
    "- Easy to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Configuration\n",
    "ppo_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        train_batch_size=4000,\n",
    "        # PPO specific\n",
    "        sgd_minibatch_size=128,\n",
    "        num_sgd_iter=10,\n",
    "        clip_param=0.2,           # Clipping range\n",
    "        vf_loss_coeff=0.5,        # Value function loss coefficient\n",
    "        entropy_coeff=0.01,       # Entropy bonus for exploration\n",
    "        use_gae=True,             # Generalized Advantage Estimation\n",
    "        lambda_=0.95,             # GAE lambda\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"PPO config created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SAC (Soft Actor-Critic)\n",
    "\n",
    "**Type**: Actor-Critic, Off-policy\n",
    "\n",
    "**Best for**: Continuous action spaces, sample efficiency\n",
    "\n",
    "**Key features**:\n",
    "- Maximum entropy framework (exploration built-in)\n",
    "- Very sample efficient due to replay buffer\n",
    "- Automatic temperature tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAC Configuration (for continuous action space)\n",
    "sac_config = (\n",
    "    SACConfig()\n",
    "    .environment(\"Pendulum-v1\")  # Continuous action space\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        train_batch_size=256,\n",
    "        # SAC specific\n",
    "        tau=0.005,                # Soft update coefficient\n",
    "        initial_alpha=1.0,        # Entropy coefficient\n",
    "        target_entropy=\"auto\",    # Automatic entropy tuning\n",
    "        n_step=1,\n",
    "        replay_buffer_config={\n",
    "            \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "            \"capacity\": 100000,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"SAC config created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. A2C (Advantage Actor-Critic)\n",
    "\n",
    "**Type**: Actor-Critic, On-policy\n",
    "\n",
    "**Best for**: Simple problems, baseline for comparison\n",
    "\n",
    "**Key features**:\n",
    "- Synchronous version of A3C\n",
    "- Simple and fast\n",
    "- Good for understanding actor-critic methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C Configuration\n",
    "a2c_config = (\n",
    "    A2CConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        train_batch_size=500,\n",
    "        # A2C specific\n",
    "        vf_loss_coeff=0.5,\n",
    "        entropy_coeff=0.01,\n",
    "        use_gae=True,\n",
    "        lambda_=0.95,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"A2C config created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Selection Guide\n",
    "\n",
    "| Scenario | Recommended Algorithm |\n",
    "|----------|----------------------|\n",
    "| Discrete actions, need sample efficiency | DQN |\n",
    "| Continuous actions, sample efficiency critical | SAC |\n",
    "| General purpose, stability important | PPO |\n",
    "| Simple baseline | A2C |\n",
    "| Multi-agent setting | PPO, QMIX |\n",
    "| Offline RL (fixed dataset) | CQL, BCQ |\n",
    "| Image observations | DQN (with CNN), PPO (with CNN) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(config, name, n_iters=20):\n",
    "    \"\"\"Train an algorithm and return learning curve.\"\"\"\n",
    "    algo = config.build()\n",
    "    rewards = []\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        result = algo.train()\n",
    "        reward = result[\"env_runners\"][\"episode_reward_mean\"]\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"{name} - Iter {i+1}: {reward:.2f}\")\n",
    "    \n",
    "    algo.stop()\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PPO vs DQN vs A2C on CartPole\n",
    "# Note: This may take several minutes\n",
    "\n",
    "print(\"Training PPO...\")\n",
    "ppo_rewards = train_and_evaluate(ppo_config, \"PPO\", n_iters=20)\n",
    "\n",
    "print(\"\\nTraining DQN...\")\n",
    "dqn_rewards = train_and_evaluate(dqn_config, \"DQN\", n_iters=20)\n",
    "\n",
    "print(\"\\nTraining A2C...\")\n",
    "a2c_rewards = train_and_evaluate(a2c_config, \"A2C\", n_iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ppo_rewards, label='PPO', linewidth=2)\n",
    "plt.plot(dqn_rewards, label='DQN', linewidth=2)\n",
    "plt.plot(a2c_rewards, label='A2C', linewidth=2)\n",
    "plt.axhline(y=475, color='gray', linestyle='--', label='Solved')\n",
    "plt.xlabel('Training Iteration')\n",
    "plt.ylabel('Mean Episode Reward')\n",
    "plt.title('Algorithm Comparison on CartPole-v1')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-Policy vs Off-Policy\n",
    "\n",
    "### On-Policy (PPO, A2C)\n",
    "- Uses data from current policy only\n",
    "- More stable, but less sample efficient\n",
    "- Better for parallel data collection\n",
    "\n",
    "### Off-Policy (DQN, SAC)\n",
    "- Can reuse old experience (replay buffer)\n",
    "- More sample efficient\n",
    "- Can be less stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate off-policy sample efficiency with SAC on Pendulum\n",
    "print(\"Training SAC on Pendulum-v1...\")\n",
    "\n",
    "sac_algo = sac_config.build()\n",
    "sac_rewards = []\n",
    "\n",
    "for i in range(30):\n",
    "    result = sac_algo.train()\n",
    "    reward = result[\"env_runners\"][\"episode_reward_mean\"]\n",
    "    sac_rewards.append(reward)\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"SAC Iter {i+1}: {reward:.2f}\")\n",
    "\n",
    "sac_algo.stop()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sac_rewards, linewidth=2)\n",
    "plt.xlabel('Training Iteration')\n",
    "plt.ylabel('Mean Episode Reward')\n",
    "plt.title('SAC on Pendulum-v1 (Continuous Action Space)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Algorithms in RLlib\n",
    "\n",
    "RLlib supports many more algorithms:\n",
    "\n",
    "### Model-Free\n",
    "- **IMPALA**: Distributed actor-critic with V-trace\n",
    "- **APEX-DQN**: Distributed DQN with prioritized experience replay\n",
    "- **TD3**: Twin Delayed DDPG (continuous actions)\n",
    "- **DDPG**: Deep Deterministic Policy Gradient\n",
    "\n",
    "### Multi-Agent\n",
    "- **QMIX**: Q-value mixing for cooperative agents\n",
    "- **MADDPG**: Multi-agent DDPG\n",
    "\n",
    "### Offline RL\n",
    "- **CQL**: Conservative Q-Learning\n",
    "- **MARWIL**: Monotonic Advantage Re-Weighted Imitation Learning\n",
    "\n",
    "### Model-Based\n",
    "- **Dreamer**: World model learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **PPO** is a safe default choice for most problems\n",
    "\n",
    "2. **DQN** excels with discrete actions and when sample efficiency matters\n",
    "\n",
    "3. **SAC** is best for continuous control with its entropy regularization\n",
    "\n",
    "4. **Off-policy** methods are more sample efficient but can be less stable\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next section, we'll learn how to create custom environments for RLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
