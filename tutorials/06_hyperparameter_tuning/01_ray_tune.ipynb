{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Hyperparameter Tuning with Ray Tune\n\n**Prerequisites**: Complete [05_distributed_training](../05_distributed_training/01_scaling_rllib.ipynb)\n\nRL algorithms are VERY sensitive to hyperparameters. Let Ray Tune find the best ones!\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                    WHY HYPERPARAMETER TUNING?                               │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  SAME ALGORITHM, DIFFERENT HYPERPARAMETERS:                                 │\n│                                                                             │\n│  lr=0.01 (too high)        lr=0.0003 (good)         lr=0.00001 (too low)   │\n│                                                                             │\n│  Reward │      /\\          Reward │      ___         Reward │              │\n│         │     /  \\                │     /   \\               │   _____      │\n│         │    /    \\  crash        │    /     \\              │  /           │\n│         │   /      \\___           │   /       \\___          │ /            │\n│         │__/                      │__/                      │/             │\n│         └──────────────           └──────────────           └───────────   │\n│              Iterations                Iterations               Iterations │\n│                                                                             │\n│  Unstable: big updates         Stable: converges!       Slow: barely learns│\n│  destroy learning                                                           │\n│                                                                             │\n│  Finding the RIGHT hyperparameters can be the difference between            │\n│  success and failure!                                                       │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Ray Tune: Hyperparameter Search at Scale\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                          RAY TUNE OVERVIEW                                  │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  Ray Tune runs MANY training trials in PARALLEL:                            │\n│                                                                             │\n│     ┌─────────────────────────────────────────────────────────────────┐    │\n│     │                         SEARCH SPACE                            │    │\n│     │                                                                 │    │\n│     │  lr: [0.00001 ─────────────────────────────────────── 0.01]    │    │\n│     │  gamma: [0.9 ────────────────────────────────────────── 0.999] │    │\n│     │  batch_size: [1000, 2000, 4000, 8000]                          │    │\n│     │                                                                 │    │\n│     └─────────────────────────────────────────────────────────────────┘    │\n│                                   │                                         │\n│               ┌───────────────────┼───────────────────┐                    │\n│               │                   │                   │                    │\n│               v                   v                   v                    │\n│     ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐           │\n│     │    TRIAL 1      │ │    TRIAL 2      │ │    TRIAL 3      │           │\n│     │ lr=0.0005       │ │ lr=0.001        │ │ lr=0.0001       │           │\n│     │ gamma=0.99      │ │ gamma=0.95      │ │ gamma=0.999     │           │\n│     │ batch=4000      │ │ batch=2000      │ │ batch=8000      │           │\n│     │                 │ │                 │ │                 │           │\n│     │ reward: 420     │ │ reward: 180     │ │ reward: 490 *   │           │\n│     └─────────────────┘ └─────────────────┘ └─────────────────┘           │\n│               │                   │                   │                    │\n│               └───────────────────┴───────────────────┘                    │\n│                                   │                                         │\n│                                   v                                         │\n│                        BEST: Trial 3 config!                                │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"ray\").setLevel(logging.ERROR)\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=4,\n",
    "    object_store_memory=1 * 1024 * 1024 * 1024,\n",
    "    ignore_reinit_error=True,\n",
    ")\n",
    "print(f\"Ray initialized: {ray.cluster_resources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Defining Search Spaces\n\nTell Tune what values to try for each hyperparameter.\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                          SEARCH SPACE TYPES                                 │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  tune.uniform(a, b)                                                         │\n│  ──────────────────                                                         │\n│  Uniform between a and b                                                    │\n│  Good for: bounded parameters like gamma                                    │\n│                                                                             │\n│     |████████████████████████|                                              │\n│     a                        b                                              │\n│                                                                             │\n│  ─────────────────────────────────────────────────────────────────────      │\n│                                                                             │\n│  tune.loguniform(a, b)                                                      │\n│  ─────────────────────                                                      │\n│  Log-uniform: samples uniformly in LOG space                                │\n│  Good for: learning rates, coefficients that span orders of magnitude       │\n│                                                                             │\n│     |████████  ████  ██  █ █|                                               │\n│     1e-5              1e-2   (more samples at small values)                 │\n│                                                                             │\n│  ─────────────────────────────────────────────────────────────────────      │\n│                                                                             │\n│  tune.choice([a, b, c])                                                     │\n│  ──────────────────────                                                     │\n│  Pick one from a list                                                       │\n│  Good for: categorical choices like batch sizes                             │\n│                                                                             │\n│     [1000]  [2000]  [4000]  [8000]                                          │\n│                                                                             │\n│  ─────────────────────────────────────────────────────────────────────      │\n│                                                                             │\n│  tune.grid_search([a, b, c])                                                │\n│  ───────────────────────────                                                │\n│  Try EVERY value (exhaustive)                                               │\n│  Good for: small sets you want to fully explore                             │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a search space\n",
    "print(\"SEARCH SPACE EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Learning rate: log-uniform because it spans orders of magnitude\n",
    "print(f\"\\nLearning rate (loguniform 1e-5 to 1e-3):\")\n",
    "for _ in range(5):\n",
    "    print(f\"  {tune.loguniform(1e-5, 1e-3).sample():.6f}\")\n",
    "\n",
    "# Gamma: uniform because it's bounded\n",
    "print(f\"\\nGamma (uniform 0.9 to 0.999):\")\n",
    "for _ in range(5):\n",
    "    print(f\"  {tune.uniform(0.9, 0.999).sample():.4f}\")\n",
    "\n",
    "# Batch size: choice because we want specific values\n",
    "print(f\"\\nBatch size (choice):\")\n",
    "for _ in range(5):\n",
    "    print(f\"  {tune.choice([1000, 2000, 4000, 8000]).sample()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## ASHA Scheduler: Early Stopping\n\nStop bad trials early to save resources!\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                            ASHA SCHEDULER                                   │\n│             (Asynchronous Successive Halving Algorithm)                     │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  IDEA: Don't waste resources on trials that are clearly losing              │\n│                                                                             │\n│  Without ASHA:                      With ASHA:                              │\n│  ──────────────                     ──────────                              │\n│                                                                             │\n│  Run ALL trials to completion       Stop bad trials early                   │\n│                                                                             │\n│  Reward │    * good                 Reward │    * good                      │\n│         │   /                              │   /                            │\n│         │  /  ─── meh                      │  /                             │\n│         │ /  /                             │ /                              │\n│         │/  /  ─── bad                     │/  X stopped!                   │\n│         └──────────────                    └──────────────                  │\n│         0   10  20  30  40  50             0   10  20  30  40  50           │\n│                                                 ^                           │\n│  Wasted compute on bad trials!             Save resources for               │\n│                                            promising trials!                │\n│                                                                             │\n│  HOW IT WORKS:                                                              │\n│  1. grace_period: minimum iterations before stopping                        │\n│  2. Compare trials at checkpoints                                           │\n│  3. Stop bottom performers                                                  │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASHA scheduler configuration\n",
    "asha_scheduler = ASHAScheduler(\n",
    "    metric=\"env_runners/episode_return_mean\",  # What to optimize\n",
    "    mode=\"max\",                                 # Maximize reward\n",
    "    max_t=30,                                   # Max iterations per trial\n",
    "    grace_period=5,                             # Min iterations before stopping\n",
    "    reduction_factor=2,                         # How aggressively to stop\n",
    ")\n",
    "\n",
    "print(\"ASHA Scheduler:\")\n",
    "print(\"  - Stop bad trials after 5 iterations\")\n",
    "print(\"  - Keep running good trials up to 30 iterations\")\n",
    "print(\"  - Reduction factor 2: keep top 50% at each checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Running a Tune Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a config with search space\n",
    "tunable_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=1)  # Keep small for demo\n",
    "    .training(\n",
    "        # These will be SEARCHED\n",
    "        lr=tune.loguniform(1e-5, 1e-3),\n",
    "        gamma=tune.uniform(0.95, 0.999),\n",
    "        train_batch_size=tune.choice([1000, 2000, 4000]),\n",
    "        \n",
    "        # These are fixed\n",
    "        sgd_minibatch_size=128,\n",
    "        num_sgd_iter=10,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Tunable config created with search space:\")\n",
    "print(\"  - lr: loguniform(1e-5, 1e-3)\")\n",
    "print(\"  - gamma: uniform(0.95, 0.999)\")\n",
    "print(\"  - train_batch_size: choice([1000, 2000, 4000])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tuning experiment\n",
    "print(\"Running hyperparameter search...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=tunable_config,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        scheduler=asha_scheduler,\n",
    "        num_samples=6,             # Try 6 different configs\n",
    "        max_concurrent_trials=2,   # Run 2 at a time\n",
    "    ),\n",
    "    run_config=tune.RunConfig(\n",
    "        stop={\"training_iteration\": 20},\n",
    "        verbose=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze results\n",
    "best_result = results.get_best_result(\n",
    "    metric=\"env_runners/episode_return_mean\",\n",
    "    mode=\"max\"\n",
    ")\n",
    "\n",
    "print(\"\\nBEST TRIAL\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Reward: {best_result.metrics['env_runners']['episode_return_mean']:.1f}\")\n",
    "print(f\"\\nBest hyperparameters:\")\n",
    "print(f\"  lr:               {best_result.config['lr']:.6f}\")\n",
    "print(f\"  gamma:            {best_result.config['gamma']:.4f}\")\n",
    "print(f\"  train_batch_size: {best_result.config['train_batch_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Population Based Training (PBT)\n\nThe most powerful scheduler for RL: adapts hyperparameters DURING training!\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                      POPULATION BASED TRAINING                              │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  KEY IDEA: Don't just search, EVOLVE during training                        │\n│                                                                             │\n│  Regular Search:                   PBT:                                     │\n│  ───────────────                   ────                                     │\n│  Fix hyperparams at start          Change hyperparams AS YOU TRAIN          │\n│                                                                             │\n│  ┌──────────────────────────────────────────────────────────────────────┐  │\n│  │                         PBT PROCESS                                  │  │\n│  │                                                                      │  │\n│  │  Population of 8 agents training in parallel:                        │  │\n│  │                                                                      │  │\n│  │  Start:  [A1] [A2] [A3] [A4] [A5] [A6] [A7] [A8]                     │  │\n│  │          all random hyperparameters                                  │  │\n│  │                                                                      │  │\n│  │  After 10 iters, compare rewards:                                    │  │\n│  │          A3 and A5 are doing best!                                   │  │\n│  │                                                                      │  │\n│  │  Exploit: Copy A3's weights to A1, A7 (they were worst)             │  │\n│  │  Explore: Mutate A1, A7's hyperparameters slightly                  │  │\n│  │                                                                      │  │\n│  │  Result: Bad agents get a \"boost\" from good agents,                 │  │\n│  │          and try slightly different hyperparameters                  │  │\n│  │                                                                      │  │\n│  │  Repeat every N iterations!                                          │  │\n│  └──────────────────────────────────────────────────────────────────────┘  │\n│                                                                             │\n│  WHY PBT IS GREAT FOR RL:                                                   │\n│  - Optimal hyperparams CHANGE during training                               │\n│  - Early: high exploration (high entropy, high lr)                          │\n│  - Late: low exploration (low entropy, low lr)                              │\n│  - PBT naturally discovers this schedule!                                   │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PBT scheduler configuration\n",
    "pbt_scheduler = PopulationBasedTraining(\n",
    "    time_attr=\"training_iteration\",\n",
    "    metric=\"env_runners/episode_return_mean\",\n",
    "    mode=\"max\",\n",
    "    perturbation_interval=5,        # Check every 5 iterations\n",
    "    \n",
    "    # Which hyperparameters to mutate\n",
    "    hyperparam_mutations={\n",
    "        \"lr\": tune.loguniform(1e-5, 1e-3),\n",
    "        \"entropy_coeff\": tune.loguniform(1e-4, 1e-2),\n",
    "    },\n",
    "    \n",
    "    quantile_fraction=0.25,  # Bottom 25% exploit top 25%\n",
    "    resample_probability=0.25,  # 25% chance to resample vs perturb\n",
    ")\n",
    "\n",
    "print(\"PBT Scheduler:\")\n",
    "print(\"  - Check every 5 iterations\")\n",
    "print(\"  - Bottom 25% copy from top 25%\")\n",
    "print(\"  - Mutate lr and entropy_coeff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Hyperparameter Sensitivity\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                     HYPERPARAMETER IMPORTANCE                               │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  MOST IMPORTANT (tune first):                                               │\n│  ────────────────────────────                                               │\n│                                                                             │\n│  1. Learning Rate (lr)                                                      │\n│     └─> Try: 1e-5 to 1e-3 (log scale)                                       │\n│     └─> HUGE impact on stability and speed                                  │\n│                                                                             │\n│  2. Batch Size (train_batch_size)                                           │\n│     └─> Try: 1000, 2000, 4000, 8000                                         │\n│     └─> Larger = more stable, slower                                        │\n│                                                                             │\n│  MODERATELY IMPORTANT:                                                      │\n│  ─────────────────────                                                      │\n│                                                                             │\n│  3. Discount Factor (gamma)                                                 │\n│     └─> Try: 0.95 to 0.999                                                  │\n│     └─> Higher = longer horizon, but harder to train                        │\n│                                                                             │\n│  4. Entropy Coefficient (entropy_coeff)                                     │\n│     └─> Try: 0.0001 to 0.1                                                  │\n│     └─> Higher = more exploration                                           │\n│                                                                             │\n│  LESS IMPORTANT (use defaults):                                             │\n│  ──────────────────────────────                                             │\n│                                                                             │\n│  5. GAE Lambda (lambda_)                                                    │\n│     └─> Default 0.95 usually fine                                           │\n│                                                                             │\n│  6. Clip Parameter (clip_param)                                             │\n│     └─> Default 0.2 usually fine                                            │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Best Practices\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                       TUNING BEST PRACTICES                                 │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  1. START WITH KNOWN GOOD DEFAULTS                                          │\n│     ─────────────────────────────────                                       │\n│     PPO defaults that usually work:                                         │\n│       lr = 3e-4                                                             │\n│       gamma = 0.99                                                          │\n│       clip_param = 0.2                                                      │\n│       entropy_coeff = 0.01                                                  │\n│                                                                             │\n│  2. TUNE LEARNING RATE FIRST                                                │\n│     ─────────────────────────────                                           │\n│     It has the biggest impact!                                              │\n│                                                                             │\n│  3. USE LOG-UNIFORM FOR RATES/COEFFICIENTS                                  │\n│     ─────────────────────────────────────                                   │\n│     tune.loguniform(1e-5, 1e-3) not tune.uniform(0, 0.001)                  │\n│                                                                             │\n│  4. USE PBT FOR RL (adapts during training)                                 │\n│     ───────────────────────────────────────                                 │\n│                                                                             │\n│  5. SET REASONABLE STOPPING CRITERIA                                        │\n│     ──────────────────────────────────                                      │\n│     Don't run forever - diminishing returns                                 │\n│                                                                             │\n│  6. USE ASHA FOR QUICK EXPLORATION                                          │\n│     ─────────────────────────────────                                       │\n│     Quickly filter out bad configs                                          │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n1. **tune.loguniform** for learning rates and coefficients\n\n2. **tune.choice** for categorical values like batch sizes\n\n3. **ASHA** for quick exploration with early stopping\n\n4. **PBT** for best RL results (adapts during training)\n\n5. **Learning rate** is usually the most important hyperparameter\n\n## What's Next?\n\n```\n┌──────────────────┐          ┌──────────────────┐          ┌──────────────────┐\n│   06 Ray Tune    │          │  07 Production   │          │ 08 Best Practice │\n│  (you are here)  │   ───>   │                  │   ───>   │                  │\n│                  │          │  Deploy trained  │          │     Industry     │\n│  - Search spaces │          │  policies        │          │     patterns     │\n│  - ASHA, PBT     │          │                  │          │                  │\n└──────────────────┘          └──────────────────┘          └──────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}