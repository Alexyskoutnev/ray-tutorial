{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Hyperparameter Tuning with Ray Tune\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand hyperparameter search strategies\n",
    "- Use Ray Tune for automated tuning\n",
    "- Apply schedulers and search algorithms\n",
    "- Implement Population Based Training (PBT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Hyperparameter Tuning?\n",
    "\n",
    "RL algorithms are sensitive to hyperparameters:\n",
    "\n",
    "| Hyperparameter | Too Low | Too High |\n",
    "|----------------|---------|----------|\n",
    "| Learning rate | Slow learning | Unstable training |\n",
    "| Batch size | High variance | Slow updates |\n",
    "| Discount factor (Î³) | Myopic behavior | Slow credit assignment |\n",
    "| Entropy coefficient | Premature convergence | Random behavior |\n",
    "| Clip parameter (PPO) | Conservative updates | Large policy changes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search space\n",
    "search_space = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-3),           # Log-uniform: good for learning rates\n",
    "    \"gamma\": tune.uniform(0.9, 0.999),            # Uniform: bounded range\n",
    "    \"clip_param\": tune.choice([0.1, 0.2, 0.3]),   # Categorical choice\n",
    "    \"entropy_coeff\": tune.loguniform(1e-4, 1e-2),\n",
    "    \"train_batch_size\": tune.choice([2000, 4000, 8000]),\n",
    "}\n",
    "\n",
    "print(\"Search space defined\")\n",
    "print(f\"Learning rate range: {1e-5} to {1e-3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure PPO with tunable parameters\n",
    "def create_tunable_config():\n",
    "    return (\n",
    "        PPOConfig()\n",
    "        .environment(\"CartPole-v1\")\n",
    "        .framework(\"torch\")\n",
    "        .env_runners(num_env_runners=2)\n",
    "        .training(\n",
    "            lr=tune.loguniform(1e-5, 1e-3),\n",
    "            gamma=tune.uniform(0.9, 0.999),\n",
    "            clip_param=tune.choice([0.1, 0.2, 0.3]),\n",
    "            entropy_coeff=tune.loguniform(1e-4, 1e-2),\n",
    "            train_batch_size=tune.choice([2000, 4000]),\n",
    "            sgd_minibatch_size=128,\n",
    "            num_sgd_iter=tune.choice([5, 10, 20]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "config = create_tunable_config()\n",
    "print(\"Tunable config created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASHA Scheduler (Aggressive Early Stopping)\n",
    "\n",
    "ASHA (Asynchronous Successive Halving Algorithm) stops poorly-performing trials early to focus resources on promising ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASHA scheduler configuration\n",
    "asha_scheduler = ASHAScheduler(\n",
    "    metric=\"env_runners/episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    max_t=50,          # Max iterations\n",
    "    grace_period=10,   # Min iterations before stopping\n",
    "    reduction_factor=2, # Halving factor\n",
    ")\n",
    "\n",
    "# Run tuning with ASHA\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=config,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        scheduler=asha_scheduler,\n",
    "        num_samples=8,  # Number of trials\n",
    "        max_concurrent_trials=4,\n",
    "    ),\n",
    "    run_config=tune.RunConfig(\n",
    "        stop={\"training_iteration\": 50},\n",
    "        checkpoint_config=tune.CheckpointConfig(\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# results = tuner.fit()  # Uncomment to run\n",
    "print(\"ASHA tuner configured (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna search algorithm\n",
    "optuna_search = OptunaSearch(\n",
    "    metric=\"env_runners/episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "# Combined with ASHA for efficiency\n",
    "tuner_optuna = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=config,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        search_alg=optuna_search,\n",
    "        scheduler=asha_scheduler,\n",
    "        num_samples=16,\n",
    "    ),\n",
    "    run_config=tune.RunConfig(\n",
    "        stop={\"training_iteration\": 50},\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Optuna + ASHA tuner configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population Based Training (PBT)\n",
    "\n",
    "PBT combines hyperparameter tuning with training:\n",
    "- Trains a population of agents in parallel\n",
    "- Periodically copies weights from best performers\n",
    "- Mutates hyperparameters to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PBT scheduler\n",
    "pbt_scheduler = PopulationBasedTraining(\n",
    "    time_attr=\"training_iteration\",\n",
    "    metric=\"env_runners/episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    perturbation_interval=5,  # Check every 5 iterations\n",
    "    hyperparam_mutations={\n",
    "        # Parameters to mutate\n",
    "        \"lr\": tune.loguniform(1e-5, 1e-3),\n",
    "        \"entropy_coeff\": tune.loguniform(1e-4, 1e-2),\n",
    "        \"clip_param\": [0.1, 0.2, 0.3],\n",
    "    },\n",
    "    quantile_fraction=0.25,  # Bottom 25% exploit top 25%\n",
    "    resample_probability=0.25,  # 25% chance to resample vs perturb\n",
    ")\n",
    "\n",
    "# PBT requires fixed initial hyperparameters\n",
    "pbt_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        clip_param=0.2,\n",
    "        entropy_coeff=0.01,\n",
    "        train_batch_size=4000,\n",
    "    )\n",
    ")\n",
    "\n",
    "tuner_pbt = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=pbt_config,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        scheduler=pbt_scheduler,\n",
    "        num_samples=8,  # Population size\n",
    "    ),\n",
    "    run_config=tune.RunConfig(\n",
    "        stop={\"training_iteration\": 50},\n",
    "        checkpoint_config=tune.CheckpointConfig(\n",
    "            checkpoint_frequency=5,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"PBT tuner configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Tuning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"Analyze tuning results.\"\"\"\n",
    "    # Get best result\n",
    "    best_result = results.get_best_result(\n",
    "        metric=\"env_runners/episode_reward_mean\",\n",
    "        mode=\"max\"\n",
    "    )\n",
    "    \n",
    "    print(\"Best Trial:\")\n",
    "    print(f\"  Reward: {best_result.metrics['env_runners']['episode_reward_mean']:.2f}\")\n",
    "    print(f\"  Config: {best_result.config}\")\n",
    "    \n",
    "    # Get all results as dataframe\n",
    "    df = results.get_dataframe()\n",
    "    \n",
    "    # Plot hyperparameter importance\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    params = ['lr', 'gamma', 'clip_param', 'entropy_coeff']\n",
    "    for ax, param in zip(axes.flat, params):\n",
    "        if param in df.columns:\n",
    "            ax.scatter(df[param], df['env_runners/episode_reward_mean'])\n",
    "            ax.set_xlabel(param)\n",
    "            ax.set_ylabel('Reward')\n",
    "            ax.set_title(f'{param} vs Reward')\n",
    "            if param == 'lr':\n",
    "                ax.set_xscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_result\n",
    "\n",
    "# Example usage (with actual results):\n",
    "# best = analyze_results(results)\n",
    "print(\"Analysis function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Complete Tuning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small-scale tuning example (runs quickly)\n",
    "quick_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .training(\n",
    "        lr=tune.grid_search([1e-4, 3e-4, 1e-3]),  # Grid search\n",
    "        train_batch_size=2000,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run small grid search\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=quick_config,\n",
    "    run_config=tune.RunConfig(\n",
    "        stop={\"training_iteration\": 10},\n",
    "        verbose=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Running quick grid search...\")\n",
    "results = tuner.fit()\n",
    "\n",
    "# Analyze\n",
    "best = results.get_best_result(\n",
    "    metric=\"env_runners/episode_reward_mean\",\n",
    "    mode=\"max\"\n",
    ")\n",
    "print(f\"\\nBest learning rate: {best.config['lr']}\")\n",
    "print(f\"Best reward: {best.metrics['env_runners']['episode_reward_mean']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for RL Hyperparameter Tuning\n",
    "\n",
    "### 1. Start with known good defaults\n",
    "```python\n",
    "# PPO defaults that often work\n",
    "lr = 3e-4\n",
    "gamma = 0.99\n",
    "clip_param = 0.2\n",
    "entropy_coeff = 0.01\n",
    "```\n",
    "\n",
    "### 2. Tune learning rate first (most impactful)\n",
    "\n",
    "### 3. Use log-uniform for rates/coefficients\n",
    "\n",
    "### 4. PBT is best for RL (adapts during training)\n",
    "\n",
    "### 5. Always set reasonable stopping criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready tuning template\n",
    "def production_tune(env_name, num_samples=32, max_iters=100):\n",
    "    \"\"\"Production hyperparameter tuning setup.\"\"\"\n",
    "    \n",
    "    config = (\n",
    "        PPOConfig()\n",
    "        .environment(env_name)\n",
    "        .framework(\"torch\")\n",
    "        .env_runners(num_env_runners=4)\n",
    "        .training(\n",
    "            lr=tune.loguniform(1e-5, 1e-3),\n",
    "            gamma=tune.uniform(0.95, 0.999),\n",
    "            clip_param=tune.uniform(0.1, 0.3),\n",
    "            entropy_coeff=tune.loguniform(1e-4, 1e-1),\n",
    "            vf_loss_coeff=tune.uniform(0.5, 1.0),\n",
    "            train_batch_size=tune.choice([4000, 8000, 16000]),\n",
    "            sgd_minibatch_size=tune.choice([128, 256, 512]),\n",
    "            num_sgd_iter=tune.choice([5, 10, 20, 30]),\n",
    "            lambda_=tune.uniform(0.9, 1.0),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    scheduler = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"env_runners/episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        perturbation_interval=10,\n",
    "        hyperparam_mutations={\n",
    "            \"lr\": tune.loguniform(1e-5, 1e-3),\n",
    "            \"entropy_coeff\": tune.loguniform(1e-4, 1e-1),\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    tuner = tune.Tuner(\n",
    "        \"PPO\",\n",
    "        param_space=config,\n",
    "        tune_config=tune.TuneConfig(\n",
    "            scheduler=scheduler,\n",
    "            num_samples=num_samples,\n",
    "        ),\n",
    "        run_config=tune.RunConfig(\n",
    "            stop={\"training_iteration\": max_iters},\n",
    "            checkpoint_config=tune.CheckpointConfig(\n",
    "                checkpoint_frequency=10,\n",
    "                checkpoint_at_end=True,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return tuner\n",
    "\n",
    "print(\"Production tuning template defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **ASHA** is efficient for quick exploration with early stopping\n",
    "\n",
    "2. **Optuna/HyperOpt** provide intelligent Bayesian search\n",
    "\n",
    "3. **PBT** is ideal for RL (adapts hyperparameters during training)\n",
    "\n",
    "4. **Learning rate** is usually the most important hyperparameter\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next section, we'll cover production deployment strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
