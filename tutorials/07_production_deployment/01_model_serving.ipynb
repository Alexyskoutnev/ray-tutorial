{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Deployment with Ray Serve\n",
    "\n",
    "**Prerequisites**: Complete [06_hyperparameter_tuning](../06_hyperparameter_tuning/01_ray_tune.ipynb)\n",
    "\n",
    "You've trained a great policy. Now let's deploy it!\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                     FROM TRAINING TO PRODUCTION                             │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│  TRAINING                                PRODUCTION                        │\n",
    "│  ────────                                ──────────                        │\n",
    "│                                                                             │\n",
    "│  ┌─────────────┐                         ┌─────────────┐                   │\n",
    "│  │   Train     │                         │   Serve     │                   │\n",
    "│  │   Loop      │  ────> checkpoint ────> │   API       │                   │\n",
    "│  │             │                         │             │                   │\n",
    "│  └─────────────┘                         └─────────────┘                   │\n",
    "│        │                                        │                          │\n",
    "│        v                                        v                          │\n",
    "│   Maximize reward                         Minimize latency                 │\n",
    "│   Explore safely                          Serve reliably                   │\n",
    "│   Takes hours                             Responds in ms                   │\n",
    "│                                                                             │\n",
    "│  DIFFERENT GOALS:                                                          │\n",
    "│  Training: Learn the best policy                                           │\n",
    "│  Serving: Use the policy fast and reliably                                 │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Serve: Scalable Model Serving\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                          RAY SERVE ARCHITECTURE                             │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│     Client Request                                                         │\n",
    "│          │                                                                  │\n",
    "│          v                                                                  │\n",
    "│   ┌─────────────────────────────────────────────────────────────────────┐  │\n",
    "│   │                        HTTP PROXY                                   │  │\n",
    "│   │                    (handles routing)                                │  │\n",
    "│   └────────────────────────────┬────────────────────────────────────────┘  │\n",
    "│                                │                                           │\n",
    "│         ┌──────────────────────┼──────────────────────┐                   │\n",
    "│         │                      │                      │                   │\n",
    "│         v                      v                      v                   │\n",
    "│  ┌─────────────┐       ┌─────────────┐       ┌─────────────┐             │\n",
    "│  │  Replica 1  │       │  Replica 2  │       │  Replica 3  │             │\n",
    "│  │             │       │             │       │             │             │\n",
    "│  │ ┌─────────┐ │       │ ┌─────────┐ │       │ ┌─────────┐ │             │\n",
    "│  │ │ Policy  │ │       │ │ Policy  │ │       │ │ Policy  │ │             │\n",
    "│  │ │ (copy)  │ │       │ │ (copy)  │ │       │ │ (copy)  │ │             │\n",
    "│  │ └─────────┘ │       │ └─────────┘ │       │ └─────────┘ │             │\n",
    "│  └─────────────┘       └─────────────┘       └─────────────┘             │\n",
    "│                                                                            │\n",
    "│  KEY FEATURES:                                                            │\n",
    "│  • Auto-scaling: Add/remove replicas based on load                        │\n",
    "│  • Batching: Combine requests for GPU efficiency                          │\n",
    "│  • A/B Testing: Route traffic to different model versions                 │\n",
    "│  • Zero-downtime: Update models without service interruption              │\n",
    "│                                                                            │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"ray\").setLevel(logging.ERROR)\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Dict\n",
    "\n",
    "ray.init(\n",
    "    num_cpus=4,\n",
    "    object_store_memory=1 * 1024 * 1024 * 1024,\n",
    "    ignore_reinit_error=True,\n",
    ")\n",
    "print(f\"Ray initialized: {ray.cluster_resources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Train and Save a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a quick model for demo\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        lr=3e-4,\n",
    "        train_batch_size=2000,\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build_algo()\n",
    "\n",
    "print(\"Training model...\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(5):\n",
    "    result = algo.train()\n",
    "    reward = result[\"env_runners\"][\"episode_return_mean\"]\n",
    "    print(f\"Iter {i+1}: Reward = {reward:.1f}\")\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint_path = algo.save()\n",
    "print(f\"\\nCheckpoint saved to: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Create a Serve Deployment\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         DEPLOYMENT ANATOMY                                  │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│  @serve.deployment(                                                        │\n",
    "│      num_replicas=2,           # How many copies to run                   │\n",
    "│      ray_actor_options={       # Resources per replica                    │\n",
    "│          \"num_cpus\": 1,                                                   │\n",
    "│          \"num_gpus\": 0,                                                   │\n",
    "│      },                                                                    │\n",
    "│  )                                                                         │\n",
    "│  class PolicyServer:                                                       │\n",
    "│                                                                             │\n",
    "│      def __init__(self, checkpoint_path):                                  │\n",
    "│          # Load model once at startup                                      │\n",
    "│          self.algo = Algorithm.from_checkpoint(checkpoint_path)           │\n",
    "│                                                                             │\n",
    "│      async def __call__(self, request):                                    │\n",
    "│          # Handle inference request                                        │\n",
    "│          obs = await request.json()                                        │\n",
    "│          action = self.algo.compute_single_action(obs)                    │\n",
    "│          return {\"action\": action}                                        │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(\n",
    "    num_replicas=1,  # Start with 1 for demo\n",
    "    ray_actor_options={\"num_cpus\": 1},\n",
    ")\n",
    "class PolicyServer:\n",
    "    \"\"\"Ray Serve deployment for RL policy inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_path: str):\n",
    "        \"\"\"Load the trained algorithm from checkpoint.\"\"\"\n",
    "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "        self.algo = Algorithm.from_checkpoint(checkpoint_path)\n",
    "        self.request_count = 0\n",
    "        print(\"Policy loaded and ready!\")\n",
    "    \n",
    "    async def __call__(self, request) -> Dict:\n",
    "        \"\"\"Handle inference request.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Parse request\n",
    "        data = await request.json()\n",
    "        observation = np.array(data[\"observation\"])\n",
    "        \n",
    "        # Get action from policy\n",
    "        action = self.algo.compute_single_action(observation)\n",
    "        \n",
    "        # Track metrics\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        self.request_count += 1\n",
    "        \n",
    "        return {\n",
    "            \"action\": int(action),\n",
    "            \"latency_ms\": round(latency_ms, 2),\n",
    "        }\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"Return server metrics.\"\"\"\n",
    "        return {\"request_count\": self.request_count}\n",
    "\n",
    "print(\"PolicyServer deployment class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the server\n",
    "serve.start()\n",
    "\n",
    "# Bind the checkpoint path and deploy\n",
    "deployment = PolicyServer.bind(checkpoint_path)\n",
    "handle = serve.run(deployment, name=\"rl-policy\")\n",
    "\n",
    "print(\"\\nServer deployed!\")\n",
    "print(\"Endpoint: http://localhost:8000/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test the Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Get a sample observation\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs, _ = env.reset()\n",
    "env.close()\n",
    "\n",
    "print(\"Testing deployed policy...\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Observation: {obs}\")\n",
    "\n",
    "# Send request to server\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/\",\n",
    "    json={\"observation\": obs.tolist()}\n",
    ")\n",
    "\n",
    "result = response.json()\n",
    "print(f\"\\nResponse: {result}\")\n",
    "print(f\"  Action: {result['action']}\")\n",
    "print(f\"  Latency: {result['latency_ms']:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a full episode through the server\n",
    "print(\"\\nRunning full episode via API...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs, _ = env.reset()\n",
    "total_reward = 0\n",
    "steps = 0\n",
    "total_latency = 0\n",
    "\n",
    "while True:\n",
    "    # Get action from server\n",
    "    response = requests.post(\n",
    "        \"http://localhost:8000/\",\n",
    "        json={\"observation\": obs.tolist()}\n",
    "    )\n",
    "    result = response.json()\n",
    "    action = result[\"action\"]\n",
    "    total_latency += result[\"latency_ms\"]\n",
    "    \n",
    "    # Step environment\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    steps += 1\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"Episode completed!\")\n",
    "print(f\"  Steps: {steps}\")\n",
    "print(f\"  Total reward: {total_reward}\")\n",
    "print(f\"  Avg latency: {total_latency/steps:.2f} ms/request\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## A/B Testing: Deploy Multiple Versions\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                            A/B TESTING                                      │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  Route traffic between model versions to test safely:                       │\n│                                                                             │\n│     Client Request                                                          │\n│          │                                                                  │\n│          v                                                                  │\n│   ┌─────────────────────────────────────────────────┐                       │\n│   │                   ROUTER                        │                       │\n│   │                                                 │                       │\n│   │  if random() < 0.9:   # 90% of traffic          │                       │\n│   │      use Policy A (production)                  │                       │\n│   │  else:                # 10% of traffic          │                       │\n│   │      use Policy B (canary)                      │                       │\n│   │                                                 │                       │\n│   └──────────────────┬──────────────────────────────┘                       │\n│                      │                                                      │\n│           ┌──────────┴──────────┐                                           │\n│           │                     │                                           │\n│           v                     v                                           │\n│   ┌─────────────────┐   ┌─────────────────┐                                 │\n│   │   Policy A      │   │   Policy B      │                                 │\n│   │  (production)   │   │   (canary)      │                                 │\n│   │                 │   │                 │                                 │\n│   │   90% traffic   │   │   10% traffic   │                                 │\n│   └─────────────────┘   └─────────────────┘                                 │\n│                                                                             │\n│  WHY A/B TEST?                                                              │\n│  - Test new models safely with small traffic percentage                     │\n│  - Compare performance metrics between versions                             │\n│  - Roll back instantly if new model performs worse                          │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Monitoring Your Deployment\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                         WHAT TO MONITOR                                     │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  LATENCY                                                                    │\n│  ───────                                                                    │\n│  - p50, p95, p99 latencies                                                  │\n│  - Alert if p99 > threshold                                                 │\n│                                                                             │\n│     Latency │                                                               │\n│       (ms)  │  ___    ___    ___                                            │\n│        100  │ /   \\  /   \\  /   \\   <-- spikes need investigation           │\n│         50  │/     \\/     \\/     \\___                                       │\n│             └─────────────────────────                                      │\n│                    Time                                                     │\n│                                                                             │\n│  ────────────────────────────────────────────────────────────────────       │\n│                                                                             │\n│  THROUGHPUT                                                                 │\n│  ──────────                                                                 │\n│  - Requests per second                                                      │\n│  - Scale replicas if throughput < capacity                                  │\n│                                                                             │\n│  ────────────────────────────────────────────────────────────────────       │\n│                                                                             │\n│  ERRORS                                                                     │\n│  ──────                                                                     │\n│  - Error rate (should be < 0.1%)                                            │\n│  - Track by error type                                                      │\n│                                                                             │\n│  ────────────────────────────────────────────────────────────────────       │\n│                                                                             │\n│  BUSINESS METRICS (RL-specific)                                             │\n│  ──────────────────────────────                                             │\n│  - Average reward in production                                             │\n│  - Distribution shift detection                                             │\n│  - Action distribution (is it reasonable?)                                  │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import statistics\n",
    "\n",
    "class PolicyMonitor:\n",
    "    \"\"\"Simple monitoring for deployed policies.\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int = 100):\n",
    "        self.latencies = deque(maxlen=window_size)\n",
    "        self.actions = deque(maxlen=window_size)\n",
    "        self.errors = deque(maxlen=window_size)\n",
    "    \n",
    "    def record(self, latency_ms: float, action: int, error: bool = False):\n",
    "        \"\"\"Record a request.\"\"\"\n",
    "        self.latencies.append(latency_ms)\n",
    "        self.actions.append(action)\n",
    "        self.errors.append(error)\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get monitoring statistics.\"\"\"\n",
    "        if not self.latencies:\n",
    "            return {\"status\": \"no_data\"}\n",
    "        \n",
    "        sorted_latencies = sorted(self.latencies)\n",
    "        n = len(sorted_latencies)\n",
    "        \n",
    "        # Action distribution\n",
    "        action_counts = {}\n",
    "        for a in self.actions:\n",
    "            action_counts[a] = action_counts.get(a, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            \"requests\": n,\n",
    "            \"latency_p50_ms\": sorted_latencies[n // 2],\n",
    "            \"latency_p95_ms\": sorted_latencies[int(n * 0.95)],\n",
    "            \"latency_mean_ms\": statistics.mean(self.latencies),\n",
    "            \"error_rate\": sum(self.errors) / len(self.errors),\n",
    "            \"action_distribution\": action_counts,\n",
    "        }\n",
    "\n",
    "# Test the monitor\n",
    "monitor = PolicyMonitor()\n",
    "\n",
    "# Simulate some requests\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs, _ = env.reset()\n",
    "\n",
    "for _ in range(50):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:8000/\",\n",
    "        json={\"observation\": obs.tolist()}\n",
    "    )\n",
    "    result = response.json()\n",
    "    monitor.record(result[\"latency_ms\"], result[\"action\"])\n",
    "    \n",
    "    obs, _, done, _, _ = env.step(result[\"action\"])\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"Monitoring Stats:\")\n",
    "print(\"=\" * 50)\n",
    "stats = monitor.get_stats()\n",
    "print(f\"  Requests: {stats['requests']}\")\n",
    "print(f\"  Latency p50: {stats['latency_p50_ms']:.2f} ms\")\n",
    "print(f\"  Latency p95: {stats['latency_p95_ms']:.2f} ms\")\n",
    "print(f\"  Error rate: {stats['error_rate']:.2%}\")\n",
    "print(f\"  Actions: {stats['action_distribution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoints and Model Registry\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                        MODEL VERSIONING                                     │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│  Track model versions like code versions!                                  │\n",
    "│                                                                             │\n",
    "│  ┌─────────────────────────────────────────────────────────────────────┐   │\n",
    "│  │                       MODEL REGISTRY                                │   │\n",
    "│  ├─────────────────────────────────────────────────────────────────────┤   │\n",
    "│  │  Name          │ Version │ Status     │ Reward │ Created           │   │\n",
    "│  ├─────────────────┼─────────┼────────────┼────────┼───────────────────┤   │\n",
    "│  │  cartpole_ppo  │ v1      │ archived   │ 180    │ 2024-01-01        │   │\n",
    "│  │  cartpole_ppo  │ v2      │ production │ 450    │ 2024-01-15  ★     │   │\n",
    "│  │  cartpole_ppo  │ v3      │ staging    │ 480    │ 2024-02-01        │   │\n",
    "│  └─────────────────┴─────────┴────────────┴────────┴───────────────────┘   │\n",
    "│                                                                             │\n",
    "│  WORKFLOW:                                                                 │\n",
    "│  1. Train new model → Save checkpoint                                      │\n",
    "│  2. Register as \"staging\"                                                  │\n",
    "│  3. A/B test against production                                           │\n",
    "│  4. If better, promote to \"production\"                                    │\n",
    "│  5. Archive old version                                                    │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n1. **Ray Serve** provides scalable model serving with auto-scaling\n\n2. **Load from checkpoint** in deployment `__init__` for fast startup\n\n3. **Monitor latency, throughput, and errors** in production\n\n4. **A/B test** new models safely before full rollout\n\n5. **Version your models** like you version your code\n\n## What's Next?\n\n```\n┌──────────────────┐          ┌──────────────────┐\n│  07 Production   │          │ 08 Best Practice │\n│  (you are here)  │   ───>   │                  │\n│                  │          │  Industry        │\n│  - Ray Serve     │          │  patterns for    │\n│  - Monitoring    │          │  real-world RL   │\n│  - A/B testing   │          │                  │\n└──────────────────┘          └──────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "serve.shutdown()\n",
    "algo.stop()\n",
    "ray.shutdown()\n",
    "print(\"Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}