{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Production Model Serving\n",
    "\n",
    "## Learning Objectives\n",
    "- Export trained models for production\n",
    "- Deploy with Ray Serve for scalable inference\n",
    "- Implement A/B testing and canary deployments\n",
    "- Monitor model performance in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve\n",
    "from ray.rllib.algorithms.ppo import PPOConfig, PPO\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import requests\n",
    "\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production RL Pipeline\n",
    "\n",
    "```\n",
    "┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n",
    "│   Train     │───▶│   Export    │───▶│   Deploy    │───▶│   Monitor   │\n",
    "│   Model     │    │   Model     │    │   Serve     │    │   Metrics   │\n",
    "└─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘\n",
    "                          │                   │                  │\n",
    "                          ▼                   ▼                  ▼\n",
    "                    Checkpoint           REST API           Dashboards\n",
    "                    ONNX Export          gRPC              Alerting\n",
    "                    TorchScript          Batching           A/B Tests\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Exporting a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        lr=3e-4,\n",
    "        train_batch_size=4000,\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build()\n",
    "\n",
    "# Train for a few iterations\n",
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "    print(f\"Iter {i+1}: Reward = {result['env_runners']['episode_reward_mean']:.2f}\")\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint_path = algo.save()\n",
    "print(f\"\\nModel saved to: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export policy for inference\n",
    "policy = algo.get_policy()\n",
    "model = policy.model\n",
    "\n",
    "# For PyTorch models, you can export to TorchScript\n",
    "import torch\n",
    "\n",
    "# Get a sample observation for tracing\n",
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "sample_obs, _ = env.reset()\n",
    "\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Sample observation shape: {sample_obs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Serve Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(\n",
    "    num_replicas=2,\n",
    "    ray_actor_options={\"num_cpus\": 1},\n",
    ")\n",
    "class RLPolicyServer:\n",
    "    \"\"\"\n",
    "    Ray Serve deployment for RL policy inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_path: str):\n",
    "        # Load the trained algorithm\n",
    "        self.algo = Algorithm.from_checkpoint(checkpoint_path)\n",
    "        self.request_count = 0\n",
    "        self.total_latency = 0\n",
    "    \n",
    "    async def __call__(self, request) -> Dict:\n",
    "        \"\"\"Handle inference request.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Parse request\n",
    "        data = await request.json()\n",
    "        observation = np.array(data[\"observation\"])\n",
    "        \n",
    "        # Get action from policy\n",
    "        action = self.algo.compute_single_action(observation)\n",
    "        \n",
    "        # Track metrics\n",
    "        latency = time.time() - start_time\n",
    "        self.request_count += 1\n",
    "        self.total_latency += latency\n",
    "        \n",
    "        return {\n",
    "            \"action\": int(action) if isinstance(action, (np.integer, int)) else action.tolist(),\n",
    "            \"latency_ms\": latency * 1000,\n",
    "        }\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"Return server metrics.\"\"\"\n",
    "        avg_latency = self.total_latency / max(1, self.request_count)\n",
    "        return {\n",
    "            \"request_count\": self.request_count,\n",
    "            \"avg_latency_ms\": avg_latency * 1000,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the server\n",
    "serve.start()\n",
    "\n",
    "# Create deployment with checkpoint path\n",
    "deployment = RLPolicyServer.bind(checkpoint_path)\n",
    "handle = serve.run(deployment, name=\"rl-policy\")\n",
    "\n",
    "print(\"Server deployed!\")\n",
    "print(\"Endpoint: http://localhost:8000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the deployed server\n",
    "import requests\n",
    "\n",
    "# Send inference request\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/\",\n",
    "    json={\"observation\": sample_obs.tolist()}\n",
    ")\n",
    "\n",
    "result = response.json()\n",
    "print(f\"Response: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Inference for High Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(\n",
    "    num_replicas=2,\n",
    "    max_ongoing_requests=100,\n",
    ")\n",
    "class BatchedPolicyServer:\n",
    "    \"\"\"\n",
    "    Policy server with batched inference for high throughput.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_path: str):\n",
    "        self.algo = Algorithm.from_checkpoint(checkpoint_path)\n",
    "    \n",
    "    @serve.batch(max_batch_size=32, batch_wait_timeout_s=0.01)\n",
    "    async def batched_inference(self, observations: List[np.ndarray]) -> List[Dict]:\n",
    "        \"\"\"Process multiple observations in a batch.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Stack observations for batch inference\n",
    "        batch_obs = np.stack(observations)\n",
    "        \n",
    "        # Batch compute actions\n",
    "        actions = self.algo.compute_actions(\n",
    "            {\"default_policy\": batch_obs}\n",
    "        )[\"default_policy\"]\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Return individual results\n",
    "        return [\n",
    "            {\"action\": int(a), \"batch_size\": len(observations), \"latency_ms\": latency * 1000}\n",
    "            for a in actions\n",
    "        ]\n",
    "    \n",
    "    async def __call__(self, request) -> Dict:\n",
    "        data = await request.json()\n",
    "        observation = np.array(data[\"observation\"])\n",
    "        return await self.batched_inference(observation)\n",
    "\n",
    "print(\"Batched server class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B Testing and Canary Deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class PolicyRouter:\n",
    "    \"\"\"\n",
    "    Route requests between multiple policy versions for A/B testing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, policy_a, policy_b, traffic_split: float = 0.9):\n",
    "        self.policy_a = policy_a  # Production policy (90%)\n",
    "        self.policy_b = policy_b  # Canary policy (10%)\n",
    "        self.traffic_split = traffic_split\n",
    "        \n",
    "        # Metrics\n",
    "        self.requests_a = 0\n",
    "        self.requests_b = 0\n",
    "    \n",
    "    async def __call__(self, request) -> Dict:\n",
    "        # Route based on traffic split\n",
    "        if np.random.random() < self.traffic_split:\n",
    "            self.requests_a += 1\n",
    "            result = await self.policy_a.__call__(request)\n",
    "            result[\"policy_version\"] = \"A\"\n",
    "        else:\n",
    "            self.requests_b += 1\n",
    "            result = await self.policy_b.__call__(request)\n",
    "            result[\"policy_version\"] = \"B\"\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_traffic_stats(self) -> Dict:\n",
    "        total = self.requests_a + self.requests_b\n",
    "        return {\n",
    "            \"policy_a_requests\": self.requests_a,\n",
    "            \"policy_b_requests\": self.requests_b,\n",
    "            \"policy_a_percentage\": self.requests_a / max(1, total) * 100,\n",
    "            \"policy_b_percentage\": self.requests_b / max(1, total) * 100,\n",
    "        }\n",
    "\n",
    "print(\"A/B testing router defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Versioning and Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class ModelRegistry:\n",
    "    \"\"\"\n",
    "    Simple model registry for versioning RL policies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, registry_path: str = \"/tmp/model_registry\"):\n",
    "        self.registry_path = registry_path\n",
    "        os.makedirs(registry_path, exist_ok=True)\n",
    "        self.metadata_file = os.path.join(registry_path, \"metadata.json\")\n",
    "        self._load_metadata()\n",
    "    \n",
    "    def _load_metadata(self):\n",
    "        if os.path.exists(self.metadata_file):\n",
    "            with open(self.metadata_file, 'r') as f:\n",
    "                self.metadata = json.load(f)\n",
    "        else:\n",
    "            self.metadata = {\"models\": []}\n",
    "    \n",
    "    def _save_metadata(self):\n",
    "        with open(self.metadata_file, 'w') as f:\n",
    "            json.dump(self.metadata, f, indent=2)\n",
    "    \n",
    "    def register_model(\n",
    "        self,\n",
    "        checkpoint_path: str,\n",
    "        model_name: str,\n",
    "        metrics: Dict,\n",
    "        tags: List[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Register a new model version.\"\"\"\n",
    "        version = len([m for m in self.metadata[\"models\"] if m[\"name\"] == model_name]) + 1\n",
    "        version_id = f\"{model_name}_v{version}\"\n",
    "        \n",
    "        model_info = {\n",
    "            \"version_id\": version_id,\n",
    "            \"name\": model_name,\n",
    "            \"version\": version,\n",
    "            \"checkpoint_path\": checkpoint_path,\n",
    "            \"metrics\": metrics,\n",
    "            \"tags\": tags or [],\n",
    "            \"registered_at\": datetime.now().isoformat(),\n",
    "            \"status\": \"staging\",\n",
    "        }\n",
    "        \n",
    "        self.metadata[\"models\"].append(model_info)\n",
    "        self._save_metadata()\n",
    "        \n",
    "        print(f\"Registered model: {version_id}\")\n",
    "        return version_id\n",
    "    \n",
    "    def promote_to_production(self, version_id: str):\n",
    "        \"\"\"Promote a model to production status.\"\"\"\n",
    "        for model in self.metadata[\"models\"]:\n",
    "            if model[\"version_id\"] == version_id:\n",
    "                model[\"status\"] = \"production\"\n",
    "                model[\"promoted_at\"] = datetime.now().isoformat()\n",
    "        self._save_metadata()\n",
    "        print(f\"Promoted {version_id} to production\")\n",
    "    \n",
    "    def get_production_model(self, model_name: str) -> Dict:\n",
    "        \"\"\"Get the current production model.\"\"\"\n",
    "        for model in reversed(self.metadata[\"models\"]):\n",
    "            if model[\"name\"] == model_name and model[\"status\"] == \"production\":\n",
    "                return model\n",
    "        return None\n",
    "    \n",
    "    def list_models(self, model_name: str = None) -> List[Dict]:\n",
    "        \"\"\"List all registered models.\"\"\"\n",
    "        if model_name:\n",
    "            return [m for m in self.metadata[\"models\"] if m[\"name\"] == model_name]\n",
    "        return self.metadata[\"models\"]\n",
    "\n",
    "# Example usage\n",
    "registry = ModelRegistry()\n",
    "version_id = registry.register_model(\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    model_name=\"cartpole_ppo\",\n",
    "    metrics={\"mean_reward\": 450.0, \"episodes\": 1000},\n",
    "    tags=[\"ppo\", \"cartpole\", \"v1\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nRegistered models: {registry.list_models()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import statistics\n",
    "\n",
    "class PolicyMonitor:\n",
    "    \"\"\"\n",
    "    Monitor RL policy performance in production.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int = 1000):\n",
    "        self.window_size = window_size\n",
    "        self.latencies = deque(maxlen=window_size)\n",
    "        self.rewards = deque(maxlen=window_size)\n",
    "        self.actions = deque(maxlen=window_size)\n",
    "        self.errors = deque(maxlen=window_size)\n",
    "        \n",
    "        # Alerting thresholds\n",
    "        self.latency_threshold_ms = 100\n",
    "        self.error_rate_threshold = 0.01\n",
    "    \n",
    "    def record_request(self, latency_ms: float, action: int, reward: float = None, error: bool = False):\n",
    "        \"\"\"Record a single request.\"\"\"\n",
    "        self.latencies.append(latency_ms)\n",
    "        self.actions.append(action)\n",
    "        self.errors.append(error)\n",
    "        if reward is not None:\n",
    "            self.rewards.append(reward)\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"Get current monitoring metrics.\"\"\"\n",
    "        if not self.latencies:\n",
    "            return {\"status\": \"no_data\"}\n",
    "        \n",
    "        metrics = {\n",
    "            \"request_count\": len(self.latencies),\n",
    "            \"latency_p50_ms\": statistics.median(self.latencies),\n",
    "            \"latency_p99_ms\": sorted(self.latencies)[int(len(self.latencies) * 0.99)] if len(self.latencies) >= 100 else max(self.latencies),\n",
    "            \"latency_mean_ms\": statistics.mean(self.latencies),\n",
    "            \"error_rate\": sum(self.errors) / len(self.errors),\n",
    "        }\n",
    "        \n",
    "        if self.rewards:\n",
    "            metrics[\"mean_reward\"] = statistics.mean(self.rewards)\n",
    "        \n",
    "        # Action distribution\n",
    "        action_counts = {}\n",
    "        for a in self.actions:\n",
    "            action_counts[a] = action_counts.get(a, 0) + 1\n",
    "        metrics[\"action_distribution\"] = action_counts\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def check_alerts(self) -> List[str]:\n",
    "        \"\"\"Check for alerting conditions.\"\"\"\n",
    "        alerts = []\n",
    "        metrics = self.get_metrics()\n",
    "        \n",
    "        if metrics.get(\"latency_p99_ms\", 0) > self.latency_threshold_ms:\n",
    "            alerts.append(f\"HIGH_LATENCY: p99 latency {metrics['latency_p99_ms']:.1f}ms > {self.latency_threshold_ms}ms\")\n",
    "        \n",
    "        if metrics.get(\"error_rate\", 0) > self.error_rate_threshold:\n",
    "            alerts.append(f\"HIGH_ERROR_RATE: {metrics['error_rate']:.2%} > {self.error_rate_threshold:.2%}\")\n",
    "        \n",
    "        return alerts\n",
    "\n",
    "# Example usage\n",
    "monitor = PolicyMonitor()\n",
    "\n",
    "# Simulate some requests\n",
    "for _ in range(100):\n",
    "    monitor.record_request(\n",
    "        latency_ms=np.random.exponential(5),\n",
    "        action=np.random.choice([0, 1]),\n",
    "        reward=np.random.uniform(-1, 1),\n",
    "        error=np.random.random() < 0.005\n",
    "    )\n",
    "\n",
    "print(\"Monitoring metrics:\")\n",
    "print(json.dumps(monitor.get_metrics(), indent=2))\n",
    "print(f\"\\nAlerts: {monitor.check_alerts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop serve and cleanup\n",
    "serve.shutdown()\n",
    "algo.stop()\n",
    "ray.shutdown()\n",
    "\n",
    "print(\"Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Ray Serve** provides scalable, production-ready model serving\n",
    "\n",
    "2. **Batch inference** improves throughput significantly\n",
    "\n",
    "3. **A/B testing** and canary deployments reduce risk\n",
    "\n",
    "4. **Model registry** tracks versions and promotes to production\n",
    "\n",
    "5. **Monitoring** is essential for production RL systems\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the final section, we'll cover industry patterns and best practices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
