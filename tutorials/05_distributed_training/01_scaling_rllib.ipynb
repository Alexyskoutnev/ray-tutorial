{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Scaling RLlib: From Laptop to Cluster\n\n**Prerequisites**: Complete [02_rllib_basics](../02_rllib_basics/01_ray_setup.ipynb) and [03_custom_environments](../03_custom_environments/01_gymnasium_envs.ipynb)\n\nRLlib is built on Ray, which means you can scale from 1 CPU to thousands with minimal code changes!\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                        WHY DISTRIBUTED RL?                                  │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  RL needs MILLIONS of environment interactions.                             │\n│                                                                             │\n│  1 environment:                     16 parallel environments:               │\n│  ──────────────                     ─────────────────────────               │\n│                                                                             │\n│  ┌───────────┐                      ┌───┐ ┌───┐ ┌───┐ ┌───┐                │\n│  │           │                      │Env│ │Env│ │Env│ │Env│                │\n│  │    Env    │                      └─┬─┘ └─┬─┘ └─┬─┘ └─┬─┘                │\n│  │           │                      ┌───┐ ┌───┐ ┌───┐ ┌───┐                │\n│  └─────┬─────┘                      │Env│ │Env│ │Env│ │Env│                │\n│        │                            └─┬─┘ └─┬─┘ └─┬─┘ └─┬─┘                │\n│        v                            ┌───┐ ┌───┐ ┌───┐ ┌───┐                │\n│   1000 steps/sec                    │Env│ │Env│ │Env│ │Env│                │\n│                                     └─┬─┘ └─┬─┘ └─┬─┘ └─┬─┘                │\n│   1M steps = 17 minutes             ┌───┐ ┌───┐ ┌───┐ ┌───┐                │\n│                                     │Env│ │Env│ │Env│ │Env│                │\n│                                     └───┘ └───┘ └───┘ └───┘                │\n│                                           │                                │\n│                                           v                                │\n│                                     16,000 steps/sec                       │\n│                                                                             │\n│                                     1M steps = 1 minute!                    │\n│                                                                             │\n│  Training that took hours --> takes minutes with parallelization!           │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## RLlib's Distributed Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                    RLLIB DISTRIBUTED ARCHITECTURE                           │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│                      ┌────────────────────────────┐                         │\n│                      │    ALGORITHM (Driver)      │                         │\n│                      │                            │                         │\n│                      │  - Coordinates everything  │                         │\n│                      │  - Manages training loop   │                         │\n│                      │  - Stores final policy     │                         │\n│                      └─────────────┬──────────────┘                         │\n│                                    │                                        │\n│              ┌─────────────────────┼─────────────────────┐                  │\n│              │                     │                     │                  │\n│              v                     v                     v                  │\n│     ┌─────────────────┐   ┌─────────────────┐   ┌─────────────────┐        │\n│     │    LEARNER      │   │  REPLAY BUFFER  │   │   EVALUATION    │        │\n│     │                 │   │  (off-policy)   │   │   WORKERS       │        │\n│     │  - GPU training │   │                 │   │                 │        │\n│     │  - Gradients    │   │  - Stores exp   │   │  - Test policy  │        │\n│     │  - Updates      │   │  - Samples      │   │  - No training  │        │\n│     └─────────────────┘   └─────────────────┘   └─────────────────┘        │\n│                                                                             │\n│     ────────────────────────────────────────────────────────────────        │\n│                          ENVRUNNERS (Workers)                               │\n│     ────────────────────────────────────────────────────────────────        │\n│                                                                             │\n│     ┌──────────────┐ ┌──────────────┐ ┌──────────────┐ ┌──────────────┐    │\n│     │ EnvRunner 0  │ │ EnvRunner 1  │ │ EnvRunner 2  │ │ EnvRunner 3  │    │\n│     │              │ │              │ │              │ │              │    │\n│     │  ┌────────┐  │ │  ┌────────┐  │ │  ┌────────┐  │ │  ┌────────┐  │    │\n│     │  │  Env   │  │ │  │  Env   │  │ │  │  Env   │  │ │  │  Env   │  │    │\n│     │  └────────┘  │ │  └────────┘  │ │  └────────┘  │ │  └────────┘  │    │\n│     │  ┌────────┐  │ │  ┌────────┐  │ │  ┌────────┐  │ │  ┌────────┐  │    │\n│     │  │ Policy │  │ │  │ Policy │  │ │  │ Policy │  │ │  │ Policy │  │    │\n│     │  │ (copy) │  │ │  │ (copy) │  │ │  │ (copy) │  │ │  │ (copy) │  │    │\n│     │  └────────┘  │ │  └────────┘  │ │  └────────┘  │ │  └────────┘  │    │\n│     └──────────────┘ └──────────────┘ └──────────────┘ └──────────────┘    │\n│            │               │               │               │               │\n│            └───────────────┴───────────────┴───────────────┘               │\n│                                    │                                        │\n│                      Experience (s, a, r, s') flows UP                      │\n│                      Policy weights flow DOWN                               │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```\n\n**Key insight**: EnvRunners are Ray Actors! Each runs independently, collecting experience in parallel."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"ray\").setLevel(logging.ERROR)\n",
    "\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.impala import IMPALAConfig\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init(\n",
    "    num_cpus=4,\n",
    "    object_store_memory=1 * 1024 * 1024 * 1024,\n",
    "    ignore_reinit_error=True,\n",
    ")\n",
    "\n",
    "print(f\"Available resources: {ray.cluster_resources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Scaling with Workers\n",
    "\n",
    "The simplest way to scale: add more EnvRunners!\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                         WORKER SCALING                                      │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│  num_env_runners controls parallelism:                                     │\n",
    "│                                                                             │\n",
    "│  num_env_runners=1        num_env_runners=4        num_env_runners=16     │\n",
    "│  ─────────────────        ─────────────────        ──────────────────     │\n",
    "│                                                                             │\n",
    "│       ┌───┐                ┌───┐ ┌───┐             ┌───┐ ┌───┐ ┌───┐ ┌───┐│\n",
    "│       │ E │                │ E │ │ E │             │ E │ │ E │ │ E │ │ E ││\n",
    "│       └───┘                ├───┤ ├───┤             ├───┤ ├───┤ ├───┤ ├───┤│\n",
    "│                            │ E │ │ E │             │ E │ │ E │ │ E │ │ E ││\n",
    "│                            └───┘ └───┘             ├───┤ ├───┤ ├───┤ ├───┤│\n",
    "│                                                    │ E │ │ E │ │ E │ │ E ││\n",
    "│                                                    ├───┤ ├───┤ ├───┤ ├───┤│\n",
    "│                                                    │ E │ │ E │ │ E │ │ E ││\n",
    "│                                                    └───┘ └───┘ └───┘ └───┘│\n",
    "│                                                                             │\n",
    "│  1x throughput              4x throughput          16x throughput          │\n",
    "│                                                                             │\n",
    "│  TRADE-OFF:                                                                │\n",
    "│  More workers = More throughput                                            │\n",
    "│  More workers = More CPU usage                                             │\n",
    "│  More workers = More memory usage                                          │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_workers(num_workers_list, n_iters=3):\n",
    "    \"\"\"Benchmark training speed with different worker counts.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for num_workers in num_workers_list:\n",
    "        config = (\n",
    "            PPOConfig()\n",
    "            .environment(\"CartPole-v1\")\n",
    "            .framework(\"torch\")\n",
    "            .env_runners(\n",
    "                num_env_runners=num_workers,\n",
    "                num_envs_per_env_runner=1,\n",
    "            )\n",
    "            .training(\n",
    "                train_batch_size=2000,\n",
    "                sgd_minibatch_size=128,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        algo = config.build_algo()\n",
    "        \n",
    "        # Warm up\n",
    "        algo.train()\n",
    "        \n",
    "        # Benchmark\n",
    "        start = time.time()\n",
    "        for _ in range(n_iters):\n",
    "            algo.train()\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        samples_per_sec = (n_iters * 2000) / elapsed\n",
    "        results[num_workers] = samples_per_sec\n",
    "        \n",
    "        print(f\"Workers: {num_workers}, Samples/sec: {samples_per_sec:.0f}\")\n",
    "        algo.stop()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Benchmarking worker scaling...\")\n",
    "print(\"=\" * 50)\n",
    "results = benchmark_workers([1, 2], n_iters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Vectorized Environments\n",
    "\n",
    "Even more parallelism: multiple envs PER worker!\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                      VECTORIZED ENVIRONMENTS                                │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│  num_envs_per_env_runner=1          num_envs_per_env_runner=5              │\n",
    "│  ────────────────────────────        ─────────────────────────────         │\n",
    "│                                                                             │\n",
    "│  ┌──────────────────────┐            ┌──────────────────────┐              │\n",
    "│  │     EnvRunner        │            │     EnvRunner        │              │\n",
    "│  │                      │            │                      │              │\n",
    "│  │  ┌────────────────┐  │            │  ┌───┐ ┌───┐ ┌───┐   │              │\n",
    "│  │  │      Env       │  │            │  │Env│ │Env│ │Env│   │              │\n",
    "│  │  └────────────────┘  │            │  └───┘ └───┘ └───┘   │              │\n",
    "│  │                      │            │  ┌───┐ ┌───┐         │              │\n",
    "│  └──────────────────────┘            │  │Env│ │Env│         │              │\n",
    "│                                      │  └───┘ └───┘         │              │\n",
    "│  1 env per worker                    └──────────────────────┘              │\n",
    "│                                                                             │\n",
    "│                                      5 envs per worker                     │\n",
    "│                                      (vectorized step!)                    │\n",
    "│                                                                             │\n",
    "│  WHY VECTORIZE?                                                            │\n",
    "│  - Batch inference: compute actions for 5 envs at once                    │\n",
    "│  - Better GPU utilization                                                  │\n",
    "│  - Less overhead than separate workers                                     │\n",
    "│                                                                             │\n",
    "│  FORMULA:                                                                  │\n",
    "│  total_envs = num_env_runners × num_envs_per_env_runner                   │\n",
    "│                                                                             │\n",
    "│  Example: 4 workers × 5 envs = 20 parallel environments!                  │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-throughput configuration\n",
    "high_throughput_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    \n",
    "    .env_runners(\n",
    "        num_env_runners=2,            # 2 worker processes\n",
    "        num_envs_per_env_runner=4,    # 4 envs per worker = 8 total!\n",
    "        rollout_fragment_length=200,  # Steps per collection\n",
    "    )\n",
    "    \n",
    "    .training(\n",
    "        train_batch_size=4000,        # 2 × 4 × 200 = 1600 → will wait for 3 collections\n",
    "        sgd_minibatch_size=256,\n",
    "        num_sgd_iter=10,\n",
    "        lr=3e-4,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"High-throughput config:\")\n",
    "print(f\"  Workers: {high_throughput_config.num_env_runners}\")\n",
    "print(f\"  Envs per worker: {high_throughput_config.num_envs_per_env_runner}\")\n",
    "print(f\"  Total parallel envs: {high_throughput_config.num_env_runners * high_throughput_config.num_envs_per_env_runner}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## GPU Configuration\n\nGPUs accelerate the LEARNING part (gradient computation).\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                          GPU USAGE IN RLLIB                                 │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  WHERE DOES GPU HELP?                                                       │\n│  ────────────────────                                                       │\n│                                                                             │\n│  Data Collection (EnvRunners)          Learning (Trainer/Learner)           │\n│  ────────────────────────────          ──────────────────────────           │\n│                                                                             │\n│  env.step() is Python                  Neural network forward/backward      │\n│  --> CPU bound!                        --> GPU accelerated!                 │\n│                                                                             │\n│  GPU DOESN'T HELP HERE                 GPU HELPS HERE                       │\n│                                                                             │\n│  ────────────────────────────────────────────────────────────────────       │\n│                                                                             │\n│  CONFIGURATION:                                                             │\n│                                                                             │\n│  # Single GPU training                                                      │\n│  .resources(num_gpus=1)                                                     │\n│                                                                             │\n│  # Multi-GPU with new Learner API                                           │\n│  .learners(                                                                 │\n│      num_learners=2,        # 2 learner workers                             │\n│      num_gpus_per_learner=1 # Each gets 1 GPU                               │\n│  )                                                                          │\n│                                                                             │\n│  ────────────────────────────────────────────────────────────────────       │\n│                                                                             │\n│  TYPICAL SETUP:                                                             │\n│                                                                             │\n│      ┌────────────────────────────────────────────────────┐                 │\n│      │   Many CPUs for data collection (EnvRunners)       │                 │\n│      │   ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐ │                 │\n│      │   │CPU│ │CPU│ │CPU│ │CPU│ │CPU│ │CPU│ │CPU│ │CPU│ │                 │\n│      │   └───┘ └───┘ └───┘ └───┘ └───┘ └───┘ └───┘ └───┘ │                 │\n│      └────────────────────────┬───────────────────────────┘                 │\n│                               │ experience                                  │\n│                               v                                             │\n│      ┌────────────────────────────────────────────────────┐                 │\n│      │   1-2 GPUs for training (Learner)                  │                 │\n│      │                 ┌─────┐ ┌─────┐                    │                 │\n│      │                 │ GPU │ │ GPU │                    │                 │\n│      │                 └─────┘ └─────┘                    │                 │\n│      └────────────────────────────────────────────────────┘                 │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single GPU configuration\n",
    "single_gpu_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(\n",
    "        num_env_runners=4,\n",
    "        num_cpus_per_env_runner=1,\n",
    "    )\n",
    "    .resources(\n",
    "        num_gpus=1,  # GPU for training\n",
    "        num_cpus_for_main_process=1,\n",
    "    )\n",
    "    .training(\n",
    "        train_batch_size=4000,\n",
    "        model={\"fcnet_hiddens\": [256, 256]},\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Single GPU config: num_gpus=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-GPU with Learner API (RLlib 2.x+)\n",
    "multi_gpu_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(\n",
    "        num_env_runners=8,\n",
    "    )\n",
    "    .learners(\n",
    "        num_learners=2,           # 2 learner workers\n",
    "        num_gpus_per_learner=1,   # Each gets 1 GPU\n",
    "    )\n",
    "    .training(\n",
    "        train_batch_size=8000,    # Larger batch for multi-GPU\n",
    "        minibatch_size=256,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Multi-GPU config:\")\n",
    "print(\"  2 learners × 1 GPU each = 2 GPUs for training\")\n",
    "print(\"  8 EnvRunners for data collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## IMPALA: Built for Scale\n",
    "\n",
    "For truly massive scale (100+ workers), use algorithms designed for it.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                              IMPALA                                         │\n",
    "│               Importance Weighted Actor-Learner Architecture               │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│  KEY IDEA: Decouple acting from learning                                   │\n",
    "│                                                                             │\n",
    "│  PPO (synchronous):              IMPALA (asynchronous):                    │\n",
    "│  ─────────────────               ──────────────────────                    │\n",
    "│                                                                             │\n",
    "│  Collect ──> Train ──> Collect   Actors collect CONTINUOUSLY               │\n",
    "│     │          ↑         │       Learner trains CONTINUOUSLY               │\n",
    "│     └──── wait ┘         │                                                  │\n",
    "│                                  ┌─────────────────────────┐               │\n",
    "│  Workers wait while              │      LEARNER (GPU)      │               │\n",
    "│  trainer updates                 │   ┌─────────────────┐   │               │\n",
    "│                                  │   │  Train on any   │   │               │\n",
    "│                                  │   │  available data │   │               │\n",
    "│                                  │   └─────────────────┘   │               │\n",
    "│                                  └──────────┬──────────────┘               │\n",
    "│                                             │                              │\n",
    "│                          ┌──────────────────┼──────────────────┐           │\n",
    "│                          │                  │                  │           │\n",
    "│                     ┌────┴────┐        ┌────┴────┐        ┌────┴────┐      │\n",
    "│                     │ Actor 1 │        │ Actor 2 │        │Actor 100│      │\n",
    "│                     │ (keeps  │        │ (keeps  │   ...  │ (keeps  │      │\n",
    "│                     │ running)│        │ running)│        │ running)│      │\n",
    "│                     └─────────┘        └─────────┘        └─────────┘      │\n",
    "│                                                                             │\n",
    "│  PROBLEM: Actors have OLD policy (not updated yet)                        │\n",
    "│  SOLUTION: V-trace corrects for policy lag!                               │\n",
    "│                                                                             │\n",
    "│  USE IMPALA WHEN:                                                          │\n",
    "│  • You have 50+ workers                                                    │\n",
    "│  • Training is the bottleneck                                              │\n",
    "│  • You want maximum throughput                                             │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPALA configuration\n",
    "impala_config = (\n",
    "    IMPALAConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(\n",
    "        num_env_runners=4,\n",
    "        num_envs_per_env_runner=5,  # Vectorized\n",
    "    )\n",
    "    .resources(\n",
    "        num_gpus=0,  # Would use 1+ in production\n",
    "    )\n",
    "    .training(\n",
    "        lr=5e-4,\n",
    "        train_batch_size=500,\n",
    "        # V-trace parameters (IMPALA's secret sauce)\n",
    "        vtrace=True,\n",
    "        vtrace_clip_rho_threshold=1.0,\n",
    "        vtrace_clip_pg_rho_threshold=1.0,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"IMPALA config created\")\n",
    "print(\"  V-trace enabled for off-policy correction\")\n",
    "print(\"  Total envs: 4 workers × 5 envs = 20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ray Cluster: Multi-Node Training\n",
    "\n",
    "For really big jobs, spread across multiple machines.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                           RAY CLUSTER                                       │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│  ┌─────────────────────────────────────────────────────────────────────┐   │\n",
    "│  │                         HEAD NODE                                   │   │\n",
    "│  │                                                                     │   │\n",
    "│  │   ┌─────────────┐  ┌─────────────┐  ┌─────────────┐               │   │\n",
    "│  │   │   Driver    │  │   GCS       │  │  Dashboard  │               │   │\n",
    "│  │   │  (your code)│  │ (metadata)  │  │ (localhost: │               │   │\n",
    "│  │   │             │  │             │  │     8265)   │               │   │\n",
    "│  │   └─────────────┘  └─────────────┘  └─────────────┘               │   │\n",
    "│  │                                                                     │   │\n",
    "│  └─────────────────────────────────────────────────────────────────────┘   │\n",
    "│                                   │                                        │\n",
    "│                    ┌──────────────┼──────────────┐                        │\n",
    "│                    │              │              │                        │\n",
    "│                    v              v              v                        │\n",
    "│  ┌──────────────────┐ ┌──────────────────┐ ┌──────────────────┐          │\n",
    "│  │   WORKER NODE 1  │ │   WORKER NODE 2  │ │   WORKER NODE 3  │          │\n",
    "│  │                  │ │                  │ │                  │          │\n",
    "│  │  ┌────┐ ┌────┐   │ │  ┌────┐ ┌────┐   │ │  ┌────┐ ┌────┐   │          │\n",
    "│  │  │ER 1│ │ER 2│   │ │  │ER 3│ │ER 4│   │ │  │ER 5│ │ER 6│   │          │\n",
    "│  │  └────┘ └────┘   │ │  └────┘ └────┘   │ │  └────┘ └────┘   │          │\n",
    "│  │                  │ │                  │ │                  │          │\n",
    "│  │  16 CPUs, 64GB   │ │  16 CPUs, 64GB   │ │  16 CPUs, 64GB   │          │\n",
    "│  └──────────────────┘ └──────────────────┘ └──────────────────┘          │\n",
    "│                                                                            │\n",
    "│  SETUP OPTIONS:                                                           │\n",
    "│  ─────────────                                                            │\n",
    "│  1. Manual: ray start --head / ray start --address=HEAD_IP                │\n",
    "│  2. Cloud:  ray up cluster.yaml (AWS, GCP, Azure)                         │\n",
    "│  3. K8s:    KubeRay operator                                              │\n",
    "│                                                                            │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example cluster config (for reference)\n",
    "cluster_yaml = \"\"\"\n",
    "# Save as cluster.yaml\n",
    "cluster_name: rllib-cluster\n",
    "\n",
    "max_workers: 4\n",
    "\n",
    "provider:\n",
    "    type: aws\n",
    "    region: us-west-2\n",
    "\n",
    "head_node:\n",
    "    InstanceType: m5.2xlarge  # 8 CPUs\n",
    "\n",
    "worker_nodes:\n",
    "    InstanceType: m5.2xlarge  # 8 CPUs each\n",
    "\n",
    "setup_commands:\n",
    "    - pip install \"ray[rllib]\" torch\n",
    "\"\"\"\n",
    "\n",
    "print(\"Cluster setup commands:\")\n",
    "print(\"  ray up cluster.yaml       # Start cluster\")\n",
    "print(\"  ray submit cluster.yaml train.py  # Run job\")\n",
    "print(\"  ray down cluster.yaml     # Stop cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Scaling Cheat Sheet\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                        SCALING CHEAT SHEET                                  │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│  LAPTOP (4-8 CPUs, no GPU)                                                 │\n",
    "│  ─────────────────────────                                                 │\n",
    "│  .env_runners(num_env_runners=2, num_envs_per_env_runner=2)               │\n",
    "│  .resources(num_gpus=0)                                                    │\n",
    "│                                                                             │\n",
    "│  WORKSTATION (16+ CPUs, 1 GPU)                                             │\n",
    "│  ──────────────────────────────                                            │\n",
    "│  .env_runners(num_env_runners=8, num_envs_per_env_runner=4)               │\n",
    "│  .resources(num_gpus=1)                                                    │\n",
    "│                                                                             │\n",
    "│  SMALL CLUSTER (4 nodes, 64 CPUs, 4 GPUs)                                  │\n",
    "│  ─────────────────────────────────────────                                 │\n",
    "│  .env_runners(num_env_runners=32, num_envs_per_env_runner=4)              │\n",
    "│  .learners(num_learners=4, num_gpus_per_learner=1)                        │\n",
    "│                                                                             │\n",
    "│  LARGE CLUSTER (use IMPALA)                                                │\n",
    "│  ───────────────────────────                                               │\n",
    "│  IMPALAConfig()                                                            │\n",
    "│  .env_runners(num_env_runners=100, num_envs_per_env_runner=5)             │\n",
    "│  .resources(num_gpus=8)                                                    │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n1. **Scale workers** (`num_env_runners`) for more data collection throughput\n\n2. **Vectorize** (`num_envs_per_env_runner`) for more efficient batching\n\n3. **Use GPU** for training acceleration (the Learner)\n\n4. **IMPALA** for massive scale (100+ workers)\n\n5. **Ray Cluster** for multi-node training\n\n## What's Next?\n\n```\n┌──────────────────┐          ┌──────────────────┐          ┌──────────────────┐\n│  05 Distributed  │          │    06 Ray Tune   │          │  07 Production   │\n│  (you are here)  │   ───>   │                  │   ───>   │                  │\n│                  │          │  Find optimal    │          │  Deploy your     │\n│  - Workers       │          │  hyperparameters │          │  trained policy  │\n│  - GPUs          │          │                  │          │                  │\n│  - Clusters      │          │                  │          │                  │\n└──────────────────┘          └──────────────────┘          └──────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}