{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Scaling RLlib Training\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand RLlib's distributed architecture\n",
    "- Configure multi-GPU and multi-node training\n",
    "- Optimize resource utilization\n",
    "- Use Ray Cluster for large-scale training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLlib Distributed Architecture\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                        Driver (Trainer)                       │\n",
    "│                                                              │\n",
    "│  ┌─────────────┐   ┌─────────────┐   ┌─────────────┐        │\n",
    "│  │   Policy    │   │  Learner    │   │   Replay    │        │\n",
    "│  │   (GPU)     │   │  (GPU)      │   │   Buffer    │        │\n",
    "│  └─────────────┘   └─────────────┘   └─────────────┘        │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "                              │\n",
    "              ┌───────────────┼───────────────┐\n",
    "              │               │               │\n",
    "              ▼               ▼               ▼\n",
    "       ┌──────────┐    ┌──────────┐    ┌──────────┐\n",
    "       │ EnvRunner│    │ EnvRunner│    │ EnvRunner│\n",
    "       │  (CPU)   │    │  (CPU)   │    │  (CPU)   │\n",
    "       │  ┌───┐   │    │  ┌───┐   │    │  ┌───┐   │\n",
    "       │  │Env│   │    │  │Env│   │    │  │Env│   │\n",
    "       │  └───┘   │    │  └───┘   │    │  └───┘   │\n",
    "       └──────────┘    └──────────┘    └──────────┘\n",
    "```\n",
    "\n",
    "Key components:\n",
    "- **Driver**: Coordinates training, manages policy\n",
    "- **Learner**: Performs gradient updates (can use GPU)\n",
    "- **EnvRunners**: Collect experience (CPU workers)\n",
    "- **Replay Buffer**: Stores experience (for off-policy algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.impala import IMPALAConfig\n",
    "from ray.rllib.algorithms.apex_dqn import ApexDQNConfig\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize Ray with resource specification\n",
    "ray.init(\n",
    "    ignore_reinit_error=True,\n",
    "    # Uncomment for specific resource allocation:\n",
    "    # num_cpus=8,\n",
    "    # num_gpus=1,\n",
    ")\n",
    "\n",
    "print(f\"Available resources: {ray.cluster_resources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling with Multiple Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_workers(num_workers_list, n_iters=5):\n",
    "    \"\"\"Benchmark training speed with different worker counts.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for num_workers in num_workers_list:\n",
    "        config = (\n",
    "            PPOConfig()\n",
    "            .environment(\"CartPole-v1\")\n",
    "            .framework(\"torch\")\n",
    "            .env_runners(\n",
    "                num_env_runners=num_workers,\n",
    "                num_envs_per_env_runner=1,\n",
    "            )\n",
    "            .training(\n",
    "                train_batch_size=4000,\n",
    "                sgd_minibatch_size=128,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        algo = config.build()\n",
    "        \n",
    "        # Warm up\n",
    "        algo.train()\n",
    "        \n",
    "        # Benchmark\n",
    "        start = time.time()\n",
    "        for _ in range(n_iters):\n",
    "            algo.train()\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        samples_per_sec = (n_iters * 4000) / elapsed\n",
    "        results[num_workers] = samples_per_sec\n",
    "        \n",
    "        print(f\"Workers: {num_workers}, Samples/sec: {samples_per_sec:.0f}\")\n",
    "        algo.stop()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Benchmark with different worker counts\n",
    "# results = benchmark_workers([1, 2, 4], n_iters=3)\n",
    "print(\"Benchmark example (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single GPU configuration\n",
    "single_gpu_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(\n",
    "        num_env_runners=4,\n",
    "        num_cpus_per_env_runner=1,\n",
    "    )\n",
    "    .resources(\n",
    "        num_gpus=1,  # GPU for training\n",
    "        num_cpus_for_main_process=1,\n",
    "    )\n",
    "    .training(\n",
    "        train_batch_size=4000,\n",
    "        model={\n",
    "            \"fcnet_hiddens\": [256, 256],\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Single GPU config created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-GPU with Learner API (RLlib 2.x)\n",
    "multi_gpu_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(\n",
    "        num_env_runners=8,\n",
    "    )\n",
    "    .learners(\n",
    "        num_learners=2,        # Number of learner workers\n",
    "        num_gpus_per_learner=1,  # GPU per learner\n",
    "    )\n",
    "    .training(\n",
    "        train_batch_size=8000,\n",
    "        minibatch_size=256,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Multi-GPU config created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPALA: Scalable Distributed Training\n",
    "\n",
    "IMPALA (Importance Weighted Actor-Learner Architecture) is designed for massive scale:\n",
    "- Asynchronous actors and learners\n",
    "- V-trace for off-policy correction\n",
    "- Can scale to thousands of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impala_config = (\n",
    "    IMPALAConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(\n",
    "        num_env_runners=4,\n",
    "        num_envs_per_env_runner=5,  # Vectorized environments\n",
    "    )\n",
    "    .resources(\n",
    "        num_gpus=1,\n",
    "    )\n",
    "    .training(\n",
    "        lr=5e-4,\n",
    "        train_batch_size=500,\n",
    "        # V-trace parameters\n",
    "        vtrace=True,\n",
    "        vtrace_clip_rho_threshold=1.0,\n",
    "        vtrace_clip_pg_rho_threshold=1.0,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train IMPALA\n",
    "# algo = impala_config.build()\n",
    "# result = algo.train()\n",
    "print(\"IMPALA config created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APEX-DQN: Distributed Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apex_config = (\n",
    "    ApexDQNConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(\n",
    "        num_env_runners=4,\n",
    "    )\n",
    "    .resources(\n",
    "        num_gpus=1,\n",
    "    )\n",
    "    .training(\n",
    "        lr=5e-4,\n",
    "        n_step=3,\n",
    "        # Apex-specific: prioritized replay\n",
    "        replay_buffer_config={\n",
    "            \"type\": \"MultiAgentPrioritizedReplayBuffer\",\n",
    "            \"capacity\": 100000,\n",
    "            \"prioritized_replay_alpha\": 0.6,\n",
    "            \"prioritized_replay_beta\": 0.4,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"APEX-DQN config created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Cluster Setup\n",
    "\n",
    "For multi-node training, you need a Ray cluster.\n",
    "\n",
    "### Option 1: Manual Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_config.yaml example:\n",
    "cluster_yaml = \"\"\"\n",
    "cluster_name: rllib-cluster\n",
    "\n",
    "max_workers: 4\n",
    "\n",
    "provider:\n",
    "    type: aws\n",
    "    region: us-west-2\n",
    "\n",
    "auth:\n",
    "    ssh_user: ubuntu\n",
    "\n",
    "head_node:\n",
    "    InstanceType: m5.2xlarge\n",
    "    ImageId: ami-0a2363a9cff180a64  # Ray AMI\n",
    "\n",
    "worker_nodes:\n",
    "    InstanceType: m5.2xlarge\n",
    "    ImageId: ami-0a2363a9cff180a64\n",
    "\n",
    "setup_commands:\n",
    "    - pip install \"ray[rllib]\" torch\n",
    "\"\"\"\n",
    "\n",
    "print(\"Example Ray cluster config\")\n",
    "print(\"Commands:\")\n",
    "print(\"  ray up cluster_config.yaml    # Start cluster\")\n",
    "print(\"  ray submit cluster_config.yaml train.py  # Submit job\")\n",
    "print(\"  ray down cluster_config.yaml  # Stop cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Kubernetes with KubeRay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RayCluster Kubernetes manifest example\n",
    "kuberay_yaml = \"\"\"\n",
    "apiVersion: ray.io/v1alpha1\n",
    "kind: RayCluster\n",
    "metadata:\n",
    "  name: rllib-cluster\n",
    "spec:\n",
    "  rayVersion: '2.9.0'\n",
    "  headGroupSpec:\n",
    "    rayStartParams:\n",
    "      dashboard-host: '0.0.0.0'\n",
    "    template:\n",
    "      spec:\n",
    "        containers:\n",
    "        - name: ray-head\n",
    "          image: rayproject/ray-ml:2.9.0\n",
    "          resources:\n",
    "            limits:\n",
    "              cpu: \"4\"\n",
    "              memory: \"8Gi\"\n",
    "              nvidia.com/gpu: \"1\"\n",
    "  workerGroupSpecs:\n",
    "  - groupName: workers\n",
    "    replicas: 4\n",
    "    rayStartParams: {}\n",
    "    template:\n",
    "      spec:\n",
    "        containers:\n",
    "        - name: ray-worker\n",
    "          image: rayproject/ray-ml:2.9.0\n",
    "          resources:\n",
    "            limits:\n",
    "              cpu: \"2\"\n",
    "              memory: \"4Gi\"\n",
    "\"\"\"\n",
    "\n",
    "print(\"KubeRay cluster manifest example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-throughput configuration\n",
    "high_throughput_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    \n",
    "    # Maximize parallelism\n",
    "    .env_runners(\n",
    "        num_env_runners=16,           # Many workers\n",
    "        num_envs_per_env_runner=5,    # Vectorized envs per worker\n",
    "        rollout_fragment_length=200,  # Steps before sending data\n",
    "        sample_timeout_s=60,\n",
    "    )\n",
    "    \n",
    "    # GPU training\n",
    "    .resources(\n",
    "        num_gpus=1,\n",
    "        num_cpus_for_main_process=1,\n",
    "    )\n",
    "    \n",
    "    # Large batches for GPU efficiency\n",
    "    .training(\n",
    "        train_batch_size=16000,       # Large batch\n",
    "        sgd_minibatch_size=4096,      # GPU-friendly minibatch\n",
    "        num_sgd_iter=10,\n",
    "        lr=3e-4,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"High-throughput config created\")\n",
    "print(f\"Expected samples per iteration: 16000\")\n",
    "print(f\"Workers x Envs x Fragment: 16 x 5 x 200 = 16000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient configuration for large observations\n",
    "memory_efficient_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(\n",
    "        num_env_runners=4,\n",
    "        # Compress observations to save memory\n",
    "        compress_observations=True,\n",
    "    )\n",
    "    .training(\n",
    "        train_batch_size=4000,\n",
    "        # Gradient accumulation for memory efficiency\n",
    "        sgd_minibatch_size=256,  # Smaller minibatch\n",
    "        num_sgd_iter=16,          # More iterations\n",
    "    )\n",
    "    .debugging(\n",
    "        # Log memory usage\n",
    "        log_level=\"WARN\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Memory-efficient config created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_distributed.py\n",
    "train_script = '''\n",
    "#!/usr/bin/env python\n",
    "\"\"\"Distributed RLlib training script.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "def main(args):\n",
    "    # Connect to Ray cluster\n",
    "    ray.init(address=args.ray_address)\n",
    "    \n",
    "    config = (\n",
    "        PPOConfig()\n",
    "        .environment(args.env)\n",
    "        .framework(\"torch\")\n",
    "        .env_runners(\n",
    "            num_env_runners=args.num_workers,\n",
    "            num_envs_per_env_runner=args.envs_per_worker,\n",
    "        )\n",
    "        .resources(\n",
    "            num_gpus=args.num_gpus,\n",
    "        )\n",
    "        .training(\n",
    "            train_batch_size=args.batch_size,\n",
    "            lr=args.lr,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Run with Tune for experiment tracking\n",
    "    tuner = tune.Tuner(\n",
    "        \"PPO\",\n",
    "        param_space=config,\n",
    "        run_config=tune.RunConfig(\n",
    "            name=args.experiment_name,\n",
    "            stop={\"training_iteration\": args.max_iterations},\n",
    "            checkpoint_config=tune.CheckpointConfig(\n",
    "                checkpoint_frequency=10,\n",
    "                checkpoint_at_end=True,\n",
    "            ),\n",
    "            storage_path=args.storage_path,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    results = tuner.fit()\n",
    "    print(f\"Best result: {results.get_best_result()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--ray-address\", default=\"auto\")\n",
    "    parser.add_argument(\"--env\", default=\"CartPole-v1\")\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=8)\n",
    "    parser.add_argument(\"--envs-per-worker\", type=int, default=5)\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=1)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=4000)\n",
    "    parser.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    parser.add_argument(\"--max-iterations\", type=int, default=100)\n",
    "    parser.add_argument(\"--experiment-name\", default=\"ppo_experiment\")\n",
    "    parser.add_argument(\"--storage-path\", default=\"/tmp/ray_results\")\n",
    "    \n",
    "    main(parser.parse_args())\n",
    "'''\n",
    "\n",
    "print(\"Distributed training script example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Scale workers** for data collection, GPUs for training\n",
    "\n",
    "2. **IMPALA/APEX** are designed for massive distributed training\n",
    "\n",
    "3. **Ray Cluster** enables multi-node training (AWS, GCP, Kubernetes)\n",
    "\n",
    "4. **Tune batch sizes** to match your hardware capabilities\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next section, we'll cover hyperparameter tuning with Ray Tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
