{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Deep Reinforcement Learning: Neural Networks Meet RL\n\n**Prerequisites**: Complete [01_rl_concepts](./01_rl_concepts.ipynb) first!\n\nIn the last notebook, we used a Q-table to store values. But what happens when states become complex?\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                    THE PROBLEM: STATE SPACES EXPLODE                        │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  Q-Table works:              Q-Table FAILS:                                 │\n│  ─────────────               ──────────────                                 │\n│                                                                             │\n│  GridWorld (16 states)       Atari Game (pixels)                            │\n│  ┌───┬───┬───┬───┐           ┌─────────────────┐                            │\n│  │ Q │ Q │ Q │ Q │           │ 210 x 160 x 3   │                            │\n│  ├───┼───┼───┼───┤           │ = 100,800 pixels│                            │\n│  │ Q │ Q │ Q │ Q │           │                 │                            │\n│  ├───┼───┼───┼───┤           │ Each pixel 0-255│                            │\n│  │ Q │ Q │ Q │ Q │           │ = 256^100800    │                            │\n│  ├───┼───┼───┼───┤           │ possible states │                            │\n│  │ Q │ Q │ Q │ Q │           │                 │                            │\n│  └───┴───┴───┴───┘           │ (more than atoms│                            │\n│  16 x 4 = 64 entries         │  in universe!)  │                            │\n│  Easy to store!              └─────────────────┘                            │\n│                              Impossible to store!                           │\n│                                                                             │\n│  SOLUTION: Use a neural network to GENERALIZE across similar states         │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## The Key Insight: Function Approximation\n\nInstead of storing Q(s,a) in a table, we **approximate** it with a neural network:\n\n```\nQ-TABLE APPROACH                    NEURAL NETWORK APPROACH\n─────────────────                   ───────────────────────\n\nState (0,0) ──> Look up row 0       State (0,0) ──┐\n                     │                            │\n                     v                            v\n              ┌─────────────┐              ┌─────────────┐\n              │ 0.5 0.2 0.8 │              │   Neural    │\n              │ 0.1 0.9 0.3 │              │   Network   │\n              │ ... ... ... │              │   Q(s;θ)    │\n              └─────────────┘              └─────────────┘\n                     │                            │\n                     v                            v\n              Q-values for                 Q-values for\n              each action                  each action\n              [0.5, 0.2, 0.8]              [0.5, 0.2, 0.8]\n\nMemory: O(states × actions)        Memory: O(network parameters)\n        Grows with state space             FIXED size!\n```\n\n**The magic**: Similar states produce similar Q-values (generalization)!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## DQN: Deep Q-Network\n\nDQN (Mnih et al., 2015) was a breakthrough - it played Atari games at superhuman level!\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                         DQN ARCHITECTURE                                    │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│                          ┌─────────────────────────────────┐                │\n│                          │        Q-Network                │                │\n│     State s              │                                 │                │\n│  ┌─────────────┐         │  ┌─────┐   ┌─────┐   ┌─────┐  │   Q-values     │\n│  │ obs[0]      │────────>│  │     │   │     │   │     │  │  ┌─────────┐   │\n│  │ obs[1]      │         │  │ 128 │──>│ 128 │──>│  n  │──│─>│ Q(s,a0) │   │\n│  │ obs[2]      │         │  │     │   │     │   │     │  │  │ Q(s,a1) │   │\n│  │ obs[3]      │         │  └─────┘   └─────┘   └─────┘  │  │ Q(s,a2) │   │\n│  └─────────────┘         │   ReLU      ReLU     Linear   │  │   ...   │   │\n│   (4 numbers for         │                                │  └─────────┘   │\n│    CartPole)             └─────────────────────────────────┘                │\n│                                                                             │\n│  Action Selection: a = argmax Q(s, a)                                       │\n│                              a                                              │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```\n\nThe network takes a state and outputs Q-values for ALL actions at once!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### DQN's Two Key Innovations\n\nNaive neural network Q-learning is unstable. DQN fixes this with two tricks:\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                    INNOVATION #1: EXPERIENCE REPLAY                         │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  PROBLEM: Consecutive samples are correlated (s1->s2->s3->...)              │\n│           Neural networks learn poorly from correlated data!                │\n│                                                                             │\n│  SOLUTION: Store experiences in a buffer, sample RANDOMLY                   │\n│                                                                             │\n│  ┌────────────────────────────────────────────────────────────┐             │\n│  │               REPLAY BUFFER (size = 10,000)                │             │\n│  ├────────────────────────────────────────────────────────────┤             │\n│  │  (s₁, a₁, r₁, s₁')  <── oldest                             │             │\n│  │  (s₂, a₂, r₂, s₂')                                         │             │\n│  │  (s₃, a₃, r₃, s₃')      ↑                                  │             │\n│  │       ...               │ Random sample batch of 32        │             │\n│  │  (s₈, a₈, r₈, s₈')      ↓                                  │             │\n│  │       ...                                                   │             │\n│  │  (sₙ, aₙ, rₙ, sₙ')  <── newest                             │             │\n│  └────────────────────────────────────────────────────────────┘             │\n│                              │                                              │\n│                              v                                              │\n│                    Train on random batch                                    │\n│                    (breaks correlation!)                                    │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                    INNOVATION #2: TARGET NETWORK                            │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  PROBLEM: We update Q using Q itself → moving target → instability!         │\n│                                                                             │\n│       Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]                      │\n│                              ↑                                              │\n│                        This changes as                                      │\n│                        we update Q!                                         │\n│                                                                             │\n│  SOLUTION: Use a SEPARATE \"target\" network that updates slowly              │\n│                                                                             │\n│     ┌─────────────────┐                    ┌─────────────────┐              │\n│     │  Policy Network │                    │  Target Network │              │\n│     │    Q(s; θ)      │                    │    Q(s; θ⁻)     │              │\n│     │                 │  copy weights      │                 │              │\n│     │  Updates every  │ ───────────────>   │  Updates every  │              │\n│     │  training step  │  (every 100 steps) │  100 steps      │              │\n│     └─────────────────┘                    └─────────────────┘              │\n│            │                                        │                       │\n│            v                                        v                       │\n│     Used for action                         Used for computing              │\n│     selection                               target Q-values                 │\n│                                                                             │\n│  The target is now STABLE for 100 steps → much easier to learn!             │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for DQN.\n",
    "    \n",
    "    Stores (state, action, reward, next_state, done) tuples\n",
    "    and provides random sampling for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store a transition.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"Sample a RANDOM batch of transitions.\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.FloatTensor(dones)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network.\n",
    "    \n",
    "    Architecture: state → [128] → [128] → Q-values for each action\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # First hidden layer + ReLU\n",
    "        x = F.relu(self.fc2(x))  # Second hidden layer + ReLU\n",
    "        return self.fc3(x)       # Output Q-values (no activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### The DQN Training Loop\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                          DQN TRAINING LOOP                                  │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  1. COLLECT EXPERIENCE                                                      │\n│     ┌─────┐  action (ε-greedy)  ┌─────────────┐                             │\n│     │Agent│ ──────────────────> │ Environment │                             │\n│     │     │ <────────────────── │             │                             │\n│     └─────┘  state, reward      └─────────────┘                             │\n│        │                                                                    │\n│        v                                                                    │\n│  2. STORE IN REPLAY BUFFER                                                  │\n│     ┌─────────────────────────────────┐                                     │\n│     │  (s, a, r, s', done)  ───────>  │ Replay Buffer                       │\n│     └─────────────────────────────────┘                                     │\n│                                                                             │\n│  3. SAMPLE RANDOM BATCH                                                     │\n│     ┌─────────────────────────────────┐                                     │\n│     │  Random 32 samples  <─────────  │ Replay Buffer                       │\n│     └─────────────────────────────────┘                                     │\n│                                                                             │\n│  4. COMPUTE LOSS                                                            │\n│                                                                             │\n│     predicted = Q(s, a; θ)           ← Policy network                       │\n│     target = r + γ max Q(s', a'; θ⁻) ← Target network (frozen)              │\n│                   a'                                                        │\n│     loss = (predicted - target)²                                            │\n│                                                                             │\n│  5. UPDATE POLICY NETWORK                                                   │\n│     θ ← θ - α ∇loss                                                         │\n│                                                                             │\n│  6. PERIODICALLY UPDATE TARGET NETWORK                                      │\n│     Every N steps: θ⁻ ← θ                                                   │\n│                                                                             │\n│  REPEAT!                                                                    │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent with experience replay and target network.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int,\n",
    "                 lr: float = 1e-3, gamma: float = 0.99,\n",
    "                 epsilon_start: float = 1.0, epsilon_end: float = 0.01,\n",
    "                 epsilon_decay: float = 0.995, buffer_size: int = 10000,\n",
    "                 batch_size: int = 64, target_update: int = 10):\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.update_counter = 0\n",
    "        \n",
    "        # Two networks: policy (updates often) and target (updates slowly)\n",
    "        self.policy_net = DQN(state_dim, action_dim)\n",
    "        self.target_net = DQN(state_dim, action_dim)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())  # Start same\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)  # Explore\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "            return q_values.argmax().item()  # Exploit\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update the network from replay buffer.\"\"\"\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        # Sample random batch\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Current Q values from POLICY network\n",
    "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Target Q values from TARGET network (frozen)\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_net(next_states).max(1)[0]\n",
    "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "        \n",
    "        # Compute MSE loss\n",
    "        loss = F.mse_loss(current_q.squeeze(), target_q)\n",
    "        \n",
    "        # Optimize policy network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        # Decay epsilon (less exploration over time)\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env_name: str = 'CartPole-v1', n_episodes: int = 300):\n",
    "    \"\"\"Train DQN on an environment.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    print(\"Training DQN on CartPole\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.buffer.push(state, action, reward, next_state, float(done))\n",
    "            agent.update()\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            print(f\"Episode {episode + 1:>4}, Avg Reward: {avg_reward:>6.1f}, Epsilon: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, episode_rewards\n",
    "\n",
    "# Train the agent\n",
    "agent, rewards = train_dqn(n_episodes=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "window = 20\n",
    "smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.plot(smoothed)\n",
    "plt.axhline(y=195, color='r', linestyle='--', label='Solved threshold (195)')\n",
    "plt.axhline(y=475, color='g', linestyle='--', label='Max score (500)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward (smoothed)')\n",
    "plt.title('DQN Training on CartPole-v1')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Policy Gradient Methods\n\nDQN learns Q-values and derives a policy from them. **Policy Gradient** methods take a different approach - they learn the policy DIRECTLY.\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                    VALUE-BASED vs POLICY-BASED                              │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  VALUE-BASED (DQN)                  POLICY-BASED (Policy Gradient)          │\n│  ─────────────────                  ──────────────────────────────          │\n│                                                                             │\n│  Learn: Q(s, a)                     Learn: π(a|s) directly                  │\n│                                                                             │\n│       State s                            State s                            │\n│          │                                  │                               │\n│          v                                  v                               │\n│    ┌───────────┐                     ┌───────────┐                          │\n│    │  Network  │                     │  Network  │                          │\n│    └───────────┘                     └───────────┘                          │\n│          │                                  │                               │\n│          v                                  v                               │\n│    ┌─────────────┐                   ┌─────────────┐                        │\n│    │Q(s,a1)=0.5  │                   │P(a1|s)=0.7  │                        │\n│    │Q(s,a2)=0.8  │ ← pick max        │P(a2|s)=0.3  │ ← sample from          │\n│    └─────────────┘                   └─────────────┘   distribution         │\n│          │                                  │                               │\n│    Action = a2                       Action ~ Categorical([0.7, 0.3])       │\n│    (deterministic)                   (stochastic)                           │\n│                                                                             │\n│  Pros:                              Pros:                                   │\n│  - Sample efficient                 - Works with continuous actions         │\n│  - Stable with replay               - Natural exploration                   │\n│                                     - Can learn stochastic policies         │\n│                                                                             │\n│  Cons:                              Cons:                                   │\n│  - Discrete actions only            - High variance                         │\n│  - Can be unstable                  - Less sample efficient                 │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### REINFORCE: The Simplest Policy Gradient\n\nThe key insight: **increase probability of actions that led to high rewards**.\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                         REINFORCE INTUITION                                 │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  After an episode:                                                          │\n│                                                                             │\n│  t=0: state s₀ ──> action a₀ ──> ... ──> total reward = 100 (good!)        │\n│                                                                             │\n│  Update rule:                                                               │\n│  ────────────                                                               │\n│  If total reward was HIGH:                                                  │\n│      INCREASE P(a₀|s₀), P(a₁|s₁), ...  (do more of this!)                  │\n│                                                                             │\n│  If total reward was LOW:                                                   │\n│      DECREASE P(a₀|s₀), P(a₁|s₁), ...  (do less of this!)                  │\n│                                                                             │\n│                                                                             │\n│  Mathematically:                                                            │\n│                                                                             │\n│     ∇J(θ) = Σ  ∇ log π(aₜ|sₜ; θ) × Gₜ                                      │\n│             t                                                               │\n│                                                                             │\n│     where Gₜ = total reward from time t (\"return\")                          │\n│                                                                             │\n│  This is called the \"Policy Gradient Theorem\"                               │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Policy network that outputs action probabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=-1)  # Outputs PROBABILITIES (sum to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    \"\"\"REINFORCE (Monte Carlo Policy Gradient) agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int,\n",
    "                 lr: float = 1e-3, gamma: float = 0.99):\n",
    "        self.gamma = gamma\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        # Store episode data\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Sample action from policy (stochastic!).\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        probs = self.policy(state_tensor)  # Get probabilities\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # Save log probability for training\n",
    "        self.saved_log_probs.append(dist.log_prob(action))\n",
    "        return action.item()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update policy using collected episode data.\"\"\"\n",
    "        # Calculate returns (reward-to-go)\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        returns = torch.FloatTensor(returns)\n",
    "        # Normalize for stability\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Policy gradient loss: -log(π) × G\n",
    "        # (negative because we do gradient ASCENT on reward)\n",
    "        policy_loss = []\n",
    "        for log_prob, G in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        \n",
    "        loss = torch.stack(policy_loss).sum()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear episode data\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce(env_name: str = 'CartPole-v1', n_episodes: int = 500):\n",
    "    \"\"\"Train REINFORCE agent.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = REINFORCEAgent(state_dim, action_dim)\n",
    "    episode_rewards = []\n",
    "    \n",
    "    print(\"Training REINFORCE on CartPole\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.rewards.append(reward)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.update()  # Update AFTER episode ends (Monte Carlo)\n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            print(f\"Episode {episode + 1:>4}, Avg Reward: {avg_reward:>6.1f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, episode_rewards\n",
    "\n",
    "# Train REINFORCE\n",
    "pg_agent, pg_rewards = train_reinforce(n_episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Actor-Critic: Best of Both Worlds\n\nREINFORCE has high variance (noisy gradients). Actor-Critic methods fix this:\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                         ACTOR-CRITIC ARCHITECTURE                           │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│                              State s                                        │\n│                                 │                                           │\n│                    ┌────────────┴────────────┐                              │\n│                    │                         │                              │\n│                    v                         v                              │\n│             ┌─────────────┐           ┌─────────────┐                       │\n│             │   ACTOR     │           │   CRITIC    │                       │\n│             │  π(a|s; θ)  │           │   V(s; w)   │                       │\n│             │             │           │             │                       │\n│             │ \"What to do\"│           │ \"How good   │                       │\n│             │             │           │  is this?\"  │                       │\n│             └─────────────┘           └─────────────┘                       │\n│                    │                         │                              │\n│                    v                         v                              │\n│              Action probs               Value estimate                      │\n│              [0.7, 0.3]                    V(s) = 42                        │\n│                    │                         │                              │\n│                    └───────────┬─────────────┘                              │\n│                                │                                            │\n│                                v                                            │\n│                         ADVANTAGE                                           │\n│                    A = r + γV(s') - V(s)                                    │\n│                                                                             │\n│                    \"Was action better or worse than expected?\"              │\n│                                                                             │\n│  If A > 0: Action was BETTER than expected → increase probability           │\n│  If A < 0: Action was WORSE than expected → decrease probability            │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```\n\nThis leads to modern algorithms like **A2C, A3C, PPO, and SAC** (which we'll use in RLlib)!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary: Algorithm Family Tree\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                          RL ALGORITHM FAMILIES                              │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│                         Reinforcement Learning                              │\n│                                   │                                         │\n│                 ┌─────────────────┴─────────────────┐                       │\n│                 │                                   │                       │\n│           VALUE-BASED                         POLICY-BASED                  │\n│           \"Learn Q(s,a)\"                      \"Learn π(a|s)\"                │\n│                 │                                   │                       │\n│         ┌───────┴───────┐                   ┌───────┴───────┐               │\n│         │               │                   │               │               │\n│      Q-Table          DQN              REINFORCE      Actor-Critic          │\n│   (small states)  (neural net)        (vanilla)     (reduced variance)     │\n│                         │                                   │               │\n│                 ┌───────┴───────┐               ┌───────────┼───────────┐   │\n│                 │               │               │           │           │   │\n│              Double          Dueling         A2C/A3C       PPO         SAC  │\n│              DQN             DQN            (parallel)   (stable)   (entropy)│\n│                                                                             │\n│  Discrete actions only ◄──────────────►  Continuous actions supported      │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n| Method | Key Idea | Pros | Cons |\n|--------|----------|------|------|\n| **DQN** | Neural net Q-values + replay + target net | Sample efficient | Discrete only |\n| **REINFORCE** | Directly optimize policy | Simple, continuous actions | High variance |\n| **Actor-Critic** | Combine value + policy learning | Lower variance, flexible | More complex |\n\n## What's Next?\n\nNow that you understand the foundations, we'll use **RLlib** to:\n- Train these algorithms at scale (parallel workers)\n- Use optimized implementations (PPO, SAC, etc.)\n- Not worry about the low-level details!\n\n```\n┌───────────────────┐          ┌───────────────────┐\n│  01.2 Deep RL     │   ───>   │  02.1 RLlib Setup │\n│  (you are here)   │          │                   │\n│                   │          │  - PPO in 10 lines│\n│  - DQN            │          │  - Parallel envs  │\n│  - Policy Gradient│          │  - Easy config    │\n│  - Actor-Critic   │          │                   │\n└───────────────────┘          └───────────────────┘\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}