{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Robotics Capstone: Teaching a Robot to Walk\n\nThe grand finale! We'll use everything we've learned to train a simulated robot to walk.\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                          THE GOAL: LOCOMOTION                               │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│     Start: Robot falls over          End: Robot walks forward!              │\n│                                                                             │\n│         ┌───┐                              ┌───┐                            │\n│        /│   │\\                            /│   │\\                           │\n│       X │   │ X                          / │   │ \\    --> direction         │\n│         └───┘                           /  └───┘  \\                         │\n│           │                            /     │     \\                        │\n│          ═══                        ──┘      │      └──                     │\n│         (oops)                     /         │         \\                    │\n│                                ───┘          │          └───                │\n│                                           (walking!)                        │\n│                                                                             │\n│  Using: Ray + RLlib + PPO + MuJoCo                                          │\n└─────────────────────────────────────────────────────────────────────────────┘\n```\n\n## What We'll Cover\n\n1. **MuJoCo Physics** - Industry-standard robot simulation\n2. **Continuous Control** - Real-valued joint torques (not discrete actions)\n3. **PPO Algorithm** - State-of-the-art for locomotion\n4. **Training at Scale** - Multiple parallel environments\n5. **Evaluation & Recording** - Watch your robot walk!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "```bash\n",
    "pip install \"ray[rllib]\" gymnasium[mujoco] mujoco torch numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Suppress warnings\nimport warnings\nimport logging\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger(\"ray\").setLevel(logging.ERROR)\n\nimport ray\nfrom ray.rllib.algorithms.ppo import PPOConfig\nimport gymnasium as gym\nimport numpy as np\nimport time\n\n# Initialize Ray\nray.init(\n    num_cpus=4,  # Limit CPUs on M1\n    object_store_memory=1 * 1024 * 1024 * 1024,  # 1GB object store\n    ignore_reinit_error=True,\n)\nprint(f\"Ray initialized with {ray.cluster_resources()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Understanding the Ant Environment\n\nThe MuJoCo Ant is a 4-legged robot with 8 actuated joints:\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                             ANT ANATOMY                                     │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│                              TORSO                                          │\n│                             ┌─────┐                                         │\n│                            /│     │\\                                        │\n│           Hip joint ─────>/ │     │ \\<───── Hip joint                       │\n│                          /  │     │  \\                                      │\n│                         /   └─────┘   \\                                     │\n│        Ankle joint ───>/       │       \\<─── Ankle joint                    │\n│                       /        │        \\                                   │\n│                   ───┘         │         └───                               │\n│                  /             │             \\                               │\n│             Foot               │              Foot                          │\n│                                │                                            │\n│                          (same for back legs)                               │\n│                                                                             │\n│  Observation (27 dims):                                                     │\n│  - Torso position, orientation, velocity                                    │\n│  - Joint angles and angular velocities                                      │\n│  - Contact forces with ground                                               │\n│                                                                             │\n│  Action (8 dims):                                                           │\n│  - Torque for each of the 8 joints (continuous, range [-1, 1])              │\n│                                                                             │\n│  Reward:                                                                    │\n│  - Forward velocity (move in +x direction)                                  │\n│  - Alive bonus (don't fall over)                                            │\n│  - Control cost penalty (don't waste energy)                                │\n│  - Contact cost penalty (smooth movement)                                   │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the environment\n",
    "env = gym.make(\"Ant-v4\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ANT ENVIRONMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"  - Shape: {env.observation_space.shape}\")\n",
    "print(f\"  - Range: [{env.observation_space.low.min():.1f}, {env.observation_space.high.max():.1f}]\")\n",
    "print()\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"  - Shape: {env.action_space.shape}\")\n",
    "print(f\"  - Range: [{env.action_space.low.min():.1f}, {env.action_space.high.max():.1f}]\")\n",
    "print()\n",
    "print(\"This is CONTINUOUS control - actions are real numbers, not discrete choices!\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch random actions (the robot will fall over immediately)\n",
    "env = gym.make(\"Ant-v4\", render_mode=\"human\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "print(\"Random policy (robot falls over):\")\n",
    "for step in range(200):\n",
    "    action = env.action_space.sample()  # Random joint torques\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        print(f\"  Episode ended at step {step}, total reward: {total_reward:.1f}\")\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "print(f\"\\nRandom policy reward: {total_reward:.1f} (this is bad!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Configure PPO for Locomotion\n\nPPO (Proximal Policy Optimization) is the go-to algorithm for continuous control:\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                      WHY PPO FOR LOCOMOTION?                                │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  1. STABLE: Clipped objective prevents destructive large updates            │\n│                                                                             │\n│     Policy Update                                                           │\n│     ─────────────                                                           │\n│        │                                                                    │\n│        │  Too big? ──> Clip it!                                             │\n│        │  ┌─────────────────┐                                               │\n│        └─>│ max update = ε  │  (ε ≈ 0.2)                                    │\n│           └─────────────────┘                                               │\n│                                                                             │\n│  2. SAMPLE EFFICIENT: Multiple epochs on same batch                         │\n│                                                                             │\n│     Collect ──> Train ──> Train ──> Train ──> Collect again                 │\n│      data       epoch1    epoch2    epoch3     new data                     │\n│                                                                             │\n│  3. PARALLELIZABLE: Many workers collect experience simultaneously          │\n│                                                                             │\n│     ┌────────┐  ┌────────┐  ┌────────┐  ┌────────┐                         │\n│     │Worker 1│  │Worker 2│  │Worker 3│  │Worker 4│                         │\n│     │  Ant   │  │  Ant   │  │  Ant   │  │  Ant   │                         │\n│     └───┬────┘  └───┬────┘  └───┬────┘  └───┬────┘                         │\n│         └───────────┴─────┬─────┴───────────┘                               │\n│                           v                                                 │\n│                    ┌─────────────┐                                          │\n│                    │   Trainer   │                                          │\n│                    │  (updates)  │                                          │\n│                    └─────────────┘                                          │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure PPO for the Ant\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"Ant-v4\")\n",
    "    .framework(\"torch\")\n",
    "    \n",
    "    # Parallel rollouts - more ants = faster learning!\n",
    "    .env_runners(\n",
    "        num_env_runners=4,           # 4 parallel workers\n",
    "        num_envs_per_env_runner=1,   # 1 env per worker\n",
    "    )\n",
    "    \n",
    "    # PPO hyperparameters tuned for locomotion\n",
    "    .training(\n",
    "        lr=3e-4,                     # Learning rate\n",
    "        gamma=0.99,                  # Discount factor (long-term thinking)\n",
    "        lambda_=0.95,                # GAE parameter\n",
    "        clip_param=0.2,              # PPO clipping\n",
    "        train_batch_size=4000,       # Samples per training batch\n",
    "        sgd_minibatch_size=256,      # Minibatch size\n",
    "        num_sgd_iter=10,             # Epochs per batch\n",
    "        \n",
    "        # Neural network architecture\n",
    "        model={\n",
    "            \"fcnet_hiddens\": [256, 256],  # Two hidden layers\n",
    "            \"fcnet_activation\": \"tanh\",   # Tanh works well for locomotion\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    .resources(\n",
    "        num_gpus=0,  # Set to 1 if you have a GPU\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"PPO Configuration:\")\n",
    "print(f\"  - Workers: {config.num_env_runners}\")\n",
    "print(f\"  - Batch size: {config.train_batch_size}\")\n",
    "print(f\"  - Learning rate: {config.lr}\")\n",
    "print(f\"  - Network: {config.model['fcnet_hiddens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build the algorithm\nalgo = config.build_algo()  # Use new API\nprint(\"PPO algorithm built successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Train the Robot!\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                         TRAINING PROGRESS                                   │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  Iteration 0-10:    Robot falls over immediately                            │\n│                     Reward: ~0-500                                          │\n│                                                                             │\n│  Iteration 10-50:   Robot learns to stay upright                            │\n│                     Starts twitching forward                                │\n│                     Reward: ~500-1500                                       │\n│                                                                             │\n│  Iteration 50-100:  Robot develops a walking gait                           │\n│                     Movement becomes smoother                               │\n│                     Reward: ~1500-3000                                      │\n│                                                                             │\n│  Iteration 100+:    Robot walks efficiently                                 │\n│                     Stable, fast locomotion                                 │\n│                     Reward: ~3000-5000+                                     │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training loop\nNUM_ITERATIONS = 100  # Increase for better results (200+ recommended)\n\nprint(\"=\" * 70)\nprint(\"TRAINING THE ANT TO WALK\")\nprint(\"=\" * 70)\nprint(f\"{'Iter':>5} | {'Reward (mean)':>15} | {'Reward (max)':>12} | {'Episodes':>10}\")\nprint(\"-\" * 70)\n\nbest_reward = -float('inf')\nrewards_history = []\n\nfor i in range(NUM_ITERATIONS):\n    result = algo.train()\n    \n    # Use new API key names\n    mean_reward = result['env_runners']['episode_return_mean']\n    max_reward = result['env_runners']['episode_return_max']\n    episodes = result['env_runners']['num_episodes']\n    \n    rewards_history.append(mean_reward)\n    \n    # Track best\n    if mean_reward > best_reward:\n        best_reward = mean_reward\n        checkpoint = algo.save()\n        marker = \" *NEW BEST*\"\n    else:\n        marker = \"\"\n    \n    # Print progress every 10 iterations\n    if (i + 1) % 10 == 0 or i == 0:\n        print(f\"{i+1:>5} | {mean_reward:>15.1f} | {max_reward:>12.1f} | {episodes:>10}{marker}\")\n\nprint(\"-\" * 70)\nprint(f\"Training complete! Best mean reward: {best_reward:.1f}\")\nprint(f\"Best checkpoint saved to: {checkpoint}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Episode Reward')\n",
    "plt.title('Training Progress')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Smoothed version\n",
    "window = 10\n",
    "smoothed = np.convolve(rewards_history, np.ones(window)/window, mode='valid')\n",
    "plt.plot(smoothed)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Episode Reward (smoothed)')\n",
    "plt.title(f'Training Progress (smoothed, window={window})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Watch Your Robot Walk!\n",
    "\n",
    "The moment of truth - let's see if our training paid off!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained policy\n",
    "env = gym.make(\"Ant-v4\", render_mode=\"human\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATING TRAINED POLICY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "num_episodes = 3\n",
    "total_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while True:\n",
    "        # Get action from trained policy\n",
    "        action = algo.compute_single_action(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    total_rewards.append(episode_reward)\n",
    "    print(f\"Episode {episode + 1}: Reward = {episode_reward:.1f}, Steps = {steps}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Average reward over {num_episodes} episodes: {np.mean(total_rewards):.1f}\")\n",
    "print(f\"Compare to random policy: ~0-500\")\n",
    "print(\"\\nThe robot learned to walk!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. What Did the Robot Learn?\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                      EMERGENT WALKING BEHAVIOR                              │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  The robot discovered several key principles on its own:                    │\n│                                                                             │\n│  1. BALANCE: Keep center of mass over support polygon                       │\n│                                                                             │\n│     Good: ┌───┐           Bad:    ┌───┐                                     │\n│          / COM \\                   │COM│ <- falling!                        │\n│         /   │   \\                  │   │                                    │\n│        ─────┼─────                 └───┴──                                  │\n│             │                        /                                      │\n│       ══════╬══════              ═══╱                                       │\n│        (stable)                  (unstable)                                 │\n│                                                                             │\n│  2. GAIT PATTERN: Alternating diagonal legs (like real ants!)               │\n│                                                                             │\n│     Phase 1:    Phase 2:                                                    │\n│     X───X       ───X                                                        │\n│     │   │       X   X                                                       │\n│     ───X       X───                                                         │\n│                                                                             │\n│  3. ENERGY EFFICIENCY: Smooth, coordinated movements                        │\n│     (penalized by control cost in reward)                                   │\n│                                                                             │\n│  4. FORWARD MOMENTUM: Lean slightly forward to initiate movement            │\n│                                                                             │\n│  The network learned all of this from scratch - no human demonstration!     │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "Congratulations! You've completed the tutorial series. Here's what you can try next:\n",
    "\n",
    "### More Robots\n",
    "```python\n",
    "# Try other MuJoCo environments\n",
    "\"HalfCheetah-v4\"    # 2D running robot (easier)\n",
    "\"Hopper-v4\"         # 1-legged hopping robot\n",
    "\"Walker2d-v4\"       # 2D bipedal walker\n",
    "\"Humanoid-v4\"       # 3D humanoid (hardest!)\n",
    "```\n",
    "\n",
    "### Improve Training\n",
    "- **More iterations**: 200-500 for better policies\n",
    "- **More workers**: Scale to 8-16 for faster training\n",
    "- **GPU training**: Add `num_gpus=1` for faster updates\n",
    "- **Ray Tune**: Hyperparameter search for optimal performance\n",
    "\n",
    "### Advanced Topics\n",
    "- **Curriculum learning**: Start with easier tasks\n",
    "- **Domain randomization**: Vary physics for robustness\n",
    "- **Sim-to-real**: Transfer to real robots\n",
    "- **Multi-agent**: Multiple robots cooperating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "algo.stop()\n",
    "ray.shutdown()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TUTORIAL COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "You've learned:\n",
    "\n",
    "  [x] Ray Core: Tasks, Actors, Object Store\n",
    "  [x] RL Fundamentals: MDPs, Q-learning, Policy Gradients\n",
    "  [x] RLlib: PPO, SAC, DQN algorithms\n",
    "  [x] Custom Environments: Gymnasium interface\n",
    "  [x] Training at Scale: Distributed workers\n",
    "  [x] Robotics: Continuous control with PPO\n",
    "\n",
    "You trained a robot to walk from scratch using reinforcement learning!\n",
    "\n",
    "Next: Try Humanoid-v4 for the ultimate challenge.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}