{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Reinforcement Learning Fundamentals\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand core RL concepts: Agent, Environment, State, Action, Reward\n",
    "- Learn the Markov Decision Process (MDP) framework\n",
    "- Understand the exploration vs exploitation tradeoff\n",
    "- Implement a simple RL agent from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement Learning (RL) is a type of machine learning where an **agent** learns to make decisions by interacting with an **environment**.\n",
    "\n",
    "```\n",
    "┌─────────┐    action     ┌─────────────┐\n",
    "│  Agent  │──────────────▶│ Environment │\n",
    "│         │◀──────────────│             │\n",
    "└─────────┘  state, reward └─────────────┘\n",
    "```\n",
    "\n",
    "### Key Components:\n",
    "1. **Agent**: The learner/decision maker\n",
    "2. **Environment**: What the agent interacts with\n",
    "3. **State (s)**: Current situation of the agent\n",
    "4. **Action (a)**: What the agent can do\n",
    "5. **Reward (r)**: Feedback signal from environment\n",
    "6. **Policy (π)**: Strategy that maps states to actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's install required packages\n",
    "# !pip install numpy matplotlib gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multi-Armed Bandit Problem\n",
    "\n",
    "Let's start with the simplest RL problem - the multi-armed bandit.\n",
    "\n",
    "Imagine you're in a casino with multiple slot machines (bandits). Each machine has a different (unknown) probability of winning. Your goal: maximize total reward over time.\n",
    "\n",
    "This illustrates the **exploration vs exploitation** tradeoff:\n",
    "- **Exploration**: Try new machines to learn their payoffs\n",
    "- **Exploitation**: Use the best known machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    \"\"\"A simple multi-armed bandit environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int = 10):\n",
    "        self.n_arms = n_arms\n",
    "        # True probabilities (unknown to the agent)\n",
    "        self.true_probs = np.random.uniform(0, 1, n_arms)\n",
    "    \n",
    "    def pull(self, arm: int) -> float:\n",
    "        \"\"\"Pull an arm and get reward (1 or 0).\"\"\"\n",
    "        if random.random() < self.true_probs[arm]:\n",
    "            return 1.0\n",
    "        return 0.0\n",
    "    \n",
    "    def optimal_arm(self) -> int:\n",
    "        \"\"\"Return the best arm (for evaluation).\"\"\"\n",
    "        return np.argmax(self.true_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent:\n",
    "    \"\"\"Agent using epsilon-greedy strategy.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int, epsilon: float = 0.1):\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.q_values = np.zeros(n_arms)  # Estimated values\n",
    "        self.n_pulls = np.zeros(n_arms)   # Number of pulls per arm\n",
    "    \n",
    "    def select_arm(self) -> int:\n",
    "        \"\"\"Select arm using epsilon-greedy.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            # Explore: random arm\n",
    "            return random.randint(0, self.n_arms - 1)\n",
    "        else:\n",
    "            # Exploit: best known arm\n",
    "            return np.argmax(self.q_values)\n",
    "    \n",
    "    def update(self, arm: int, reward: float):\n",
    "        \"\"\"Update estimates based on observed reward.\"\"\"\n",
    "        self.n_pulls[arm] += 1\n",
    "        # Incremental mean update\n",
    "        self.q_values[arm] += (reward - self.q_values[arm]) / self.n_pulls[arm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(n_steps: int = 1000, n_arms: int = 10, epsilon: float = 0.1):\n",
    "    \"\"\"Run a bandit experiment and track performance.\"\"\"\n",
    "    bandit = MultiArmedBandit(n_arms)\n",
    "    agent = EpsilonGreedyAgent(n_arms, epsilon)\n",
    "    \n",
    "    rewards = []\n",
    "    optimal_actions = []\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        arm = agent.select_arm()\n",
    "        reward = bandit.pull(arm)\n",
    "        agent.update(arm, reward)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        optimal_actions.append(arm == bandit.optimal_arm())\n",
    "    \n",
    "    return rewards, optimal_actions\n",
    "\n",
    "# Run experiment\n",
    "rewards, optimal = run_experiment(n_steps=1000)\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Cumulative reward\n",
    "axes[0].plot(np.cumsum(rewards))\n",
    "axes[0].set_xlabel('Steps')\n",
    "axes[0].set_ylabel('Cumulative Reward')\n",
    "axes[0].set_title('Learning Progress')\n",
    "\n",
    "# Optimal action percentage (rolling average)\n",
    "window = 50\n",
    "optimal_pct = np.convolve(optimal, np.ones(window)/window, mode='valid')\n",
    "axes[1].plot(optimal_pct)\n",
    "axes[1].set_xlabel('Steps')\n",
    "axes[1].set_ylabel('Optimal Action %')\n",
    "axes[1].set_title('Optimal Action Selection Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process (MDP)\n",
    "\n",
    "Most RL problems are formalized as MDPs, which extend bandits to sequential decision making.\n",
    "\n",
    "An MDP is defined by:\n",
    "- **S**: Set of states\n",
    "- **A**: Set of actions\n",
    "- **P(s'|s,a)**: Transition probabilities\n",
    "- **R(s,a,s')**: Reward function\n",
    "- **γ (gamma)**: Discount factor (0 to 1)\n",
    "\n",
    "### The Markov Property\n",
    "\"The future depends only on the present, not the past\"\n",
    "\n",
    "$$P(s_{t+1}|s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1}|s_t, a_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridWorld: A Simple MDP\n",
    "\n",
    "Let's implement a simple gridworld environment to understand MDPs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    Simple 4x4 GridWorld.\n",
    "    \n",
    "    Layout:\n",
    "    S . . .\n",
    "    . X . .\n",
    "    . . . .\n",
    "    . . . G\n",
    "    \n",
    "    S = Start, G = Goal (+1), X = Obstacle (-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (3, 3)\n",
    "        self.obstacle = (1, 1)\n",
    "        self.state = self.start\n",
    "        \n",
    "        # Actions: 0=up, 1=right, 2=down, 3=left\n",
    "        self.actions = [(−1, 0), (0, 1), (1, 0), (0, −1)]\n",
    "        self.n_actions = 4\n",
    "        self.n_states = self.size * self.size\n",
    "    \n",
    "    def reset(self) -> Tuple[int, int]:\n",
    "        \"\"\"Reset to start state.\"\"\"\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool]:\n",
    "        \"\"\"Take action, return (next_state, reward, done).\"\"\"\n",
    "        dy, dx = self.actions[action]\n",
    "        new_y = max(0, min(self.size - 1, self.state[0] + dy))\n",
    "        new_x = max(0, min(self.size - 1, self.state[1] + dx))\n",
    "        \n",
    "        self.state = (new_y, new_x)\n",
    "        \n",
    "        # Check terminal states\n",
    "        if self.state == self.goal:\n",
    "            return self.state, 1.0, True\n",
    "        elif self.state == self.obstacle:\n",
    "            return self.state, -1.0, True\n",
    "        else:\n",
    "            return self.state, -0.01, False  # Small penalty for each step\n",
    "    \n",
    "    def state_to_idx(self, state: Tuple[int, int]) -> int:\n",
    "        \"\"\"Convert (y, x) to flat index.\"\"\"\n",
    "        return state[0] * self.size + state[1]\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Print the grid.\"\"\"\n",
    "        for y in range(self.size):\n",
    "            row = \"\"\n",
    "            for x in range(self.size):\n",
    "                if (y, x) == self.state:\n",
    "                    row += \"A \"  # Agent\n",
    "                elif (y, x) == self.goal:\n",
    "                    row += \"G \"\n",
    "                elif (y, x) == self.obstacle:\n",
    "                    row += \"X \"\n",
    "                else:\n",
    "                    row += \". \"\n",
    "            print(row)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the environment\n",
    "env = GridWorld()\n",
    "state = env.reset()\n",
    "print(\"Initial state:\")\n",
    "env.render()\n",
    "\n",
    "# Take some random actions\n",
    "print(\"Taking actions: right, down, right, down...\")\n",
    "for action in [1, 2, 1, 2, 1, 2]:  # right, down, right, down...\n",
    "    state, reward, done = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Reward: {reward}, Done: {done}\")\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Functions\n",
    "\n",
    "Two key concepts in RL:\n",
    "\n",
    "### State-Value Function V(s)\n",
    "Expected return starting from state s, following policy π:\n",
    "\n",
    "$$V^\\pi(s) = E_\\pi[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} | S_0 = s]$$\n",
    "\n",
    "### Action-Value Function Q(s, a)\n",
    "Expected return starting from state s, taking action a, then following π:\n",
    "\n",
    "$$Q^\\pi(s, a) = E_\\pi[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} | S_0 = s, A_0 = a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning: Our First RL Algorithm\n",
    "\n",
    "Q-Learning is a model-free algorithm that learns Q-values directly from experience.\n",
    "\n",
    "**Update rule:**\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n",
    "\n",
    "Where:\n",
    "- α (alpha): Learning rate\n",
    "- γ (gamma): Discount factor\n",
    "- r: Reward received\n",
    "- s': Next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning agent for GridWorld.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_states: int, n_actions: int, \n",
    "                 alpha: float = 0.1, gamma: float = 0.99, epsilon: float = 0.1):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha      # Learning rate\n",
    "        self.gamma = gamma      # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        \n",
    "        # Initialize Q-table with zeros\n",
    "        self.q_table = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    def select_action(self, state_idx: int) -> int:\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        return np.argmax(self.q_table[state_idx])\n",
    "    \n",
    "    def update(self, state_idx: int, action: int, reward: float, \n",
    "               next_state_idx: int, done: bool):\n",
    "        \"\"\"Q-Learning update.\"\"\"\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * np.max(self.q_table[next_state_idx])\n",
    "        \n",
    "        # Q-Learning update\n",
    "        self.q_table[state_idx, action] += self.alpha * (\n",
    "            target - self.q_table[state_idx, action]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(n_episodes: int = 500):\n",
    "    \"\"\"Train Q-Learning agent on GridWorld.\"\"\"\n",
    "    env = GridWorld()\n",
    "    agent = QLearningAgent(env.n_states, env.n_actions)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        state_idx = env.state_to_idx(state)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.select_action(state_idx)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state_idx = env.state_to_idx(next_state)\n",
    "            \n",
    "            agent.update(state_idx, action, reward, next_state_idx, done)\n",
    "            \n",
    "            state_idx = next_state_idx\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done or steps > 100:  # Max steps to prevent infinite loops\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "    \n",
    "    return agent, episode_rewards, episode_lengths\n",
    "\n",
    "# Train the agent\n",
    "agent, rewards, lengths = train_q_learning(n_episodes=500)\n",
    "\n",
    "# Plot learning curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Smooth the curves\n",
    "window = 20\n",
    "smoothed_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "smoothed_lengths = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "\n",
    "axes[0].plot(smoothed_rewards)\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Total Reward')\n",
    "axes[0].set_title('Episode Rewards (smoothed)')\n",
    "\n",
    "axes[1].plot(smoothed_lengths)\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Steps')\n",
    "axes[1].set_title('Episode Lengths (smoothed)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned policy\n",
    "def visualize_policy(agent, env):\n",
    "    \"\"\"Show the learned policy as arrows.\"\"\"\n",
    "    arrows = ['↑', '→', '↓', '←']\n",
    "    \n",
    "    print(\"Learned Policy:\")\n",
    "    for y in range(env.size):\n",
    "        row = \"\"\n",
    "        for x in range(env.size):\n",
    "            if (y, x) == env.goal:\n",
    "                row += \"G \"\n",
    "            elif (y, x) == env.obstacle:\n",
    "                row += \"X \"\n",
    "            else:\n",
    "                state_idx = y * env.size + x\n",
    "                best_action = np.argmax(agent.q_table[state_idx])\n",
    "                row += arrows[best_action] + \" \"\n",
    "        print(row)\n",
    "\n",
    "env = GridWorld()\n",
    "visualize_policy(agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **RL is about learning from interaction** - The agent learns by trying actions and observing rewards\n",
    "\n",
    "2. **Exploration vs Exploitation** - Balance trying new things with using what works\n",
    "\n",
    "3. **Value Functions** - Estimate how good states/actions are for long-term reward\n",
    "\n",
    "4. **Q-Learning** - Learn optimal policy without knowing environment dynamics\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll:\n",
    "- Explore more RL algorithms (SARSA, Policy Gradients)\n",
    "- Introduce function approximation (Neural Networks)\n",
    "- Set up Ray and RLlib for scalable RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Modify epsilon**: Try different epsilon values (0.01, 0.2, 0.5). How does it affect learning?\n",
    "\n",
    "2. **Add more obstacles**: Modify GridWorld to have multiple obstacles. Does the agent still learn?\n",
    "\n",
    "3. **Implement SARSA**: Modify Q-Learning to use SARSA update: Q(s,a) ← Q(s,a) + α[r + γQ(s',a') - Q(s,a)]\n",
    "\n",
    "4. **Decaying epsilon**: Implement epsilon decay (start high, decrease over time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
