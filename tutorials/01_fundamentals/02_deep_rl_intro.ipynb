{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Deep Reinforcement Learning Introduction\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand why we need function approximation\n",
    "- Learn the basics of Deep Q-Networks (DQN)\n",
    "- Understand Policy Gradient methods\n",
    "- Implement a simple neural network-based agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "\n",
    "Q-tables work for small state spaces, but real-world problems have:\n",
    "- **Continuous states**: Robot joint angles, pixel images\n",
    "- **High dimensions**: Atari games have 210×160×3 = 100,800 pixels\n",
    "- **Combinatorial explosion**: Chess has ~10^43 possible states\n",
    "\n",
    "**Solution**: Use function approximation (neural networks) to generalize across states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# !pip install torch numpy gymnasium matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network (DQN)\n",
    "\n",
    "DQN (Mnih et al., 2015) was a breakthrough that played Atari games at superhuman level.\n",
    "\n",
    "### Key Innovations:\n",
    "\n",
    "1. **Neural Network Q-Function**: Q(s,a;θ) approximates Q*(s,a)\n",
    "\n",
    "2. **Experience Replay**: Store transitions in a buffer, sample randomly\n",
    "   - Breaks correlation between consecutive samples\n",
    "   - Improves sample efficiency\n",
    "\n",
    "3. **Target Network**: Separate network for computing targets\n",
    "   - Stabilizes training\n",
    "   - Updated periodically from main network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for DQN.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store a transition.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"Sample a batch of transitions.\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.LongTensor(actions),\n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.FloatTensor(dones)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent with experience replay and target network.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int,\n",
    "                 lr: float = 1e-3, gamma: float = 0.99,\n",
    "                 epsilon_start: float = 1.0, epsilon_end: float = 0.01,\n",
    "                 epsilon_decay: float = 0.995, buffer_size: int = 10000,\n",
    "                 batch_size: int = 64, target_update: int = 10):\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.update_counter = 0\n",
    "        \n",
    "        # Networks\n",
    "        self.policy_net = DQN(state_dim, action_dim)\n",
    "        self.target_net = DQN(state_dim, action_dim)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = self.policy_net(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update the network from replay buffer.\"\"\"\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Target Q values (using target network)\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_net(next_states).max(1)[0]\n",
    "            target_q = rewards + self.gamma * next_q * (1 - dones)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q.squeeze(), target_q)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(env_name: str = 'CartPole-v1', n_episodes: int = 300):\n",
    "    \"\"\"Train DQN on a Gymnasium environment.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = DQNAgent(state_dim, action_dim)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_loss = []\n",
    "        \n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.buffer.push(state, action, reward, next_state, float(done))\n",
    "            loss = agent.update()\n",
    "            if loss > 0:\n",
    "                episode_loss.append(loss)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        losses.append(np.mean(episode_loss) if episode_loss else 0)\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            print(f\"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}, Epsilon: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, episode_rewards, losses\n",
    "\n",
    "# Train the agent\n",
    "agent, rewards, losses = train_dqn(n_episodes=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Rewards\n",
    "window = 20\n",
    "smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "axes[0].plot(smoothed)\n",
    "axes[0].axhline(y=195, color='r', linestyle='--', label='Solved threshold')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Reward')\n",
    "axes[0].set_title('Episode Rewards (CartPole-v1)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Loss\n",
    "smoothed_loss = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "axes[1].plot(smoothed_loss)\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Training Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Methods\n",
    "\n",
    "Instead of learning Q-values, we can directly optimize the policy.\n",
    "\n",
    "### REINFORCE Algorithm\n",
    "\n",
    "Parameterize policy as π(a|s;θ) and optimize:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = E_\\pi[\\sum_t \\nabla_\\theta \\log \\pi(a_t|s_t;\\theta) \\cdot G_t]$$\n",
    "\n",
    "Where G_t is the return from time t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Simple policy network.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    \"\"\"REINFORCE (Monte Carlo Policy Gradient) agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int,\n",
    "                 lr: float = 1e-3, gamma: float = 0.99):\n",
    "        self.gamma = gamma\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"Sample action from policy.\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        probs = self.policy(state_tensor)\n",
    "        \n",
    "        # Sample from categorical distribution\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        self.saved_log_probs.append(dist.log_prob(action))\n",
    "        return action.item()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update policy using collected episode data.\"\"\"\n",
    "        # Calculate returns\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        returns = torch.FloatTensor(returns)\n",
    "        # Normalize returns for stability\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Policy gradient loss\n",
    "        policy_loss = []\n",
    "        for log_prob, G in zip(self.saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        \n",
    "        loss = torch.stack(policy_loss).sum()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Clear episode data\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce(env_name: str = 'CartPole-v1', n_episodes: int = 500):\n",
    "    \"\"\"Train REINFORCE agent.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = REINFORCEAgent(state_dim, action_dim)\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.rewards.append(reward)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.update()\n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-50:])\n",
    "            print(f\"Episode {episode + 1}, Avg Reward: {avg_reward:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, episode_rewards\n",
    "\n",
    "# Train REINFORCE\n",
    "pg_agent, pg_rewards = train_reinforce(n_episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Value vs Policy Methods\n",
    "\n",
    "| Aspect | Value-Based (DQN) | Policy-Based (REINFORCE) |\n",
    "|--------|-------------------|-------------------------|\n",
    "| Output | Q-values | Action probabilities |\n",
    "| Action space | Discrete only | Discrete or Continuous |\n",
    "| Exploration | ε-greedy | Built-in (stochastic) |\n",
    "| Sample efficiency | Higher (replay) | Lower (on-policy) |\n",
    "| Convergence | Can be unstable | Stable but high variance |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic: Best of Both Worlds\n",
    "\n",
    "Combines value and policy methods:\n",
    "- **Actor**: Policy network that selects actions\n",
    "- **Critic**: Value network that evaluates actions\n",
    "\n",
    "This reduces variance while maintaining flexibility.\n",
    "\n",
    "```\n",
    "┌─────────┐     state      ┌────────┐\n",
    "│  State  │───────────────▶│ Actor  │──▶ action\n",
    "│         │                └────────┘\n",
    "│         │     state      ┌────────┐\n",
    "│         │───────────────▶│ Critic │──▶ value estimate\n",
    "└─────────┘                └────────┘\n",
    "```\n",
    "\n",
    "This leads to algorithms like A2C, A3C, PPO, SAC (covered later with RLlib)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Function approximation** enables RL in complex environments\n",
    "\n",
    "2. **DQN** uses replay buffers and target networks for stable training\n",
    "\n",
    "3. **Policy Gradients** directly optimize the policy, work with continuous actions\n",
    "\n",
    "4. **Actor-Critic** methods combine both approaches\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next section, we'll set up Ray and RLlib to train these algorithms at scale!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Double DQN**: Implement Double DQN to reduce overestimation bias\n",
    "\n",
    "2. **Dueling DQN**: Separate state-value and advantage streams\n",
    "\n",
    "3. **Baseline subtraction**: Add a baseline to REINFORCE to reduce variance\n",
    "\n",
    "4. **Different environments**: Try LunarLander-v2 or Acrobot-v1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
