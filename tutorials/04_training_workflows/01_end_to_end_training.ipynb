{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 4.1 End-to-End Training Workflows\n\n**Prerequisites**: Complete modules 00-03\n\n## Learning Objectives\n- Structure a complete RL training pipeline\n- Implement proper experiment tracking\n- Handle checkpointing and resumption\n- Evaluate and compare trained policies\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                        COMPLETE TRAINING WORKFLOW                           │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│   1. SETUP          2. TRAIN           3. EVALUATE       4. DEPLOY         │\n│   ┌─────────┐       ┌─────────┐       ┌─────────┐       ┌─────────┐        │\n│   │ Config  │       │ Loop    │       │ Test    │       │ Export  │        │\n│   │ Env     │  ──>  │ Log     │  ──>  │ Compare │  ──>  │ Serve   │        │\n│   │ Algo    │       │ Save    │       │ Visualize│      │ Monitor │        │\n│   └─────────┘       └─────────┘       └─────────┘       └─────────┘        │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "from ray import tune\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# M1-friendly Ray init\n",
    "ray.init(\n",
    "    num_cpus=4,\n",
    "    object_store_memory=1 * 1024 * 1024 * 1024,\n",
    "    ignore_reinit_error=True,\n",
    ")\n",
    "\n",
    "print(f\"Ray initialized: {ray.cluster_resources()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Structure\n",
    "\n",
    "A well-organized RL project:\n",
    "\n",
    "```\n",
    "my_rl_project/\n",
    "├── configs/                 # Algorithm configurations\n",
    "│   ├── ppo_cartpole.py\n",
    "│   └── sac_pendulum.py\n",
    "├── envs/                    # Custom environments\n",
    "│   └── my_env.py\n",
    "├── scripts/\n",
    "│   ├── train.py            # Training script\n",
    "│   ├── evaluate.py         # Evaluation script\n",
    "│   └── serve.py            # Serving script\n",
    "├── checkpoints/            # Saved models\n",
    "├── results/                # Logs and metrics\n",
    "└── requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Management\n",
    "\n",
    "Keep configs separate and version-controlled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment configuration as a dictionary\n",
    "# This makes it easy to save, load, and modify\n",
    "\n",
    "EXPERIMENT_CONFIG = {\n",
    "    \"name\": \"cartpole_ppo_v1\",\n",
    "    \"env\": \"CartPole-v1\",\n",
    "    \"algorithm\": \"PPO\",\n",
    "    \n",
    "    # Training settings\n",
    "    \"training\": {\n",
    "        \"lr\": 3e-4,\n",
    "        \"gamma\": 0.99,\n",
    "        \"train_batch_size\": 4000,\n",
    "        \"sgd_minibatch_size\": 128,\n",
    "        \"num_sgd_iter\": 10,\n",
    "    },\n",
    "    \n",
    "    # Network architecture\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"tanh\",\n",
    "    },\n",
    "    \n",
    "    # Workers\n",
    "    \"num_workers\": 2,\n",
    "    \n",
    "    # Stopping criteria\n",
    "    \"stop\": {\n",
    "        \"episode_reward_mean\": 475,\n",
    "        \"training_iteration\": 100,\n",
    "    },\n",
    "}\n",
    "\n",
    "def config_to_rllib(exp_config):\n",
    "    \"\"\"Convert experiment config to RLlib config.\"\"\"\n",
    "    return (\n",
    "        PPOConfig()\n",
    "        .environment(exp_config[\"env\"])\n",
    "        .framework(\"torch\")\n",
    "        .env_runners(num_env_runners=exp_config[\"num_workers\"])\n",
    "        .training(\n",
    "            lr=exp_config[\"training\"][\"lr\"],\n",
    "            gamma=exp_config[\"training\"][\"gamma\"],\n",
    "            train_batch_size=exp_config[\"training\"][\"train_batch_size\"],\n",
    "            sgd_minibatch_size=exp_config[\"training\"][\"sgd_minibatch_size\"],\n",
    "            num_sgd_iter=exp_config[\"training\"][\"num_sgd_iter\"],\n",
    "            model=exp_config[\"model\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(f\"Experiment: {EXPERIMENT_CONFIG['name']}\")\n",
    "print(f\"Environment: {EXPERIMENT_CONFIG['env']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Training with Proper Logging\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                            TRAINING LOOP                                    │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│   for iteration in range(max_iters):                                        │\n│       │                                                                     │\n│       ├──> result = algo.train()                                            │\n│       │                                                                     │\n│       ├──> Log metrics (reward, loss, entropy)                              │\n│       │                                                                     │\n│       ├──> Check stopping criteria                                          │\n│       │       └──> if solved: break                                         │\n│       │                                                                     │\n│       └──> Checkpoint (every N iterations)                                  │\n│               └──> algo.save(checkpoint_dir)                                │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingTracker:\n",
    "    \"\"\"Track and log training metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name, log_dir=\"./results\"):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.log_dir = Path(log_dir) / experiment_name\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.metrics = []\n",
    "        self.best_reward = -float('inf')\n",
    "        self.best_checkpoint = None\n",
    "        \n",
    "    def log(self, iteration, result):\n",
    "        \"\"\"Log metrics from training result.\"\"\"\n",
    "        metrics = {\n",
    "            \"iteration\": iteration,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"episode_reward_mean\": result[\"env_runners\"][\"episode_reward_mean\"],\n",
    "            \"episode_reward_max\": result[\"env_runners\"][\"episode_reward_max\"],\n",
    "            \"episode_reward_min\": result[\"env_runners\"][\"episode_reward_min\"],\n",
    "            \"episodes_total\": result[\"env_runners\"][\"num_episodes\"],\n",
    "            \"timesteps_total\": result.get(\"timesteps_total\", 0),\n",
    "        }\n",
    "        self.metrics.append(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def is_best(self, reward):\n",
    "        \"\"\"Check if this is the best reward so far.\"\"\"\n",
    "        if reward > self.best_reward:\n",
    "            self.best_reward = reward\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def save_checkpoint(self, algo, checkpoint_path):\n",
    "        \"\"\"Save checkpoint and track best.\"\"\"\n",
    "        self.best_checkpoint = checkpoint_path\n",
    "        \n",
    "    def save_metrics(self):\n",
    "        \"\"\"Save all metrics to file.\"\"\"\n",
    "        metrics_file = self.log_dir / \"metrics.json\"\n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "        print(f\"Metrics saved to {metrics_file}\")\n",
    "        \n",
    "    def summary(self):\n",
    "        \"\"\"Print training summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Experiment: {self.experiment_name}\")\n",
    "        print(f\"Total iterations: {len(self.metrics)}\")\n",
    "        print(f\"Best reward: {self.best_reward:.2f}\")\n",
    "        print(f\"Best checkpoint: {self.best_checkpoint}\")\n",
    "        print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, max_iterations=50, checkpoint_freq=10):\n",
    "    \"\"\"Complete training workflow.\"\"\"\n",
    "    \n",
    "    # Setup\n",
    "    rllib_config = config_to_rllib(config)\n",
    "    algo = rllib_config.build()\n",
    "    tracker = TrainingTracker(config[\"name\"])\n",
    "    \n",
    "    print(f\"Starting training: {config['name']}\")\n",
    "    print(f\"Stop criteria: {config['stop']}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Training loop\n",
    "    for i in range(max_iterations):\n",
    "        # Train\n",
    "        result = algo.train()\n",
    "        \n",
    "        # Log\n",
    "        metrics = tracker.log(i, result)\n",
    "        reward = metrics[\"episode_reward_mean\"]\n",
    "        \n",
    "        # Print progress\n",
    "        marker = \"\"\n",
    "        if tracker.is_best(reward):\n",
    "            marker = \" ** NEW BEST **\"\n",
    "            checkpoint = algo.save(tracker.log_dir / \"checkpoints\")\n",
    "            tracker.save_checkpoint(algo, checkpoint)\n",
    "            \n",
    "        if (i + 1) % 5 == 0 or marker:\n",
    "            print(f\"Iter {i+1:3d} | Reward: {reward:7.2f} | Episodes: {metrics['episodes_total']:5d}{marker}\")\n",
    "        \n",
    "        # Check stopping criteria\n",
    "        if reward >= config[\"stop\"][\"episode_reward_mean\"]:\n",
    "            print(f\"\\nSolved at iteration {i+1}!\")\n",
    "            break\n",
    "            \n",
    "        if i + 1 >= config[\"stop\"][\"training_iteration\"]:\n",
    "            print(f\"\\nReached max iterations.\")\n",
    "            break\n",
    "    \n",
    "    # Save final metrics\n",
    "    tracker.save_metrics()\n",
    "    tracker.summary()\n",
    "    \n",
    "    return algo, tracker\n",
    "\n",
    "# Run training\n",
    "algo, tracker = train(EXPERIMENT_CONFIG, max_iterations=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "Proper evaluation of trained policies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(algo, env_name, num_episodes=10, render=False):\n",
    "    \"\"\"Evaluate a trained policy.\"\"\"\n",
    "    \n",
    "    env = gym.make(env_name, render_mode=\"human\" if render else None)\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs, info = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "            action = algo.compute_single_action(obs)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    results = {\n",
    "        \"mean_reward\": np.mean(episode_rewards),\n",
    "        \"std_reward\": np.std(episode_rewards),\n",
    "        \"min_reward\": np.min(episode_rewards),\n",
    "        \"max_reward\": np.max(episode_rewards),\n",
    "        \"mean_length\": np.mean(episode_lengths),\n",
    "        \"episodes\": episode_rewards,\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate\n",
    "eval_results = evaluate_policy(algo, EXPERIMENT_CONFIG[\"env\"], num_episodes=20)\n",
    "\n",
    "print(\"\\nEVALUATION RESULTS\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Mean Reward:  {eval_results['mean_reward']:.2f} +/- {eval_results['std_reward']:.2f}\")\n",
    "print(f\"Min Reward:   {eval_results['min_reward']:.2f}\")\n",
    "print(f\"Max Reward:   {eval_results['max_reward']:.2f}\")\n",
    "print(f\"Mean Length:  {eval_results['mean_length']:.1f} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Checkpointing and Resumption\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                          CHECKPOINT WORKFLOW                                │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│   Training crashes at iteration 50                                          │\n│              │                                                              │\n│              v                                                              │\n│   checkpoint_iter_45/  <-- Last saved checkpoint                            │\n│              │                                                              │\n│              v                                                              │\n│   algo = Algorithm.from_checkpoint(path)  <-- Resume!                       │\n│              │                                                              │\n│              v                                                              │\n│   Continue from iteration 46                                                │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint\n",
    "checkpoint_path = algo.save(\"./checkpoints/my_model\")\n",
    "print(f\"Saved to: {checkpoint_path}\")\n",
    "\n",
    "# Stop the algorithm\n",
    "algo.stop()\n",
    "\n",
    "# Later: restore from checkpoint\n",
    "restored_algo = Algorithm.from_checkpoint(checkpoint_path)\n",
    "print(\"Restored successfully!\")\n",
    "\n",
    "# Verify it works\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs, _ = env.reset()\n",
    "action = restored_algo.compute_single_action(obs)\n",
    "print(f\"Action from restored policy: {action}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Multiple Runs\n",
    "\n",
    "Compare different configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_configs(configs, max_iters=20):\n",
    "    \"\"\"Train and compare multiple configurations.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, config in configs.items():\n",
    "        print(f\"\\nTraining: {name}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        rllib_config = config_to_rllib(config)\n",
    "        algo = rllib_config.build()\n",
    "        \n",
    "        rewards = []\n",
    "        for i in range(max_iters):\n",
    "            result = algo.train()\n",
    "            reward = result[\"env_runners\"][\"episode_reward_mean\"]\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"  Iter {i+1}: {reward:.2f}\")\n",
    "        \n",
    "        results[name] = rewards\n",
    "        algo.stop()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define configs to compare\n",
    "configs_to_compare = {\n",
    "    \"small_network\": {\n",
    "        **EXPERIMENT_CONFIG,\n",
    "        \"name\": \"small_network\",\n",
    "        \"model\": {\"fcnet_hiddens\": [32, 32], \"fcnet_activation\": \"tanh\"},\n",
    "    },\n",
    "    \"large_network\": {\n",
    "        **EXPERIMENT_CONFIG,\n",
    "        \"name\": \"large_network\",\n",
    "        \"model\": {\"fcnet_hiddens\": [128, 128], \"fcnet_activation\": \"tanh\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "# comparison_results = compare_configs(configs_to_compare, max_iters=15)\n",
    "print(\"Comparison example (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(results):\n",
    "    \"\"\"Plot learning curves for comparison.\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for name, rewards in results.items():\n",
    "        plt.plot(rewards, label=name, linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Training Iteration')\n",
    "    plt.ylabel('Mean Episode Reward')\n",
    "    plt.title('Configuration Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# plot_comparison(comparison_results)\n",
    "print(\"Plotting example (uncomment after running comparison)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using Ray Tune for Experiments\n",
    "\n",
    "For more complex experiments, use Ray Tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "# Configure experiment with Tune\n",
    "tune_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        lr=3e-4,\n",
    "        train_batch_size=4000,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run with Tune for automatic logging\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    param_space=tune_config,\n",
    "    run_config=tune.RunConfig(\n",
    "        name=\"ppo_cartpole_experiment\",\n",
    "        stop={\"env_runners/episode_reward_mean\": 450, \"training_iteration\": 30},\n",
    "        checkpoint_config=tune.CheckpointConfig(\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# results = tuner.fit()\n",
    "# best = results.get_best_result(metric=\"env_runners/episode_reward_mean\", mode=\"max\")\n",
    "# print(f\"Best reward: {best.metrics['env_runners']['episode_reward_mean']}\")\n",
    "\n",
    "print(\"Tune experiment example (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n1. **Structure your project** - Separate configs, envs, scripts, and results\n\n2. **Track everything** - Log metrics, save checkpoints, record configs\n\n3. **Evaluate properly** - Multiple episodes, track variance\n\n4. **Use checkpoints** - Save frequently, enable resumption\n\n5. **Compare systematically** - Same seeds, same evaluation\n\n## What's Next\n\n```\n┌──────────────────┐     ┌──────────────────┐     ┌──────────────────┐\n│  05 Distributed  │     │     06 Tune      │     │  09 Robotics!    │\n│     Training     │ --> │   Hyperparams    │ --> │                  │\n│                  │     │                  │     │  Train an Ant    │\n│    Scale up!     │     │   Find optimal   │     │    to walk!      │\n└──────────────────┘     └──────────────────┘     └──────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "restored_algo.stop()\n",
    "ray.shutdown()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}