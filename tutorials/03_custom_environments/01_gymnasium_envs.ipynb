{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Creating Custom Gymnasium Environments\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the Gymnasium environment interface\n",
    "- Create custom environments from scratch\n",
    "- Register environments with RLlib\n",
    "- Handle different observation and action spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.registry import register_env\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Tuple, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gymnasium Interface\n",
    "\n",
    "Every Gymnasium environment must implement:\n",
    "\n",
    "```python\n",
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, config=None):\n",
    "        self.observation_space = ...  # Define observation space\n",
    "        self.action_space = ...       # Define action space\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        # Return (observation, info)\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Return (observation, reward, terminated, truncated, info)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Trading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Simple trading environment.\n",
    "    \n",
    "    State: [position, cash, price, price_change]\n",
    "    Actions: 0=hold, 1=buy, 2=sell\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "    \n",
    "    def __init__(self, config: Dict = None):\n",
    "        super().__init__()\n",
    "        config = config or {}\n",
    "        \n",
    "        self.initial_cash = config.get(\"initial_cash\", 10000)\n",
    "        self.max_steps = config.get(\"max_steps\", 200)\n",
    "        self.volatility = config.get(\"volatility\", 0.02)\n",
    "        \n",
    "        # Action space: hold, buy, sell\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        \n",
    "        # Observation space: [position, cash_normalized, price_normalized, price_change]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([-100, 0, 0, -1]),\n",
    "            high=np.array([100, 2, 2, 1]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[Dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        self.cash = self.initial_cash\n",
    "        self.position = 0  # Number of shares\n",
    "        self.price = 100.0\n",
    "        self.initial_price = self.price\n",
    "        self.step_count = 0\n",
    "        self.prev_portfolio_value = self.cash\n",
    "        \n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self) -> np.ndarray:\n",
    "        \"\"\"Get current observation.\"\"\"\n",
    "        price_change = (self.price - self.initial_price) / self.initial_price\n",
    "        return np.array([\n",
    "            self.position,\n",
    "            self.cash / self.initial_cash,\n",
    "            self.price / self.initial_price,\n",
    "            price_change\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "    def _get_portfolio_value(self) -> float:\n",
    "        \"\"\"Calculate total portfolio value.\"\"\"\n",
    "        return self.cash + self.position * self.price\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        # Execute action\n",
    "        if action == 1:  # Buy\n",
    "            if self.cash >= self.price:\n",
    "                shares_to_buy = int(self.cash // self.price)\n",
    "                self.position += shares_to_buy\n",
    "                self.cash -= shares_to_buy * self.price\n",
    "        elif action == 2:  # Sell\n",
    "            if self.position > 0:\n",
    "                self.cash += self.position * self.price\n",
    "                self.position = 0\n",
    "        \n",
    "        # Simulate price movement (random walk with drift)\n",
    "        price_change = np.random.normal(0.0001, self.volatility)\n",
    "        self.price *= (1 + price_change)\n",
    "        self.price = max(self.price, 1.0)  # Price floor\n",
    "        \n",
    "        # Calculate reward (change in portfolio value)\n",
    "        current_value = self._get_portfolio_value()\n",
    "        reward = (current_value - self.prev_portfolio_value) / self.initial_cash\n",
    "        self.prev_portfolio_value = current_value\n",
    "        \n",
    "        self.step_count += 1\n",
    "        terminated = False\n",
    "        truncated = self.step_count >= self.max_steps\n",
    "        \n",
    "        info = {\n",
    "            \"portfolio_value\": current_value,\n",
    "            \"position\": self.position,\n",
    "            \"price\": self.price\n",
    "        }\n",
    "        \n",
    "        return self._get_obs(), reward, terminated, truncated, info\n",
    "    \n",
    "    def render(self):\n",
    "        print(f\"Step {self.step_count}: Price=${self.price:.2f}, \"\n",
    "              f\"Position={self.position}, Cash=${self.cash:.2f}, \"\n",
    "              f\"Value=${self._get_portfolio_value():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the environment\n",
    "env = SimpleTradingEnv()\n",
    "obs, _ = env.reset()\n",
    "print(f\"Initial observation: {obs}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "\n",
    "# Run a few steps\n",
    "total_reward = 0\n",
    "for i in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "\n",
    "print(f\"\\nTotal reward: {total_reward:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Resource Management Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResourceManagementEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Environment for learning to manage compute resources.\n",
    "    \n",
    "    State: [current_load, num_servers, queue_length, time_of_day]\n",
    "    Actions: -1=remove server, 0=do nothing, 1=add server\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict = None):\n",
    "        super().__init__()\n",
    "        config = config or {}\n",
    "        \n",
    "        self.max_servers = config.get(\"max_servers\", 10)\n",
    "        self.server_cost = config.get(\"server_cost\", 1.0)\n",
    "        self.queue_penalty = config.get(\"queue_penalty\", 0.5)\n",
    "        self.max_steps = config.get(\"max_steps\", 288)  # 24 hours * 12 (5-min intervals)\n",
    "        \n",
    "        # Continuous action space for scaling\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Observation space\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, 0, 0]),\n",
    "            high=np.array([1, 1, 1, 1]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def _generate_load_pattern(self) -> np.ndarray:\n",
    "        \"\"\"Generate realistic daily load pattern.\"\"\"\n",
    "        # Sinusoidal pattern with peak at noon\n",
    "        t = np.linspace(0, 2 * np.pi, self.max_steps)\n",
    "        base_load = 0.3 + 0.4 * np.sin(t - np.pi/2)  # Peak at t=Ï€\n",
    "        noise = np.random.normal(0, 0.05, self.max_steps)\n",
    "        return np.clip(base_load + noise, 0.1, 1.0)\n",
    "    \n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[Dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        self.load_pattern = self._generate_load_pattern()\n",
    "        self.num_servers = 3\n",
    "        self.queue_length = 0\n",
    "        self.step_count = 0\n",
    "        \n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self) -> np.ndarray:\n",
    "        current_load = self.load_pattern[self.step_count]\n",
    "        time_of_day = self.step_count / self.max_steps\n",
    "        \n",
    "        return np.array([\n",
    "            current_load,\n",
    "            self.num_servers / self.max_servers,\n",
    "            min(self.queue_length / 100, 1.0),\n",
    "            time_of_day\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        # Scale action to server change\n",
    "        server_change = int(np.round(action[0] * 2))  # -2 to +2\n",
    "        self.num_servers = np.clip(\n",
    "            self.num_servers + server_change, 1, self.max_servers\n",
    "        )\n",
    "        \n",
    "        # Calculate capacity and queue\n",
    "        current_load = self.load_pattern[self.step_count]\n",
    "        capacity = self.num_servers / self.max_servers\n",
    "        \n",
    "        # Update queue based on load vs capacity\n",
    "        if current_load > capacity:\n",
    "            self.queue_length += (current_load - capacity) * 50\n",
    "        else:\n",
    "            self.queue_length = max(0, self.queue_length - 10)\n",
    "        \n",
    "        # Calculate reward\n",
    "        server_cost = self.num_servers * self.server_cost\n",
    "        queue_cost = self.queue_length * self.queue_penalty\n",
    "        reward = -server_cost - queue_cost\n",
    "        \n",
    "        # Normalize reward\n",
    "        reward = reward / 100\n",
    "        \n",
    "        self.step_count += 1\n",
    "        terminated = False\n",
    "        truncated = self.step_count >= self.max_steps\n",
    "        \n",
    "        info = {\n",
    "            \"servers\": self.num_servers,\n",
    "            \"queue\": self.queue_length,\n",
    "            \"load\": current_load\n",
    "        }\n",
    "        \n",
    "        return self._get_obs(), reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test resource management env\n",
    "env = ResourceManagementEnv()\n",
    "obs, _ = env.reset()\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "\n",
    "# Visualize load pattern\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(env.load_pattern)\n",
    "plt.xlabel('Time Step (5-min intervals)')\n",
    "plt.ylabel('Load')\n",
    "plt.title('Daily Load Pattern')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering with RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# Register custom environments\n",
    "def trading_env_creator(env_config):\n",
    "    return SimpleTradingEnv(env_config)\n",
    "\n",
    "def resource_env_creator(env_config):\n",
    "    return ResourceManagementEnv(env_config)\n",
    "\n",
    "register_env(\"SimpleTradingEnv\", trading_env_creator)\n",
    "register_env(\"ResourceManagementEnv\", resource_env_creator)\n",
    "\n",
    "print(\"Environments registered!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on custom trading environment\n",
    "trading_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        env=\"SimpleTradingEnv\",\n",
    "        env_config={\n",
    "            \"initial_cash\": 10000,\n",
    "            \"max_steps\": 200,\n",
    "            \"volatility\": 0.02,\n",
    "        }\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        lr=3e-4,\n",
    "        train_batch_size=2000,\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = trading_config.build()\n",
    "\n",
    "print(\"Training on SimpleTradingEnv...\")\n",
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "    print(f\"Iter {i+1}: Reward = {result['env_runners']['episode_reward_mean']:.2f}\")\n",
    "\n",
    "algo.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Space Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Gymnasium space types\n",
    "\n",
    "# Discrete - single integer action\n",
    "discrete_space = spaces.Discrete(5)  # Actions: 0, 1, 2, 3, 4\n",
    "print(f\"Discrete: {discrete_space.sample()}\")\n",
    "\n",
    "# Box - continuous bounded values\n",
    "box_space = spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32)\n",
    "print(f\"Box: {box_space.sample()}\")\n",
    "\n",
    "# MultiDiscrete - multiple discrete actions\n",
    "multi_discrete = spaces.MultiDiscrete([3, 4, 2])  # 3 actions with different ranges\n",
    "print(f\"MultiDiscrete: {multi_discrete.sample()}\")\n",
    "\n",
    "# MultiBinary - binary flags\n",
    "multi_binary = spaces.MultiBinary(4)\n",
    "print(f\"MultiBinary: {multi_binary.sample()}\")\n",
    "\n",
    "# Dict - nested spaces\n",
    "dict_space = spaces.Dict({\n",
    "    \"position\": spaces.Box(low=-10, high=10, shape=(2,)),\n",
    "    \"velocity\": spaces.Box(low=-1, high=1, shape=(2,)),\n",
    "    \"target\": spaces.Discrete(4)\n",
    "})\n",
    "print(f\"Dict: {dict_space.sample()}\")\n",
    "\n",
    "# Tuple - ordered collection\n",
    "tuple_space = spaces.Tuple((\n",
    "    spaces.Discrete(3),\n",
    "    spaces.Box(low=0, high=1, shape=(2,))\n",
    "))\n",
    "print(f\"Tuple: {tuple_space.sample()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Image-Based Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVisualEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Simple environment with image observations.\n",
    "    Agent must find target in a grid (represented as image).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict = None):\n",
    "        super().__init__()\n",
    "        config = config or {}\n",
    "        \n",
    "        self.grid_size = config.get(\"grid_size\", 8)\n",
    "        self.image_size = config.get(\"image_size\", 64)\n",
    "        \n",
    "        # Image observation (RGB)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=255,\n",
    "            shape=(self.image_size, self.image_size, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        \n",
    "        # Movement actions: up, right, down, left\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[Dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Random agent and target positions\n",
    "        self.agent_pos = np.array([0, 0])\n",
    "        self.target_pos = np.array([\n",
    "            np.random.randint(0, self.grid_size),\n",
    "            np.random.randint(0, self.grid_size)\n",
    "        ])\n",
    "        \n",
    "        # Ensure they're not the same\n",
    "        while np.array_equal(self.agent_pos, self.target_pos):\n",
    "            self.target_pos = np.array([\n",
    "                np.random.randint(0, self.grid_size),\n",
    "                np.random.randint(0, self.grid_size)\n",
    "            ])\n",
    "        \n",
    "        self.steps = 0\n",
    "        return self._render_image(), {}\n",
    "    \n",
    "    def _render_image(self) -> np.ndarray:\n",
    "        \"\"\"Render the grid as an RGB image.\"\"\"\n",
    "        cell_size = self.image_size // self.grid_size\n",
    "        img = np.ones((self.image_size, self.image_size, 3), dtype=np.uint8) * 255\n",
    "        \n",
    "        # Draw grid\n",
    "        for i in range(self.grid_size + 1):\n",
    "            pos = i * cell_size\n",
    "            img[pos:pos+1, :] = 200\n",
    "            img[:, pos:pos+1] = 200\n",
    "        \n",
    "        # Draw target (green)\n",
    "        ty, tx = self.target_pos * cell_size\n",
    "        img[ty+2:ty+cell_size-2, tx+2:tx+cell_size-2] = [0, 255, 0]\n",
    "        \n",
    "        # Draw agent (blue)\n",
    "        ay, ax = self.agent_pos * cell_size\n",
    "        img[ay+2:ay+cell_size-2, ax+2:ax+cell_size-2] = [0, 0, 255]\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        # Move agent\n",
    "        moves = [(-1, 0), (0, 1), (1, 0), (0, -1)]  # up, right, down, left\n",
    "        dy, dx = moves[action]\n",
    "        \n",
    "        new_pos = self.agent_pos + np.array([dy, dx])\n",
    "        new_pos = np.clip(new_pos, 0, self.grid_size - 1)\n",
    "        self.agent_pos = new_pos\n",
    "        \n",
    "        self.steps += 1\n",
    "        \n",
    "        # Check if reached target\n",
    "        if np.array_equal(self.agent_pos, self.target_pos):\n",
    "            return self._render_image(), 10.0, True, False, {}\n",
    "        \n",
    "        # Small negative reward for each step\n",
    "        truncated = self.steps >= 100\n",
    "        return self._render_image(), -0.1, False, truncated, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test visual environment\n",
    "env = SimpleVisualEnv()\n",
    "obs, _ = env.reset()\n",
    "\n",
    "print(f\"Observation shape: {obs.shape}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "axes[0].imshow(obs)\n",
    "axes[0].set_title(\"Initial\")\n",
    "\n",
    "for i, action in enumerate([1, 2, 2]):\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    axes[i+1].imshow(obs)\n",
    "    axes[i+1].set_title(f\"After action {action}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CNN with RLlib for Image Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register visual environment\n",
    "register_env(\"SimpleVisualEnv\", lambda c: SimpleVisualEnv(c))\n",
    "\n",
    "# Configure PPO with CNN model\n",
    "visual_config = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        env=\"SimpleVisualEnv\",\n",
    "        env_config={\"grid_size\": 8, \"image_size\": 64}\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        lr=1e-4,\n",
    "        train_batch_size=2000,\n",
    "        model={\n",
    "            # Use CNN for image input\n",
    "            \"conv_filters\": [\n",
    "                [16, [4, 4], 2],  # [num_filters, kernel_size, stride]\n",
    "                [32, [4, 4], 2],\n",
    "                [64, [4, 4], 2],\n",
    "            ],\n",
    "            \"fcnet_hiddens\": [256],\n",
    "            \"fcnet_activation\": \"relu\",\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Visual environment config created!\")\n",
    "# Training would be similar to before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Gymnasium interface** requires `reset()`, `step()`, and space definitions\n",
    "\n",
    "2. **Register environments** with RLlib using `register_env()`\n",
    "\n",
    "3. **Choose appropriate spaces** for your problem (Discrete, Box, Dict, etc.)\n",
    "\n",
    "4. **Normalize observations** for better training stability\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll explore multi-agent environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
