{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Creating Custom Environments for RLlib\n\n**Prerequisites**: Complete [02_rllib_basics](../02_rllib_basics/01_ray_setup.ipynb)\n\nYou've trained on CartPole. Now let's train on YOUR problems!\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                    THE GYMNASIUM ENVIRONMENT INTERFACE                      │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  Every RL environment follows the same pattern:                             │\n│                                                                             │\n│     ┌─────────┐                         ┌─────────────────┐                 │\n│     │  Agent  │                         │   Environment   │                 │\n│     │         │                         │                 │                 │\n│     │ (RLlib) │ ────── action ────────> │  (YOUR CODE!)   │                 │\n│     │         │                         │                 │                 │\n│     │         │ <── state, reward ───── │                 │                 │\n│     └─────────┘                         └─────────────────┘                 │\n│                                                                             │\n│  Your job: Define how the environment responds to actions                   │\n│  RLlib's job: Figure out what actions to take                               │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## The 5 Things Every Environment Needs\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                      GYMNASIUM ENV REQUIREMENTS                             │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  class MyEnv(gym.Env):                                                      │\n│                                                                             │\n│      1. __init__(self)                                                      │\n│         └─> Define observation_space and action_space                       │\n│                                                                             │\n│      2. observation_space = ...                                             │\n│         └─> What does the agent SEE? (state shape and bounds)               │\n│                                                                             │\n│      3. action_space = ...                                                  │\n│         └─> What can the agent DO? (action shape and bounds)                │\n│                                                                             │\n│      4. reset(self) -> (observation, info)                                  │\n│         └─> Start a new episode, return initial state                       │\n│                                                                             │\n│      5. step(self, action) -> (observation, reward, terminated,             │\n│                                truncated, info)                             │\n│         └─> Take action, return next state and reward                       │\n│                                                                             │\n│                                                                             │\n│  That's it! Just these 5 things and you have an RL environment.             │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"ray\").setLevel(logging.ERROR)\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.registry import register_env\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Tuple, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Understanding Spaces\n\nSpaces define what observations and actions look like.\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                          GYMNASIUM SPACES                                   │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  spaces.Discrete(n)                                                         │\n│  ──────────────────                                                         │\n│  Integer from 0 to n-1                                                      │\n│                                                                             │\n│  Example: Discrete(4) = {0, 1, 2, 3}                                        │\n│           (up, right, down, left)                                           │\n│                                                                             │\n│  ───────────────────────────────────────────────────────────────────────    │\n│                                                                             │\n│  spaces.Box(low, high, shape)                                               │\n│  ────────────────────────────                                               │\n│  Continuous values in a bounded box                                         │\n│                                                                             │\n│  Example: Box(low=-1.0, high=1.0, shape=(3,))                               │\n│           = array of 3 floats, each between -1 and 1                        │\n│           like [0.5, -0.3, 0.8]                                             │\n│                                                                             │\n│  ───────────────────────────────────────────────────────────────────────    │\n│                                                                             │\n│  spaces.MultiDiscrete([n1, n2, ...])                                        │\n│  ───────────────────────────────────                                        │\n│  Multiple discrete values                                                   │\n│                                                                             │\n│  Example: MultiDiscrete([3, 4]) = two integers, first 0-2, second 0-3       │\n│           like [2, 1]                                                       │\n│                                                                             │\n│  ───────────────────────────────────────────────────────────────────────    │\n│                                                                             │\n│  spaces.Dict({...})                                                         │\n│  ──────────────────                                                         │\n│  Dictionary of spaces (for complex observations)                            │\n│                                                                             │\n│  Example: Dict({\"position\": Box(...), \"velocity\": Box(...)})                │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what each space looks like\n",
    "\n",
    "print(\"SPACE EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Discrete - single integer action\n",
    "discrete = spaces.Discrete(4)  # 0, 1, 2, or 3\n",
    "print(f\"\\nDiscrete(4): {[discrete.sample() for _ in range(5)]}\")\n",
    "print(\"  Use for: up/down/left/right, buy/sell/hold\")\n",
    "\n",
    "# Box - continuous values\n",
    "box = spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32)\n",
    "print(f\"\\nBox(low=-1, high=1, shape=(3,)): {box.sample()}\")\n",
    "print(\"  Use for: robot joint angles, continuous control\")\n",
    "\n",
    "# MultiDiscrete - multiple discrete values\n",
    "multi_discrete = spaces.MultiDiscrete([3, 4, 2])\n",
    "print(f\"\\nMultiDiscrete([3,4,2]): {multi_discrete.sample()}\")\n",
    "print(\"  Use for: multiple independent choices\")\n",
    "\n",
    "# Dict - nested spaces\n",
    "dict_space = spaces.Dict({\n",
    "    \"position\": spaces.Box(low=-10, high=10, shape=(2,)),\n",
    "    \"velocity\": spaces.Box(low=-1, high=1, shape=(2,)),\n",
    "    \"inventory\": spaces.Discrete(5)\n",
    "})\n",
    "print(f\"\\nDict space: {dict_space.sample()}\")\n",
    "print(\"  Use for: complex observations with multiple types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Example 1: Simple Trading Environment\n\nLet's build a trading environment from scratch!\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                        TRADING ENVIRONMENT                                  │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  SCENARIO:                                                                  │\n│  You're a trader. You can buy, sell, or hold a stock.                       │\n│  Goal: Maximize profit over 200 time steps.                                 │\n│                                                                             │\n│  STATE (what the agent sees):                                               │\n│  ┌──────────────────────────────────────────┐                               │\n│  │ [position, cash, price, price_change]    │                               │\n│  │                                          │                               │\n│  │  position: how many shares you own       │                               │\n│  │  cash: how much money you have           │                               │\n│  │  price: current stock price              │                               │\n│  │  price_change: recent price movement     │                               │\n│  └──────────────────────────────────────────┘                               │\n│                                                                             │\n│  ACTIONS (what the agent can do):                                           │\n│  ┌──────────────────────────────────────────┐                               │\n│  │  0 = HOLD   (do nothing)                 │                               │\n│  │  1 = BUY    (buy as many shares as       │                               │\n│  │             possible with current cash)  │                               │\n│  │  2 = SELL   (sell all shares)            │                               │\n│  └──────────────────────────────────────────┘                               │\n│                                                                             │\n│  REWARD:                                                                    │\n│  Change in portfolio value (cash + shares × price)                          │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Simple trading environment.\n",
    "    \n",
    "    The agent learns to trade a single stock to maximize profit.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "    \n",
    "    def __init__(self, config: Dict = None):\n",
    "        super().__init__()\n",
    "        config = config or {}\n",
    "        \n",
    "        # Environment parameters\n",
    "        self.initial_cash = config.get(\"initial_cash\", 10000)\n",
    "        self.max_steps = config.get(\"max_steps\", 200)\n",
    "        self.volatility = config.get(\"volatility\", 0.02)\n",
    "        \n",
    "        # ============================================================\n",
    "        # REQUIREMENT 1 & 2: Define action and observation spaces\n",
    "        # ============================================================\n",
    "        \n",
    "        # Action space: 3 discrete choices\n",
    "        self.action_space = spaces.Discrete(3)  # 0=hold, 1=buy, 2=sell\n",
    "        \n",
    "        # Observation space: 4 continuous values\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([-100, 0, 0, -1]),   # [position, cash, price, change]\n",
    "            high=np.array([100, 2, 2, 1]),    # normalized values\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    # ================================================================\n",
    "    # REQUIREMENT 3: reset() method\n",
    "    # ================================================================\n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[Dict] = None):\n",
    "        \"\"\"Reset the environment for a new episode.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Reset state\n",
    "        self.cash = self.initial_cash\n",
    "        self.position = 0  # Number of shares owned\n",
    "        self.price = 100.0\n",
    "        self.initial_price = self.price\n",
    "        self.step_count = 0\n",
    "        self.prev_portfolio_value = self.cash\n",
    "        \n",
    "        return self._get_obs(), {}  # Return (observation, info)\n",
    "    \n",
    "    def _get_obs(self) -> np.ndarray:\n",
    "        \"\"\"Convert internal state to observation.\"\"\"\n",
    "        price_change = (self.price - self.initial_price) / self.initial_price\n",
    "        return np.array([\n",
    "            self.position,\n",
    "            self.cash / self.initial_cash,      # Normalized cash\n",
    "            self.price / self.initial_price,    # Normalized price\n",
    "            price_change\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "    def _get_portfolio_value(self) -> float:\n",
    "        \"\"\"Total value = cash + shares × price.\"\"\"\n",
    "        return self.cash + self.position * self.price\n",
    "    \n",
    "    # ================================================================\n",
    "    # REQUIREMENT 4: step() method\n",
    "    # ================================================================\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        \"\"\"Execute one step in the environment.\"\"\"\n",
    "        \n",
    "        # Execute action\n",
    "        if action == 1:  # BUY\n",
    "            if self.cash >= self.price:\n",
    "                shares_to_buy = int(self.cash // self.price)\n",
    "                self.position += shares_to_buy\n",
    "                self.cash -= shares_to_buy * self.price\n",
    "        elif action == 2:  # SELL\n",
    "            if self.position > 0:\n",
    "                self.cash += self.position * self.price\n",
    "                self.position = 0\n",
    "        # action == 0 means HOLD (do nothing)\n",
    "        \n",
    "        # Simulate price movement (random walk)\n",
    "        price_change = np.random.normal(0.0001, self.volatility)\n",
    "        self.price *= (1 + price_change)\n",
    "        self.price = max(self.price, 1.0)  # Price floor\n",
    "        \n",
    "        # Calculate reward (change in portfolio value)\n",
    "        current_value = self._get_portfolio_value()\n",
    "        reward = (current_value - self.prev_portfolio_value) / self.initial_cash\n",
    "        self.prev_portfolio_value = current_value\n",
    "        \n",
    "        # Check termination\n",
    "        self.step_count += 1\n",
    "        terminated = False  # No early termination\n",
    "        truncated = self.step_count >= self.max_steps  # Episode ends after max_steps\n",
    "        \n",
    "        info = {\n",
    "            \"portfolio_value\": current_value,\n",
    "            \"position\": self.position,\n",
    "            \"price\": self.price\n",
    "        }\n",
    "        \n",
    "        return self._get_obs(), reward, terminated, truncated, info\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Print current state.\"\"\"\n",
    "        print(f\"Step {self.step_count}: Price=${self.price:.2f}, \"\n",
    "              f\"Position={self.position}, Cash=${self.cash:.2f}, \"\n",
    "              f\"Value=${self._get_portfolio_value():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the environment\n",
    "print(\"TESTING SimpleTradingEnv\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "env = SimpleTradingEnv()\n",
    "obs, _ = env.reset()\n",
    "\n",
    "print(f\"\\nObservation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"\\nInitial observation: {obs}\")\n",
    "print(\"  [position, cash_norm, price_norm, price_change]\")\n",
    "\n",
    "# Run a few steps\n",
    "print(\"\\nRunning 10 random steps:\")\n",
    "total_reward = 0\n",
    "for i in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    action_names = [\"HOLD\", \"BUY\", \"SELL\"]\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(f\"  Action: {action_names[action]:>4}, Reward: {reward:>+.4f}, Value: ${info['portfolio_value']:.2f}\")\n",
    "\n",
    "print(f\"\\nTotal reward: {total_reward:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Registering with RLlib\n\nTo use your environment with RLlib, you need to register it.\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                    ENVIRONMENT REGISTRATION                                 │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  Step 1: Create a \"creator\" function                                        │\n│  ───────────────────────────────────                                        │\n│                                                                             │\n│      def env_creator(env_config):                                           │\n│          return MyEnv(env_config)                                           │\n│                                                                             │\n│  Step 2: Register with a name                                               │\n│  ──────────────────────────────                                             │\n│                                                                             │\n│      register_env(\"MyEnv-v0\", env_creator)                                  │\n│                                                                             │\n│  Step 3: Use in config                                                      │\n│  ─────────────────────                                                      │\n│                                                                             │\n│      config = PPOConfig().environment(\"MyEnv-v0\", env_config={...})         │\n│                                                                             │\n│                                                                             │\n│  WHY A CREATOR FUNCTION?                                                    │\n│  RLlib creates multiple copies of your environment (one per worker).        │\n│  The creator function is called for each copy.                              │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray\n",
    "ray.init(\n",
    "    num_cpus=4,\n",
    "    object_store_memory=1 * 1024 * 1024 * 1024,\n",
    "    ignore_reinit_error=True,\n",
    ")\n",
    "\n",
    "# Register the environment\n",
    "def trading_env_creator(env_config):\n",
    "    return SimpleTradingEnv(env_config)\n",
    "\n",
    "register_env(\"SimpleTradingEnv\", trading_env_creator)\n",
    "print(\"Environment registered as 'SimpleTradingEnv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the custom environment\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        env=\"SimpleTradingEnv\",  # Use registered name\n",
    "        env_config={             # Passed to env_creator\n",
    "            \"initial_cash\": 10000,\n",
    "            \"max_steps\": 200,\n",
    "            \"volatility\": 0.02,\n",
    "        }\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    .env_runners(num_env_runners=2)\n",
    "    .training(\n",
    "        lr=3e-4,\n",
    "        train_batch_size=2000,\n",
    "    )\n",
    ")\n",
    "\n",
    "algo = config.build_algo()\n",
    "\n",
    "print(\"Training on SimpleTradingEnv...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "    mean_reward = result[\"env_runners\"][\"episode_return_mean\"]\n",
    "    print(f\"Iter {i+1:>2}: Mean reward = {mean_reward:.3f}\")\n",
    "\n",
    "algo.stop()\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Example 2: Resource Management Environment\n\nA more complex example with **continuous actions**.\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                    RESOURCE MANAGEMENT ENVIRONMENT                          │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  SCENARIO:                                                                  │\n│  You manage servers for a website. Traffic varies throughout the day.       │\n│  Goal: Minimize costs while keeping users happy.                            │\n│                                                                             │\n│      Traffic Pattern (24 hours):                                            │\n│                                                                             │\n│      Load │         _____                                                   │\n│       1.0 │        /     \\        Peak at noon                              │\n│           │       /       \\                                                 │\n│       0.5 │      /         \\                                                │\n│           │_____/           \\_____                                          │\n│       0.0 └───────────────────────                                          │\n│           0:00  6:00  12:00  18:00  24:00                                   │\n│                                                                             │\n│  STATE: [current_load, num_servers, queue_length, time_of_day]              │\n│                                                                             │\n│  ACTION: Continuous value from -1 to +1                                     │\n│          -1 = remove 2 servers                                              │\n│          +1 = add 2 servers                                                 │\n│                                                                             │\n│  REWARD:                                                                    │\n│          - Server cost (more servers = higher cost)                         │\n│          - Queue penalty (long queue = unhappy users)                       │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResourceManagementEnv(gym.Env):\n",
    "    \"\"\"Environment for learning to manage compute resources.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict = None):\n",
    "        super().__init__()\n",
    "        config = config or {}\n",
    "        \n",
    "        self.max_servers = config.get(\"max_servers\", 10)\n",
    "        self.server_cost = config.get(\"server_cost\", 1.0)\n",
    "        self.queue_penalty = config.get(\"queue_penalty\", 0.5)\n",
    "        self.max_steps = config.get(\"max_steps\", 288)  # 24 hours × 12 (5-min intervals)\n",
    "        \n",
    "        # CONTINUOUS action space!\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Observation space\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, 0, 0]),\n",
    "            high=np.array([1, 1, 1, 1]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def _generate_load_pattern(self) -> np.ndarray:\n",
    "        \"\"\"Generate realistic daily load pattern.\"\"\"\n",
    "        t = np.linspace(0, 2 * np.pi, self.max_steps)\n",
    "        base_load = 0.3 + 0.4 * np.sin(t - np.pi/2)  # Peak at noon\n",
    "        noise = np.random.normal(0, 0.05, self.max_steps)\n",
    "        return np.clip(base_load + noise, 0.1, 1.0)\n",
    "    \n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[Dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        self.load_pattern = self._generate_load_pattern()\n",
    "        self.num_servers = 3\n",
    "        self.queue_length = 0\n",
    "        self.step_count = 0\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self) -> np.ndarray:\n",
    "        current_load = self.load_pattern[self.step_count]\n",
    "        time_of_day = self.step_count / self.max_steps\n",
    "        return np.array([\n",
    "            current_load,\n",
    "            self.num_servers / self.max_servers,\n",
    "            min(self.queue_length / 100, 1.0),\n",
    "            time_of_day\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        # Continuous action: scale to server change (-2 to +2)\n",
    "        server_change = int(np.round(action[0] * 2))\n",
    "        self.num_servers = np.clip(\n",
    "            self.num_servers + server_change, 1, self.max_servers\n",
    "        )\n",
    "        \n",
    "        # Calculate capacity vs load\n",
    "        current_load = self.load_pattern[self.step_count]\n",
    "        capacity = self.num_servers / self.max_servers\n",
    "        \n",
    "        # Update queue\n",
    "        if current_load > capacity:\n",
    "            self.queue_length += (current_load - capacity) * 50\n",
    "        else:\n",
    "            self.queue_length = max(0, self.queue_length - 10)\n",
    "        \n",
    "        # Calculate reward (negative = cost)\n",
    "        server_cost = self.num_servers * self.server_cost\n",
    "        queue_cost = self.queue_length * self.queue_penalty\n",
    "        reward = -(server_cost + queue_cost) / 100  # Normalize\n",
    "        \n",
    "        self.step_count += 1\n",
    "        terminated = False\n",
    "        truncated = self.step_count >= self.max_steps\n",
    "        \n",
    "        info = {\"servers\": self.num_servers, \"queue\": self.queue_length, \"load\": current_load}\n",
    "        return self._get_obs(), reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and visualize\n",
    "env = ResourceManagementEnv()\n",
    "obs, _ = env.reset()\n",
    "\n",
    "print(\"ResourceManagementEnv\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}  <- CONTINUOUS!\")\n",
    "\n",
    "# Visualize load pattern\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(env.load_pattern)\n",
    "plt.xlabel('Time Step (5-min intervals)')\n",
    "plt.ylabel('Load')\n",
    "plt.title('Daily Traffic Pattern')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Tips for Building Good Environments\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                     ENVIRONMENT DESIGN TIPS                                 │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                             │\n│  1. NORMALIZE OBSERVATIONS                                                  │\n│     ─────────────────────────                                               │\n│     Keep values roughly between -1 and 1 (or 0 and 1)                       │\n│     Neural networks train faster with normalized inputs                     │\n│                                                                             │\n│     Bad:  obs = [100000, 0.001, 5000]  (wildly different scales)            │\n│     Good: obs = [1.0, 0.1, 0.5]        (all similar scale)                  │\n│                                                                             │\n│  ───────────────────────────────────────────────────────────────────────    │\n│                                                                             │\n│  2. SHAPE REWARDS CAREFULLY                                                 │\n│     ───────────────────────────                                             │\n│     Make rewards dense (give feedback often)                                │\n│     Normalize to prevent huge values                                        │\n│                                                                             │\n│     Bad:  reward = 0 for 999 steps, then +1000000 at end                    │\n│     Good: reward = small bonus every step you're doing well                 │\n│                                                                             │\n│  ───────────────────────────────────────────────────────────────────────    │\n│                                                                             │\n│  3. INCLUDE ALL RELEVANT STATE                                              │\n│     ──────────────────────────────                                          │\n│     The agent only knows what you tell it!                                  │\n│     If something affects the reward, include it in the observation          │\n│                                                                             │\n│  ───────────────────────────────────────────────────────────────────────    │\n│                                                                             │\n│  4. TEST MANUALLY FIRST                                                     │\n│     ──────────────────────                                                  │\n│     Play your environment yourself!                                         │\n│     If you can't figure out a good strategy, neither can the agent          │\n│                                                                             │\n│  ───────────────────────────────────────────────────────────────────────    │\n│                                                                             │\n│  5. USE THE RIGHT SPACE TYPE                                                │\n│     ────────────────────────────                                            │\n│     Discrete: finite choices (up/down, buy/sell)                            │\n│     Box: continuous values (torque, speed)                                  │\n│     MultiDiscrete: multiple independent choices                             │\n│                                                                             │\n└─────────────────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **5 requirements**: `__init__`, `observation_space`, `action_space`, `reset()`, `step()`\n",
    "\n",
    "2. **Choose the right space**: Discrete for choices, Box for continuous\n",
    "\n",
    "3. **Register with RLlib** using `register_env()`\n",
    "\n",
    "4. **Normalize** observations and rewards for better training\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "```\n",
    "┌──────────────────┐          ┌──────────────────┐          ┌──────────────────┐\n",
    "│ 03 Custom Envs   │   ───>   │ 05 Distributed   │   ───>   │ 06 Tune          │\n",
    "│ (you are here)   │          │                  │          │                  │\n",
    "│                  │          │ Scale training   │          │ Find best        │\n",
    "│ - Gymnasium API  │          │ to many workers  │          │ hyperparameters  │\n",
    "│ - Spaces         │          │                  │          │                  │\n",
    "└──────────────────┘          └──────────────────┘          └──────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}